#!/usr/bin/env python3
"""
Standalone Vector Database Loader for Tenders
–Ü–∑–æ–ª—å–æ–≤–∞–Ω–∏–π —à–≤–∏–¥–∫–∏–π –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á –¥–∞–Ω–∏—Ö —É Qdrant –±–µ–∑ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –≤—ñ–¥ –æ—Å–Ω–æ–≤–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏

–ê–≤—Ç–æ—Ä: Assistant
–í–µ—Ä—Å—ñ—è: 1.0.0
"""

import json
import hashlib
import logging
import sys
import os
import re
import time
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import numpy as np
from tqdm import tqdm
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

# Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import ResponseHandlingException

# Sentence transformers
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')


@dataclass
class LoaderConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á–∞"""
    # Qdrant
    qdrant_host: str = "localhost"
    qdrant_port: int = 6333
    collection_name: str = "tender_vectors"
    
    # –ú–æ–¥–µ–ª—å
    model_name: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    embedding_dim: int = 768
    
    # –ü—Ä–æ—Ü–µ—Å—ñ–Ω–≥
    batch_size: int = 1000
    max_retries: int = 3
    retry_delay: float = 1.0
    
    # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
    use_cache: bool = True
    cache_size: int = 50000
    validate_data: bool = True
    skip_existing: bool = True
    
    # –õ–æ–≥—É–≤–∞–Ω–Ω—è
    log_level: str = "INFO"
    log_file: str = "vector_loader.log"
    
    # –û–±–º–µ–∂–µ–Ω–Ω—è
    max_text_length: int = 512
    min_text_length: int = 2


class TenderDataValidator:
    """–í–∞–ª—ñ–¥–∞—Ç–æ—Ä –¥–∞–Ω–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤"""
    
    # –û–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–æ–ª—è –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤
    REQUIRED_FIELDS_SETS = [
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä 1: —Ç–µ–Ω–¥–µ—Ä + –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫
        ['F_TENDERNUMBER', 'EDRPOU'],
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä 2: —Ç–µ–Ω–¥–µ—Ä + —Ç–æ–≤–∞—Ä
        ['F_TENDERNUMBER', 'F_ITEMNAME'],
        # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä 3: –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫ + —Ç–æ–≤–∞—Ä
        ['EDRPOU', 'F_ITEMNAME']
    ]
    
    # –í—Å—ñ –º–æ–∂–ª–∏–≤—ñ –ø–æ–ª—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    ALL_FIELDS = {
        # –Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
        'ID', 'F_TENDERNUMBER', 'EDRPOU', 
        
        # –ù–∞–∑–≤–∏ —Ç–∞ –æ–ø–∏—Å–∏
        'F_TENDERNAME', 'F_ITEMNAME', 'F_DETAILNAME', 'F_INDUSTRYNAME',
        'OWNER_NAME', 'supp_name',
        
        # –§—ñ–Ω–∞–Ω—Å–∏
        'ITEM_BUDGET', 'F_qty', 'F_price', 'F_TENDERCURRENCY', 'F_TENDERCURRENCYRATE',
        
        # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
        'CPV', 'F_codeUA',
        
        # –î–∞—Ç–∏
        'DATEEND', 'EXTRACTION_DATE',
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        'WON'
    }
    
    @staticmethod
    def validate_record(record: Dict) -> Tuple[bool, str]:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É"""
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —î —Ö–æ—á–∞ –± –æ–¥–∏–Ω –Ω–∞–±—ñ—Ä –æ–±–æ–≤'—è–∑–∫–æ–≤–∏—Ö –ø–æ–ª—ñ–≤
        for field_set in TenderDataValidator.REQUIRED_FIELDS_SETS:
            if all(record.get(field) for field in field_set):
                return True, "OK"
        
        # –Ø–∫—â–æ –∂–æ–¥–µ–Ω –Ω–∞–±—ñ—Ä –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å
        missing = []
        for field in ['F_TENDERNUMBER', 'EDRPOU', 'F_ITEMNAME']:
            if not record.get(field):
                missing.append(field)
        
        return False, f"Missing: {', '.join(missing)}"
    
    @staticmethod
    def extract_text_for_embedding(record: Dict) -> str:
        """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É"""
        text_parts = []
        
        # –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω—ñ —Ç–µ–∫—Å—Ç–æ–≤—ñ –ø–æ–ª—è
        priority_fields = ['F_ITEMNAME', 'F_TENDERNAME', 'F_DETAILNAME']
        for field in priority_fields:
            value = record.get(field, '').strip()
            if value:
                text_parts.append(value)
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–æ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        if record.get('F_INDUSTRYNAME'):
            text_parts.append(f"–≥–∞–ª—É–∑—å: {record['F_INDUSTRYNAME']}")
        
        if record.get('OWNER_NAME'):
            text_parts.append(f"–∑–∞–º–æ–≤–Ω–∏–∫: {record['OWNER_NAME']}")
        
        # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç—É –º–∞–ª–æ, –¥–æ–¥–∞—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
        if len(' '.join(text_parts)) < 20:
            if record.get('CPV'):
                text_parts.append(f"CPV: {record['CPV']}")
            if record.get('F_TENDERNUMBER'):
                text_parts.append(f"—Ç–µ–Ω–¥–µ—Ä: {record['F_TENDERNUMBER']}")
        
        return ' '.join(text_parts).strip()


class VectorDBLoader:
    """–û—Å–Ω–æ–≤–Ω–∏–π –∫–ª–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á–∞"""
    
    def __init__(self, config: LoaderConfig):
        self.config = config
        self.setup_logging()
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤
        self.client = None
        self.embedding_model = None
        
        # –ö–µ—à—ñ
        self.embedding_cache = {}
        self.hash_cache = set()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = defaultdict(int)
        
    def setup_logging(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        
        # –§–∞–π–ª–æ–≤–∏–π handler
        file_handler = logging.FileHandler(self.config.log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(logging.Formatter(log_format))
        
        # –ö–æ–Ω—Å–æ–ª—å–Ω–∏–π handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(getattr(logging, self.config.log_level))
        console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥–µ—Ä–∞
        self.logger = logging.getLogger('VectorDBLoader')
        self.logger.setLevel(logging.DEBUG)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        # –í—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –ª–æ–≥—ñ–≤ –≤—ñ–¥ –±—ñ–±–ª—ñ–æ—Ç–µ–∫
        logging.getLogger('sentence_transformers').setLevel(logging.WARNING)
        logging.getLogger('transformers').setLevel(logging.WARNING)
    
    def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤"""
        self.logger.info("="*60)
        self.logger.info("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Vector Database Loader")
        self.logger.info("="*60)
        
        # 1. –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant
        self.logger.info(f"üì° –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant {self.config.qdrant_host}:{self.config.qdrant_port}")
        try:
            self.client = QdrantClient(
                host=self.config.qdrant_host,
                port=self.config.qdrant_port,
                timeout=30
            )
            # –¢–µ—Å—Ç –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è
            self.client.get_collections()
            self.logger.info("‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant —É—Å–ø—ñ—à–Ω–µ")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant: {e}")
            raise
        
        # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        self.logger.info(f"ü§ñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {self.config.model_name}")
        try:
            self.embedding_model = SentenceTransformer(self.config.model_name)
            # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
            self.embedding_model.show_progress_bar = False
            self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            raise
        
        # 3. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        self._init_collection()
    
    def _init_collection(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–±–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"""
        collections = [col.name for col in self.client.get_collections().collections]
        
        if self.config.collection_name in collections:
            info = self.client.get_collection(self.config.collection_name)
            self.logger.info(f"üìä –ö–æ–ª–µ–∫—Ü—ñ—è —ñ—Å–Ω—É—î: {info.points_count:,} –∑–∞–ø–∏—Å—ñ–≤")
            
            if self.config.skip_existing:
                # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ —Ö–µ—à—ñ
                self.logger.info("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤...")
                self._load_existing_hashes()
        else:
            self.logger.info(f"üî® –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó '{self.config.collection_name}'")
            
            self.client.create_collection(
                collection_name=self.config.collection_name,
                vectors_config=models.VectorParams(
                    size=self.config.embedding_dim,
                    distance=models.Distance.COSINE,
                    on_disk=True  # –î–ª—è –≤–µ–ª–∏–∫–∏—Ö –æ–±'—î–º—ñ–≤
                ),
                # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                optimizers_config=models.OptimizersConfigDiff(
                    default_segment_number=4,
                    max_segment_size=2000000,
                    memmap_threshold=100000,
                    indexing_threshold=999999999,  # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é
                    flush_interval_sec=300,
                    max_optimization_threads=2
                ),
                # HNSW –≤—ñ–¥–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                hnsw_config=models.HnswConfigDiff(
                    m=0,
                    ef_construct=100,
                    full_scan_threshold=999999999,
                    max_indexing_threads=0,
                    on_disk=True
                ),
                shard_number=1,
                replication_factor=1
            )
            
            self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞")
    
    def _load_existing_hashes(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ö–µ—à—ñ–≤ —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤"""
        offset = None
        while True:
            try:
                records, offset = self.client.scroll(
                    collection_name=self.config.collection_name,
                    offset=offset,
                    limit=10000,
                    with_payload=["content_hash"],
                    with_vectors=False
                )
                
                for record in records:
                    if record.payload and 'content_hash' in record.payload:
                        self.hash_cache.add(record.payload['content_hash'])
                
                if not offset:
                    break
                    
            except Exception as e:
                self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ö–µ—à—ñ–≤: {e}")
                break
        
        self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.hash_cache):,} —Ö–µ—à—ñ–≤")
    
    def generate_content_hash(self, record: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ —Ö–µ—à—É –¥–ª—è –∑–∞–ø–∏—Å—É"""
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å—ñ –≤–∞–∂–ª–∏–≤—ñ –ø–æ–ª—è –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ
        key_parts = []
        
        # –û—Å–Ω–æ–≤–Ω—ñ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
        for field in ['F_TENDERNUMBER', 'EDRPOU', 'F_ITEMNAME', 'OWNER_NAME', 
                     'F_INDUSTRYNAME', 'CPV', 'ITEM_BUDGET', 'WON', 'ID']:
            value = record.get(field, '')
            key_parts.append(str(value))
        
        key_string = '|'.join(key_parts)
        return hashlib.sha256(key_string.encode('utf-8')).hexdigest()
    
    def create_embedding(self, text: str) -> Optional[np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        if not text:
            return None
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
        if self.config.use_cache and text in self.embedding_cache:
            self.stats['cache_hits'] += 1
            return self.embedding_cache[text]
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ–≤–∂–∏–Ω–∏
        if len(text) > self.config.max_text_length:
            text = text[:self.config.max_text_length]
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É
            embedding = self.embedding_model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False
            )
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
            if embedding is None or embedding.shape[0] != self.config.embedding_dim:
                return None
            
            # –ö–µ—à—É–≤–∞–Ω–Ω—è
            if self.config.use_cache and len(self.embedding_cache) < self.config.cache_size:
                self.embedding_cache[text] = embedding
            
            return embedding
            
        except Exception as e:
            self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É: {e}")
            return None
    
    def prepare_point(self, record: Dict, point_id: Optional[int] = None) -> Optional[models.PointStruct]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏"""
        # 1. –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        if self.config.validate_data:
            is_valid, reason = TenderDataValidator.validate_record(record)
            if not is_valid:
                self.stats['validation_failed'] += 1
                self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–≤–∞–ª—ñ–¥–Ω–∏–π): {json.dumps(record, ensure_ascii=False)} | –ü—Ä–∏—á–∏–Ω–∞: {reason}")
                return None
        
        # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        content_hash = self.generate_content_hash(record)
        if self.config.skip_existing and content_hash in self.hash_cache:
            self.stats['duplicates_skipped'] += 1
            self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª—å): {json.dumps(record, ensure_ascii=False)} | –ü—Ä–∏—á–∏–Ω–∞: –¥—É–±–ª—ñ–∫–∞—Ç –ø–æ content_hash")
            return None
        
        # 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—É
        embedding_text = TenderDataValidator.extract_text_for_embedding(record)
        if len(embedding_text) < self.config.min_text_length:
            self.stats['text_too_short'] += 1
            self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç): {json.dumps(record, ensure_ascii=False)} | –ü—Ä–∏—á–∏–Ω–∞: –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—É")
            return None
        
        # 4. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É
        embedding = self.create_embedding(embedding_text)
        if embedding is None:
            self.stats['embedding_failed'] += 1
            self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (embedding_failed): {json.dumps(record, ensure_ascii=False)} | –ü—Ä–∏—á–∏–Ω–∞: –Ω–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ –µ–º–±–µ–¥–∏–Ω–≥")
            return None
        
        # 5. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
        metadata = self._prepare_metadata(record, content_hash)
        
        # 6. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID
        if point_id is None:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —á–∞—Å—Ç–∏–Ω—É —Ö–µ—à—É —è–∫ ID
            point_id = int(content_hash[:15], 16) % (2**53)  # JavaScript safe integer
        
        # 7. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
        try:
            point = models.PointStruct(
                id=point_id,
                vector=embedding.tolist(),
                payload=metadata
            )
            
            # –î–æ–¥–∞—î–º–æ —Ö–µ—à –¥–æ –∫–µ—à—É
            self.hash_cache.add(content_hash)
            
            return point
            
        except Exception as e:
            self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏: {e}")
            self.stats['point_creation_failed'] += 1
            return None
    
    def _prepare_metadata(self, record: Dict, content_hash: str) -> Dict[str, Any]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è"""
        metadata = {
            # –°–∏—Å—Ç–µ–º–Ω—ñ –ø–æ–ª—è
            'content_hash': content_hash,
            'indexed_at': datetime.now().isoformat(),
            
            # –û—Å–Ω–æ–≤–Ω—ñ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
            'tender_number': record.get('F_TENDERNUMBER', ''),
            'edrpou': record.get('EDRPOU', ''),
            'original_id': str(record.get('ID', '')),
            
            # –ù–∞–∑–≤–∏
            'item_name': record.get('F_ITEMNAME', ''),
            'tender_name': record.get('F_TENDERNAME', ''),
            'detail_name': record.get('F_DETAILNAME', ''),
            
            # –£—á–∞—Å–Ω–∏–∫–∏
            'owner_name': record.get('OWNER_NAME', ''),
            'supplier_name': record.get('supp_name', ''),
            
            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
            'industry': record.get('F_INDUSTRYNAME', ''),
            'cpv': self._safe_int(record.get('CPV')),
            'code_ua': record.get('F_codeUA', ''),
            
            # –§—ñ–Ω–∞–Ω—Å–∏
            'budget': self._safe_float(record.get('ITEM_BUDGET')),
            'quantity': self._safe_float(record.get('F_qty')),
            'price': self._safe_float(record.get('F_price')),
            'currency': record.get('F_TENDERCURRENCY', 'UAH'),
            'currency_rate': self._safe_float(record.get('F_TENDERCURRENCYRATE'), 1.0),
            
            # –î–∞—Ç–∏
            'date_end': record.get('DATEEND', ''),
            'extraction_date': record.get('EXTRACTION_DATE', ''),
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç
            'won': bool(record.get('WON', False))
        }
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –ø—É—Å—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –º—ñ—Å—Ü—è
        metadata = {k: v for k, v in metadata.items() if v != '' and v is not None}
        
        return metadata
    
    def _safe_float(self, value: Any, default: float = 0.0) -> float:
        """–ë–µ–∑–ø–µ—á–Ω–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ float"""
        if value is None:
            return default
        try:
            if isinstance(value, str):
                value = value.replace(',', '.').strip()
            return float(value)
        except:
            return default
    
    def _safe_int(self, value: Any, default: int = 0) -> int:
        """–ë–µ–∑–ø–µ—á–Ω–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ int"""
        if value is None:
            return default
        try:
            return int(value)
        except:
            return default
    
    def upsert_batch(self, points: List[models.PointStruct], batch_num: int) -> int:
        """–í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –∑ –ø–æ–≤—Ç–æ—Ä–Ω–∏–º–∏ —Å–ø—Ä–æ–±–∞–º–∏"""
        if not points:
            return 0
        
        for attempt in range(self.config.max_retries):
            try:
                self.client.upsert(
                    collection_name=self.config.collection_name,
                    points=points,
                    wait=True  # –ß–µ–∫–∞—î–º–æ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è
                )
                
                self.logger.debug(f"‚úÖ –ë–∞—Ç—á {batch_num}: –≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(points)} —Ç–æ—á–æ–∫")
                return len(points)
                
            except Exception as e:
                self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –±–∞—Ç—á—É {batch_num}, —Å–ø—Ä–æ–±–∞ {attempt + 1}: {e}")
                
                if attempt < self.config.max_retries - 1:
                    time.sleep(self.config.retry_delay * (2 ** attempt))
                else:
                    self.logger.error(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–≤–∏—Ç–∏ –±–∞—Ç—á {batch_num}")
                    self.stats['failed_batches'] += 1
                    return 0
        
        return 0
    
    def load_file(self, file_path: str, max_records: Optional[int] = None) -> Dict[str, Any]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ —Ñ–∞–π–ª—É"""
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {file_path}")
        
        self.logger.info(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É: {file_path}")
        self.logger.info(f"üìä –†–æ–∑–º—ñ—Ä: {file_path.stat().st_size / (1024**3):.2f} GB")
        
        # –°–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ñ–∞–π–ª—É
        file_stats = defaultdict(int)
        start_time = datetime.now()
        
        # –õ—ñ—á–∏–ª—å–Ω–∏–∫ —Ä—è–¥–∫—ñ–≤
        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))
        self.logger.info(f"üìù –í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤: {total_lines:,}")
        
        # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É
        points_buffer = []
        batch_num = 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            pbar = tqdm(total=min(total_lines, max_records or total_lines), 
                       desc="–û–±—Ä–æ–±–∫–∞", unit="–∑–∞–ø–∏—Å—ñ–≤")
            
            for line_num, line in enumerate(f, 1):
                if max_records and line_num > max_records:
                    break
                
                file_stats['total_lines'] += 1
                
                try:
                    # –ü–∞—Ä—Å–∏–Ω–≥ JSON
                    record = json.loads(line.strip())
                    file_stats['parsed_ok'] += 1
                    
                    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏
                    point = self.prepare_point(record)
                    if point:
                        points_buffer.append(point)
                        file_stats['points_prepared'] += 1
                    else:
                        file_stats['points_skipped'] += 1
                        # –î–æ–¥–∞—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
                        self.logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Ä—è–¥–æ–∫ (points_skipped): {json.dumps(record, ensure_ascii=False)}")
                    
                    # –í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É
                    if len(points_buffer) >= self.config.batch_size:
                        batch_num += 1
                        inserted = self.upsert_batch(points_buffer, batch_num)
                        file_stats['points_inserted'] += inserted
                        file_stats['batches_sent'] += 1
                        
                        points_buffer.clear()
                        
                        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
                        if batch_num % 10 == 0:
                            elapsed = (datetime.now() - start_time).total_seconds()
                            speed = file_stats['points_inserted'] / elapsed if elapsed > 0 else 0
                            pbar.set_postfix({
                                'inserted': f"{file_stats['points_inserted']:,}",
                                'speed': f"{speed:.0f}/s"
                            })
                    
                except json.JSONDecodeError as e:
                    file_stats['json_errors'] += 1
                    self.logger.debug(f"JSON –ø–æ–º–∏–ª–∫–∞ –≤ —Ä—è–¥–∫—É {line_num}: {e}")
                except Exception as e:
                    self.logger.warning(f"–Ü–Ω—à–∞ –ø–æ–º–∏–ª–∫–∞ –≤ —Ä—è–¥–∫—É {line_num}: {e}")
                    file_stats['other_errors'] += 1
                    self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –≤ —Ä—è–¥–∫—É {line_num}: {e}")
                
                pbar.update(1)
                
                # –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
                if line_num % 50000 == 0:
                    gc.collect()
        
        # –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á
        if points_buffer:
            batch_num += 1
            inserted = self.upsert_batch(points_buffer, batch_num)
            file_stats['points_inserted'] += inserted
            file_stats['batches_sent'] += 1
        
        pbar.close()
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        file_stats['processing_time'] = (datetime.now() - start_time).total_seconds()
        file_stats['final_db_size'] = self.client.get_collection(self.config.collection_name).points_count
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        for key, value in file_stats.items():
            self.stats[key] += value
        
        return dict(file_stats)
    
    def print_report(self, file_stats: Dict[str, Any]):
        """–í–∏–≤—ñ–¥ –∑–≤—ñ—Ç—É –ø–æ —Ñ–∞–π–ª—É"""
        print("\n" + "="*60)
        print("üìä –ó–í–Ü–¢ –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø")
        print("="*60)
        
        print(f"\nüìà –û–±—Ä–æ–±–∫–∞:")
        print(f"   ‚Ä¢ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Ä—è–¥–∫—ñ–≤: {file_stats.get('total_lines', 0):,}")
        print(f"   ‚Ä¢ –†–æ–∑–ø–∞—Ä—Å–µ–Ω–æ JSON: {file_stats.get('parsed_ok', 0):,}")
        print(f"   ‚Ä¢ –ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ —Ç–æ—á–æ–∫: {file_stats.get('points_prepared', 0):,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª—ñ–∫–∞—Ç–∏/–Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ): {file_stats.get('points_skipped', 0):,}")
        
        print(f"\nüíæ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è:")
        print(f"   ‚Ä¢ –í—Å—Ç–∞–≤–ª–µ–Ω–æ —Ç–æ—á–æ–∫: {file_stats.get('points_inserted', 0):,}")
        print(f"   ‚Ä¢ –í—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –±–∞—Ç—á—ñ–≤: {file_stats.get('batches_sent', 0):,}")
        print(f"   ‚Ä¢ –†–æ–∑–º—ñ—Ä –ë–î: {file_stats.get('final_db_size', 0):,}")
        
        print(f"\n‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∏:")
        print(f"   ‚Ä¢ JSON –ø–æ–º–∏–ª–∫–∏: {file_stats.get('json_errors', 0):,}")
        print(f"   ‚Ä¢ –Ü–Ω—à—ñ –ø–æ–º–∏–ª–∫–∏: {file_stats.get('other_errors', 0):,}")
        
        print(f"\n‚è±Ô∏è –ü—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å:")
        processing_time = file_stats.get('processing_time', 0)
        print(f"   ‚Ä¢ –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {processing_time:.1f} —Å–µ–∫")
        
        points_inserted = file_stats.get('points_inserted', 0)
        if processing_time > 0 and points_inserted > 0:
            speed = points_inserted / processing_time
            print(f"   ‚Ä¢ –®–≤–∏–¥–∫—ñ—Å—Ç—å: {speed:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
        
        # –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
        total_lines = file_stats.get('total_lines', 0)
        if total_lines > 0 and points_inserted > 0:
            efficiency = points_inserted / total_lines * 100
            print(f"   ‚Ä¢ –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å: {efficiency:.1f}%")
        
        print("="*60)
    
    def verify_collection(self) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            info = self.client.get_collection(self.config.collection_name)
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫
            test_vector = [0.1] * self.config.embedding_dim
            start = time.time()
            results = self.client.search(
                collection_name=self.config.collection_name,
                query_vector=test_vector,
                limit=1
            )
            search_time = time.time() - start
            
            return {
                'status': 'OK',
                'points_count': info.points_count,
                'segments_count': info.segments_count if hasattr(info, 'segments_count') else 0,
                'search_time_ms': search_time * 1000,
                'collection_status': info.status
            }
            
        except Exception as e:
            return {
                'status': 'ERROR',
                'error': str(e)
            }
    
    def close(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –∑'—î–¥–Ω–∞–Ω—å —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è"""
        self.logger.info("\nüèÅ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏")
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞
        verification = self.verify_collection()
        self.logger.info(f"üìä –§—ñ–Ω–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω –ë–î: {verification}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_file = f"loader_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                'config': self.config.__dict__,
                'stats': dict(self.stats),
                'verification': verification,
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üíæ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ {stats_file}")
        
        # –û—á–∏—â–µ–Ω–Ω—è
        self.embedding_cache.clear()
        self.hash_cache.clear()
        gc.collect()


def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Standalone Vector Database Loader for Tenders",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:

1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É:
   python vector_loader.py data.jsonl

2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Ö —Ñ–∞–π–ª—ñ–≤:
   python vector_loader.py file1.jsonl file2.jsonl file3.jsonl

3. –¢–µ—Å—Ç–æ–≤–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (–ø–µ—Ä—à—ñ 10000 –∑–∞–ø–∏—Å—ñ–≤):
   python vector_loader.py --test data.jsonl

4. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ custom –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
   python vector_loader.py --batch-size 2000 --collection my_tenders data.jsonl

5. –ü–æ–≤–Ω–µ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (–≤–∏–¥–∞–ª–∏—Ç–∏ —ñ—Å–Ω—É—é—á—É –∫–æ–ª–µ–∫—Ü—ñ—é):
   python vector_loader.py --recreate data.jsonl
        """
    )
    
    # –ê—Ä–≥—É–º–µ–Ω—Ç–∏
    parser.add_argument('files', nargs='+', help='JSONL —Ñ–∞–π–ª–∏ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è')
    parser.add_argument('--host', default='localhost', help='Qdrant host')
    parser.add_argument('--port', type=int, default=6333, help='Qdrant port')
    parser.add_argument('--collection', default='tender_vectors', help='–ù–∞–∑–≤–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó')
    parser.add_argument('--model', default='sentence-transformers/paraphrase-multilingual-mpnet-base-v2', 
                       help='–ú–æ–¥–µ–ª—å –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–±—Ä–æ–±–∫–∏
    parser.add_argument('--batch-size', type=int, default=1000, help='–†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É')
    parser.add_argument('--max-records', type=int, help='–ú–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏')
    parser.add_argument('--test', action='store_true', help='–¢–µ—Å—Ç–æ–≤–∏–π —Ä–µ–∂–∏–º (10000 –∑–∞–ø–∏—Å—ñ–≤)')
    
    # –û–ø—Ü—ñ—ó
    parser.add_argument('--no-cache', action='store_true', help='–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∫–µ—à –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤')
    parser.add_argument('--no-validation', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö')
    parser.add_argument('--force', action='store_true', help='–Ü–≥–Ω–æ—Ä—É–≤–∞—Ç–∏ —ñ—Å–Ω—É—é—á—ñ –∑–∞–ø–∏—Å–∏')
    parser.add_argument('--recreate', action='store_true', help='–í–∏–¥–∞–ª–∏—Ç–∏ —Ç–∞ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–ª–µ–∫—Ü—ñ—é –∑–∞–Ω–æ–≤–æ')
    
    # –õ–æ–≥—É–≤–∞–Ω–Ω—è
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                       default='INFO', help='–†—ñ–≤–µ–Ω—å –ª–æ–≥—É–≤–∞–Ω–Ω—è')
    parser.add_argument('--log-file', default='vector_loader.log', help='–§–∞–π–ª –ª–æ–≥—ñ–≤')
    
    args = parser.parse_args()
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    config = LoaderConfig(
        qdrant_host=args.host,
        qdrant_port=args.port,
        collection_name=args.collection,
        model_name=args.model,
        batch_size=args.batch_size,
        use_cache=not args.no_cache,
        validate_data=not args.no_validation,
        skip_existing=not args.force,
        log_level=args.log_level,
        log_file=args.log_file
    )
    
    # –¢–µ—Å—Ç–æ–≤–∏–π —Ä–µ–∂–∏–º
    max_records = args.max_records
    if args.test:
        max_records = 20000
        print("üß™ –¢–ï–°–¢–û–í–ò–ô –†–ï–ñ–ò–ú: –æ–±—Ä–æ–±–∫–∞ –ø–µ—Ä—à–∏—Ö 10,000 –∑–∞–ø–∏—Å—ñ–≤")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á–∞
    loader = VectorDBLoader(config)
    
    try:
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
        loader.initialize()
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if args.recreate:
            response = input(f"\n‚ö†Ô∏è  –í–∏–¥–∞–ª–∏—Ç–∏ –∫–æ–ª–µ–∫—Ü—ñ—é '{config.collection_name}'? (y/n): ")
            if response.lower() == 'y':
                try:
                    loader.client.delete_collection(config.collection_name)
                    loader.logger.info(f"üóëÔ∏è –ö–æ–ª–µ–∫—Ü—ñ—è '{config.collection_name}' –≤–∏–¥–∞–ª–µ–Ω–∞")
                    # –ü–µ—Ä–µ—Å—Ç–≤–æ—Ä—é—î–º–æ
                    loader._init_collection()
                except Exception as e:
                    loader.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è: {e}")
        
        # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
        total_stats = defaultdict(int)
        
        for file_path in args.files:
            print(f"\n{'='*60}")
            print(f"üìÅ –§–∞–π–ª: {file_path}")
            print(f"{'='*60}")
            
            try:
                file_stats = loader.load_file(file_path, max_records)
                loader.print_report(file_stats)
                
                # –ê–≥—Ä–µ–≥–∞—Ü—ñ—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                for key, value in file_stats.items():
                    if isinstance(value, (int, float)):
                        total_stats[key] = total_stats.get(key, 0) + value
                
            except Exception as e:
                loader.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—É {file_path}: {e}")
                import traceback
                loader.logger.debug(traceback.format_exc())
        
        # –§—ñ–Ω–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
        if len(args.files) > 1:
            print("\n" + "="*60)
            print("üìä –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
            print("="*60)
            print(f"\nüìÅ –û–±—Ä–æ–±–ª–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤: {len(args.files)}")
            print(f"üìù –í—Å—å–æ–≥–æ —Ä—è–¥–∫—ñ–≤: {total_stats.get('total_lines', 0):,}")
            print(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å—ñ–≤: {total_stats.get('points_inserted', 0):,}")
            print(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {total_stats.get('processing_time', 0):.1f} —Å–µ–∫")
            
            processing_time = total_stats.get('processing_time', 0)
            points_inserted = total_stats.get('points_inserted', 0)
            if processing_time > 0 and points_inserted > 0:
                total_speed = points_inserted / processing_time
                print(f"üöÄ –°–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å: {total_speed:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
        
    except KeyboardInterrupt:
        loader.logger.warning("\n‚ö†Ô∏è –ü–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        loader.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        loader.logger.debug(traceback.format_exc())
    finally:
        loader.close()


if __name__ == "__main__":
    main()