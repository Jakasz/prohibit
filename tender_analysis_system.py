import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Set
import logging
from pathlib import Path
from collections import defaultdict, Counter
import pickle
import warnings
warnings.filterwarnings('ignore')

# ML —Ç–∞ NLP –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import joblib

# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
from qdrant_client import QdrantClient
from qdrant_client.http import models
from feature_extractor import FeatureExtractor

class TenderAnalysisSystem:
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤
    
    –ö–æ–æ—Ä–¥–∏–Ω—É—î —Ä–æ–±–æ—Ç—É –≤—Å—ñ—Ö –ø—ñ–¥—Å–∏—Å—Ç–µ–º:
    - –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—é —Ç–∞ –∞–Ω–∞–ª—ñ–∑
    - –í–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –¥–∞–Ω–∏—Ö
    - –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
    - –ê–Ω–∞–ª—ñ—Ç–∏–∫—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó
    """
    
    def __init__(self, 
                 model_path: str = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                 categories_file: Optional[str] = None,
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333):
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –æ—Å–Ω–æ–≤–Ω–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
        self.logger.info("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è TenderAnalysisSystem...")
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤
        self.embedding_model = SentenceTransformer(model_path)
        self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–æ–¥–µ–ª—å –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤: {model_path}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
        self.categories_manager = None  # –ë—É–¥–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø—ñ–∑–Ω—ñ—à–µ
        self.categories_file = categories_file
        
        # –ü—ñ–¥—Å–∏—Å—Ç–µ–º–∏ (–±—É–¥—É—Ç—å —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –ø—ñ–∑–Ω—ñ—à–µ)
        self.vector_db = None
        self.predictor = None
        self.competition_analyzer = None
        self.supplier_profiler = None
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Qdrant
        self.qdrant_config = {
            'host': qdrant_host,
            'port': qdrant_port
        }
        
        # –°—Ç–∞–Ω —Å–∏—Å—Ç–µ–º–∏
        self.is_initialized = False
        self.is_trained = False
        self.last_update = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º–∏
        self.system_metrics = {
            'total_tenders': 0,
            'total_suppliers': 0,
            'total_categories': 0,
            'model_performance': {},
            'last_training_date': None,
            'vector_db_size': 0
        }
    
    def initialize_system(self) -> bool:
        """
        –ü–æ–≤–Ω–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –ø—ñ–¥—Å–∏—Å—Ç–µ–º
        """
        try:
            self.logger.info("üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—ñ–¥—Å–∏—Å—Ç–µ–º...")
            
            # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            from category_manager import CategoryManager  # –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞–ª—ñ
            self.categories_manager = CategoryManager(self.categories_file)
            
            if Path("category_mappings.json").exists():            
                self.categories_manager.load_category_mappings("category_mappings.json")
                self.logger.info("‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–∞–ø–ø—ñ–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π")


            # 2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
            from vector_database import TenderVectorDB  # –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞–ª—ñ
            self.vector_db = TenderVectorDB(
                embedding_model=self.embedding_model,
                qdrant_host=self.qdrant_config['host'],
                qdrant_port=self.qdrant_config['port']
            )
            
            # 3. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä–∞ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤
            from supplier_profiler import SupplierProfiler  # –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞–ª—ñ
            self.supplier_profiler = SupplierProfiler(
                categories_manager=self.categories_manager,
                # embedding_model=self.embedding_model
            )
            
            # 4. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó
            from competition_analyzer import CompetitionAnalyzer  # –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞–ª—ñ
            self.competition_analyzer = CompetitionAnalyzer(
                categories_manager=self.categories_manager,
                vector_db=self.vector_db
            )
            
            # 5. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            from prediction_engine import PredictionEngine  # –ë—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–∞–ª—ñ
            self.predictor = PredictionEngine(
                supplier_profiler=self.supplier_profiler,
                competition_analyzer=self.competition_analyzer,
                categories_manager=self.categories_manager
            )
            # 6. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –µ–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –æ–∑–Ω–∞–∫
            feature_extractor = FeatureExtractor(
                categories_manager=self.categories_manager,
                competition_analyzer=self.competition_analyzer
            )
            self.predictor = PredictionEngine(
                # feature_extractor=feature_extractor
                supplier_profiler = self.supplier_profiler,
                competition_analyzer = self.competition_analyzer,
                categories_manager = self.categories_manager
            )
            
            
            self.is_initialized = True
            self.logger.info("‚úÖ –í—Å—ñ –ø—ñ–¥—Å–∏—Å—Ç–µ–º–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó: {e}")
            return False
    
    def load_and_process_data(self, 
                            historical_data: List[Dict],
                            update_mode: bool = False) -> Dict[str, Any]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        
        Args:
            historical_data: –°–ø–∏—Å–æ–∫ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ —Ç–µ–Ω–¥–µ—Ä—ñ–≤
            update_mode: True –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö –¥–∞–Ω–∏—Ö, False –¥–ª—è –ø–æ–≤–Ω–æ—ó –ø–µ—Ä–µ—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        """
        if not self.is_initialized:
            raise RuntimeError("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞. –í–∏–∫–ª–∏—á—Ç–µ initialize_system()")
        
        self.logger.info(f"üì• –û–±—Ä–æ–±–∫–∞ {len(historical_data)} –∑–∞–ø–∏—Å—ñ–≤ (—Ä–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è: {update_mode})")
        
        results = {
            'processed_records': len(historical_data),
            'new_suppliers': 0,
            'updated_suppliers': 0,
            'new_categories': 0,
            'vector_db_updates': 0,
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        try:
            # 1. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
            category_stats = self.categories_manager.process_historical_data(historical_data)
            results['new_categories'] = category_stats.get('new_categories', 0)
            
            # 2. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è/–æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤
            supplier_stats = self.supplier_profiler.build_profiles(
                historical_data, 
                update_mode=update_mode
            )
            results['new_suppliers'] = supplier_stats.get('new_profiles', 0)
            results['updated_suppliers'] = supplier_stats.get('updated_profiles', 0)
            
            # 3. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
            vector_stats = self.vector_db.index_tenders(
                historical_data, 
                update_mode=update_mode
            )
            results['vector_db_updates'] = vector_stats.get('indexed_count', 0)
            
            # 4. –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó
            self.competition_analyzer.update_competition_metrics(historical_data)
            
            # 5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
            self._update_system_metrics(historical_data)
            
            results['processing_time'] = (datetime.now() - start_time).total_seconds()
            self.last_update = datetime.now()
            
            self.logger.info(f"‚úÖ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {results['processing_time']:.2f} —Å–µ–∫")
            self.logger.info(f"üìä –ù–æ–≤—ñ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏: {results['new_suppliers']}")
            self.logger.info(f"üìä –û–Ω–æ–≤–ª–µ–Ω—ñ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∏: {results['updated_suppliers']}")
            self.logger.info(f"üìä –ó–∞–ø–∏—Å—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –ë–î: {results['vector_db_updates']}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö: {e}")
            raise
    
    def train_prediction_model(self, 
                             validation_split: float = 0.2,
                             cross_validation_folds: int = 5) -> Dict[str, Any]:
        """
        –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        """
        if not self.is_initialized:
            raise RuntimeError("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
        
        if self.system_metrics['total_tenders'] == 0:
            raise ValueError("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è. –°–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ")
        
        self.logger.info("üéØ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è...")
        
        try:
            # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            training_results = self.predictor.train_model(
                validation_split=validation_split,
                cv_folds=cross_validation_folds
            )
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —Å–∏—Å—Ç–µ–º–∏
            self.system_metrics['model_performance'] = training_results['performance_metrics']
            self.system_metrics['last_training_date'] = datetime.now().isoformat()
            
            self.is_trained = True
            
            self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
            self.logger.info(f"üìà AUC Score: {training_results['performance_metrics']['auc_score']:.4f}")
            
            return training_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {e}")
            raise
    
    def predict_tender_outcomes(self, 
                              tender_data: List[Dict],
                              include_competition_analysis: bool = True,
                              include_similar_tenders: bool = True) -> Dict[str, Any]:
        """
        –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ç–µ–Ω–¥–µ—Ä—ñ–≤
        
        Args:
            tender_data: –î–∞–Ω—ñ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            include_competition_analysis: –í–∫–ª—é—á–∏—Ç–∏ –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó
            include_similar_tenders: –í–∫–ª—é—á–∏—Ç–∏ –ø–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤
        """
        if not self.is_trained:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∞. –í–∏–∫–ª–∏—á—Ç–µ train_prediction_model()")
        
        self.logger.info(f"üîÆ –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è {len(tender_data)} –ø–æ–∑–∏—Ü—ñ–π...")
        
        results = {
            'predictions': {},
            'competition_analysis': {},
            'similar_tenders': {},
            'category_insights': {},
            'processing_time': 0
        }
        
        start_time = datetime.now()
        
        try:
            # 1. –û—Å–Ω–æ–≤–Ω—ñ –ø—Ä–æ–≥–Ω–æ–∑–∏
            predictions = self.predictor.predict_batch(tender_data)
            results['predictions'] = predictions
            
            # 2. –ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
            if include_competition_analysis:
                competition_data = {}
                for item in tender_data:
                    tender_num = item.get('F_TENDERNUMBER')
                    if tender_num:
                        competition_data[tender_num] = self.competition_analyzer.analyze_tender_competition(item)
                results['competition_analysis'] = competition_data
            
            # 3. –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
            if include_similar_tenders:
                similar_data = {}
                for item in tender_data:
                    tender_num = item.get('F_TENDERNUMBER')
                    if tender_num:
                        similar_data[tender_num] = self.vector_db.search_similar_tenders(item)
                results['similar_tenders'] = similar_data
            
            # 4. –ê–Ω–∞–ª—ñ–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
            category_insights = self._analyze_predictions_by_category(tender_data, predictions)
            results['category_insights'] = category_insights
            
            results['processing_time'] = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {results['processing_time']:.2f} —Å–µ–∫")
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è: {e}")
            raise
    
    def search_tenders(self, 
                      query: str, 
                      filters: Optional[Dict] = None,
                      limit: int = 20) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ
        """
        if not self.is_initialized:
            raise RuntimeError("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
        
        return self.vector_db.search_by_text(
            query=query,
            filters=filters,
            limit=limit
        )
    
    def get_supplier_analytics(self, 
                             edrpou: str,
                             include_competition: bool = True) -> Dict[str, Any]:
        """
        –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –ø–æ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—É
        """
        if not self.is_initialized:
            raise RuntimeError("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
        
        # –ë–∞–∑–æ–≤–∏–π –ø—Ä–æ—Ñ—ñ–ª—å
        profile = self.supplier_profiler.get_supplier_profile(edrpou)
        if not profile:
            return {'error': '–ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ'}
        
        result = {
            'profile': profile,
            'competition_metrics': {},
            'category_performance': {},
            'recommendations': []
        }
        
        # –ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—ó
        if include_competition:
            result['competition_metrics'] = self.competition_analyzer.get_supplier_competition_metrics(edrpou)
        
        # –ê–Ω–∞–ª—ñ–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö
        result['category_performance'] = self.categories_manager.get_supplier_category_performance(edrpou)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        result['recommendations'] = self._generate_supplier_recommendations(profile)
        
        return result
    
    def get_category_analytics(self, category_id: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
        """
        if not self.is_initialized:
            raise RuntimeError("–°–∏—Å—Ç–µ–º–∞ –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
        
        return {
            'category_info': self.categories_manager.get_category_info(category_id),
            'competition_metrics': self.competition_analyzer.get_category_competition_metrics(category_id),
            'top_suppliers': self.supplier_profiler.get_top_suppliers_in_category(category_id),
            'market_trends': self._analyze_category_trends(category_id)
        }
    
    def update_incremental(self, new_data: List[Dict]) -> Dict[str, Any]:
        """
        –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ –Ω–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏
        """
        self.logger.info(f"üîÑ –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ {len(new_data)} –∑–∞–ø–∏—Å—ñ–≤...")
        
        # –û–±—Ä–æ–±–∫–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        processing_results = self.load_and_process_data(new_data, update_mode=True)
        
        # –ß–∞—Å—Ç–∫–æ–≤–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
        if len(new_data) > 100:  # –ü–µ—Ä–µ–Ω–∞—á–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–Ω–∞—á–Ω–∏—Ö –æ–Ω–æ–≤–ª–µ–Ω–Ω—è—Ö
            self.logger.info("üìö –ß–∞—Å—Ç–∫–æ–≤–µ –ø–µ—Ä–µ–Ω–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
            training_results = self.train_prediction_model()
            processing_results['model_retrained'] = True
            processing_results['new_performance'] = training_results['performance_metrics']
        else:
            processing_results['model_retrained'] = False
        
        return processing_results
    
    def export_system_state(self) -> Dict[str, Any]:
        """
        –ï–∫—Å–ø–æ—Ä—Ç –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Å—Ç–∞–Ω—É —Å–∏—Å—Ç–µ–º–∏
        """
        return {
            'system_info': {
                'version': '1.0.0',
                'initialized': self.is_initialized,
                'trained': self.is_trained,
                'last_update': self.last_update.isoformat() if self.last_update else None
            },
            'metrics': self.system_metrics,
            'categories_count': len(self.categories_manager.categories) if self.categories_manager else 0,
            'suppliers_count': len(self.supplier_profiler.profiles) if self.supplier_profiler else 0,
            'vector_db_size': self.vector_db.get_collection_size() if self.vector_db else 0
        }
    
    def save_system(self, filepath: str):
        """
        –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Å—Ç–∞–Ω—É —Å–∏—Å—Ç–µ–º–∏
        """
        self.logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ –¥–æ {filepath}...")
        
        system_data = {
            'system_state': self.export_system_state(),
            'categories_manager': self.categories_manager.export_state() if self.categories_manager else None,
            'supplier_profiler': self.supplier_profiler.export_state() if self.supplier_profiler else None,
            'predictor': self.predictor.export_state() if self.predictor else None,
            'competition_analyzer': self.competition_analyzer.export_state() if self.competition_analyzer else None
        }
        
        joblib.dump(system_data, filepath)
        self.logger.info("‚úÖ –°–∏—Å—Ç–µ–º—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
    
    def load_system(self, filepath: str):
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ —Å—Ç–∞–Ω—É —Å–∏—Å—Ç–µ–º–∏
        """
        self.logger.info(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ –∑ {filepath}...")
        
        system_data = joblib.load(filepath)
        
        # –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞–Ω—É –ø—ñ–¥—Å–∏—Å—Ç–µ–º
        if system_data.get('categories_manager'):
            self.categories_manager.load_state(system_data['categories_manager'])
        
        if system_data.get('supplier_profiler'):
            self.supplier_profiler.load_state(system_data['supplier_profiler'])
        
        if system_data.get('predictor'):
            self.predictor.load_state(system_data['predictor'])
            
        if system_data.get('competition_analyzer'):
            self.competition_analyzer.load_state(system_data['competition_analyzer'])
        
        # –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å—Ç–∞–Ω—É
        system_state = system_data.get('system_state', {})
        self.is_initialized = system_state.get('initialized', False)
        self.is_trained = system_state.get('trained', False)
        self.system_metrics = system_state.get('metrics', {})
        
        last_update_str = system_state.get('last_update')
        if last_update_str:
            self.last_update = datetime.fromisoformat(last_update_str)
        
        self.logger.info("‚úÖ –°–∏—Å—Ç–µ–º—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    
    def create_vector_db_from_jsonl(self, jsonl_path: str, collection_name: Optional[str] = None, batch_size: int = 100) -> Dict[str, Any]:
        """
        –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –∑ —Ñ–∞–π–ª—É JSONL –∑ —É—Å—ñ–º–∞ –ø–æ–ª—è–º–∏
        Args:
            jsonl_path: —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É JSONL –∑ —Ç–µ–Ω–¥–µ—Ä–∞–º–∏
            collection_name: –Ω–∞–∑–≤–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó Qdrant (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        """
        self.logger.info(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ {jsonl_path} –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏...")
        if not Path(jsonl_path).exists():
            self.logger.error(f"‚ùå –§–∞–π–ª {jsonl_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            raise FileNotFoundError(f"–§–∞–π–ª {jsonl_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ JSONL
        historical_data = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    record = json.loads(line)
                    historical_data.append(record)
                except Exception as e:
                    self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON —É —Ä—è–¥–∫—É {line_num}: {e}")

        self.logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(historical_data)} –∑–∞–ø–∏—Å—ñ–≤ –∑ {jsonl_path}")

        # –Ø–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ, —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É –∫–æ–ª–µ–∫—Ü—ñ—é –∑ –Ω–æ–≤–æ—é –Ω–∞–∑–≤–æ—é
        if collection_name:
            from .vector_database import TenderVectorDB
            self.vector_db = TenderVectorDB(
                embedding_model=self.embedding_model,
                qdrant_host=self.qdrant_config['host'],
                qdrant_port=self.qdrant_config['port'],
                collection_name=collection_name
            )
            self.logger.info(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–æ –Ω–æ–≤—É –∫–æ–ª–µ–∫—Ü—ñ—é Qdrant: {collection_name}")
        elif self.vector_db is None:
            from .vector_database import TenderVectorDB
            self.vector_db = TenderVectorDB(
                embedding_model=self.embedding_model,
                qdrant_host=self.qdrant_config['host'],
                qdrant_port=self.qdrant_config['port']
            )

        # –ü–µ—Ä–µ–¥–∞—î–º–æ –º–µ–Ω–µ–¥–∂–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —É –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É (–¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó)
        if hasattr(self.vector_db, 'category_manager') and self.vector_db.category_manager is None:
            self.vector_db.category_manager = self.categories_manager

        # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ
        stats = self.vector_db.index_tenders(
            historical_data=historical_data,
            update_mode=False,
            batch_size=batch_size
        )

        self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ —Å—Ç–≤–æ—Ä–µ–Ω–∞. –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {stats.get('indexed_count', 0)} –∑–∞–ø–∏—Å—ñ–≤")
        return stats
    
    def process_large_dataset(self, jsonl_path: str, batch_size: int = 1000, max_records: int = None) -> Dict[str, Any]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞ –æ–±—Ä–æ–±–∫–∞ –≤–µ–ª–∏–∫–æ–≥–æ JSONL —Ñ–∞–π–ª—É
        
        Args:
            jsonl_path: —à–ª—è—Ö –¥–æ JSONL —Ñ–∞–π–ª—É
            batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            max_records: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ (–¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)
        """
        self.logger.info(f"üìÇ –ü–æ—Ç–æ–∫–æ–≤–∞ –æ–±—Ä–æ–±–∫–∞ {jsonl_path}")
        
        if not Path(jsonl_path).exists():
            raise FileNotFoundError(f"–§–∞–π–ª {jsonl_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        
        stats = {
            'total_read': 0,
            'total_indexed': 0,
            'total_errors': 0,
            'batches_processed': 0
        }
        
        batch_data = []
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if max_records and line_num > max_records:
                    break
                
                try:
                    record = json.loads(line.strip())
                    batch_data.append(record)
                    stats['total_read'] += 1
                    
                    if len(batch_data) >= batch_size:
                        # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—É
                        self._process_batch(batch_data, stats)
                        batch_data = []
                        
                        # –ü—Ä–æ–≥—Ä–µ—Å
                        if stats['total_read'] % 10000 == 0:
                            self.logger.info(f"–û–±—Ä–æ–±–ª–µ–Ω–æ {stats['total_read']:,} –∑–∞–ø–∏—Å—ñ–≤")
                            
                except Exception as e:
                    self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤ —Ä—è–¥–∫—É {line_num}: {e}")
                    stats['total_errors'] += 1
        
        # –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á
        if batch_data:
            self._process_batch(batch_data, stats)
        
        return stats

    def _process_batch(self, batch_data: List[Dict], stats: Dict):
        """–û–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á—É –¥–∞–Ω–∏—Ö"""
        try:
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤
            suppliers_by_edrpou = defaultdict(list)
            for item in batch_data:
                edrpou = item.get('EDRPOU')
                if edrpou:
                    suppliers_by_edrpou[edrpou].append(item)
            
            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—ñ–≤
            for edrpou, items in suppliers_by_edrpou.items():
                self.supplier_profiler.update_profile(edrpou, items)
            
            # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ
            index_results = self.vector_db.index_tenders(
                batch_data,
                update_mode=True,
                batch_size=100
            )
            
            stats['total_indexed'] += index_results['indexed_count']
            stats['total_errors'] += index_results['error_count']
            stats['batches_processed'] += 1
            
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –±–∞—Ç—á—É: {e}")
            stats['total_errors'] += len(batch_data)




    # –ü—Ä–∏–≤–∞—Ç–Ω—ñ –¥–æ–ø–æ–º—ñ–∂–Ω—ñ –º–µ—Ç–æ–¥–∏
    
    def _update_system_metrics(self, data: List[Dict]):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        self.system_metrics['total_tenders'] = len(set(item.get('F_TENDERNUMBER') for item in data))
        self.system_metrics['total_suppliers'] = len(set(item.get('EDRPOU') for item in data))
        self.system_metrics['vector_db_size'] = len(data)
    
    def _analyze_predictions_by_category(self, tender_data: List[Dict], predictions: Dict) -> Dict:
        """–ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö"""
        category_stats = defaultdict(lambda: {'count': 0, 'avg_probability': 0, 'probabilities': []})
        
        for item in tender_data:
            tender_num = item.get('F_TENDERNUMBER')
            if tender_num and tender_num in predictions:
                categories = self.categories_manager.categorize_item(item.get('F_ITEMNAME', ''))
                prob = predictions[tender_num]
                
                for category in categories:
                    category_stats[category]['count'] += 1
                    category_stats[category]['probabilities'].append(prob)
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–µ—Ä–µ–¥–Ω—ñ—Ö –∑–Ω–∞—á–µ–Ω—å
        for category, stats in category_stats.items():
            if stats['probabilities']:
                stats['avg_probability'] = np.mean(stats['probabilities'])
                stats['std_probability'] = np.std(stats['probabilities'])
                del stats['probabilities']  # –í–∏–¥–∞–ª—è—î–º–æ —Å–∏—Ä—ñ –¥–∞–Ω—ñ
        
        return dict(category_stats)
    
    def _generate_supplier_recommendations(self, profile: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–ª—è –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞"""
        recommendations = []
        
        win_rate = profile.get('position_win_rate', 0)
        if win_rate < 0.3:
            recommendations.append("–†–æ–∑–≥–ª—è–Ω—É—Ç–∏ —É—á–∞—Å—Ç—å —É –º–µ–Ω—à –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö")
        
        if len(profile.get('item_categories', {})) < 3:
            recommendations.append("–†–æ–∑—à–∏—Ä–∏—Ç–∏ –∞—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç –ø–æ—Å–ª—É–≥/—Ç–æ–≤–∞—Ä—ñ–≤")
        
        if profile.get('recent_performance', 0) < profile.get('position_win_rate', 0):
            recommendations.append("–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–∏—á–∏–Ω–∏ –∑–Ω–∏–∂–µ–Ω–Ω—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ")
        
        return recommendations
    
    def _analyze_category_trends(self, category_id: str) -> Dict:
        """–ê–Ω–∞–ª—ñ–∑ —Ç—Ä–µ–Ω–¥—ñ–≤ —É –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–µ–Ω–¥—ñ–≤
        return {
            'trend': 'stable',
            'growth_rate': 0.0,
            'seasonal_patterns': []
        }