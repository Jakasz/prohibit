import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import numpy as np
import hashlib
from tqdm import tqdm
import sys
import os
from sentence_transformers import SentenceTransformer
# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import ResponseHandlingException, UnexpectedResponse


class TenderVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω—å
    
    –§—É–Ω–∫—Ü—ñ—ó:
    - –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
    - –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è
    - –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    - –ê–Ω–∞–ª—ñ–∑ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    """
    
    def __init__(self, 
                 embedding_model,
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333,
                 collection_name: str = "tender_vectors"):
        
        self.logger = logging.getLogger(__name__)
        self.embedding_model = embedding_model
        import transformers
        import datasets
        transformers.logging.set_verbosity_error()
        datasets.logging.set_verbosity_error()
        if hasattr(self.embedding_model, 'tokenizer') and self.embedding_model.tokenizer:
            self.embedding_model.tokenizer.verbose = False

        self.collection_name = collection_name
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant
        try:
            self.client = QdrantClient(host=qdrant_host, port=qdrant_port)
            self.logger.info(f"‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–æ –¥–æ Qdrant: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant: {e}")
            raise
        

        
        # –ö–µ—à –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å
        self.embedding_cache = {}
        self.hash_cache = {}  # –î–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_indexed': 0,
            'total_updated': 0,
            'total_searches': 0,
            'last_index_time': None,
            'last_update_time': None
        }
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        self._init_collection()

    def _create_minimal_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ú–Ü–ù–Ü–ú–ê–õ–¨–ù–ò–• —ñ–Ω–¥–µ–∫—Å—ñ–≤ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–ª—ñ–≤"""
        minimal_indexes = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("won", models.PayloadSchemaType.BOOL),
        ]
        
        for field_name, field_type in minimal_indexes:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type,
                    wait=False
                )
            except Exception as e:
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")



    def _init_collection(self):        
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"""
        collections = self.client.get_collections().collections
        collection_names = [col.name for col in collections]

        if self.collection_name in collection_names:
            collection_info = self.client.get_collection(self.collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î")
            self.stats['total_indexed'] = collection_info.points_count
        else:
            self.logger.info(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{self.collection_name}' –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó...")
            try:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=768,
                        distance=models.Distance.COSINE,
                        on_disk=True
                    ),
                    optimizers_config=models.OptimizersConfigDiff(
                        default_segment_number=1,  # –ú—ñ–Ω—ñ–º—É–º —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                        max_segment_size=2000000,  # –í–µ–ª–∏–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (2M –∑–∞–ø–∏—Å—ñ–≤)
                        memmap_threshold=50000,    # –ë—ñ–ª—å—à–∏–π –ø–æ—Ä—ñ–≥ –¥–ª—è memmap
                        indexing_threshold=999999999,  # üî• –í–Ü–î–ö–õ–Æ–ß–ê–Ñ–ú–û –Ü–ù–î–ï–ö–°–ê–¶–Ü–Æ
                        flush_interval_sec=120,    # –†—ñ–¥—à–µ —Å–∫–∏–¥–∞–Ω–Ω—è –Ω–∞ –¥–∏—Å–∫
                        max_optimization_threads=1
                    ),
                    hnsw_config=models.HnswConfigDiff(
                        m=0,  # üî• –í–Ü–î–ö–õ–Æ–ß–ê–Ñ–ú–û HNSW –Ü–ù–î–ï–ö–°
                        ef_construct=100,
                        full_scan_threshold=999999999,  # –ó–∞–≤–∂–¥–∏ full scan
                        max_indexing_threads=0,  # –ë–µ–∑ –ø–æ—Ç–æ–∫—ñ–≤ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
                        on_disk=True,
                        payload_m=16
                    ),
                    # ‚ùå –í–ò–ú–ö–ù–£–õ–ò quantization –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                    # quantization_config=None,
                    shard_number=1,
                    replication_factor=1
                )
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç—ñ–ª—å–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
                self._create_minimal_payload_indexes()
                self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞ –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è")
            except Exception as e:
                self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                raise

    def _create_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–ª—ñ–≤"""
        indexes_to_create = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("won", models.PayloadSchemaType.BOOL),
            ("industry", models.PayloadSchemaType.KEYWORD),
            # –ù–ï —ñ–Ω–¥–µ–∫—Å—É—î–º–æ F_ITEMNAME - —Ü–µ —Ç–µ–∫—Å—Ç–æ–≤–µ –ø–æ–ª–µ, –∑–∞–π–º–∞—î –±–∞–≥–∞—Ç–æ –º—ñ—Å—Ü—è
        ]
        
        for field_name, field_type in indexes_to_create:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type,
                    wait=False  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
                )
            except Exception as e:
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤"""
        if not text or not isinstance(text, str):
            return ""
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        text = text.lower().strip()
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤, –∞–ª–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–º—ñ—Å—Ç–æ–≤–Ω–∏—Ö
        text = re.sub(r'[^\w\s\-\.\(\)]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å –≤–∏–º—ñ—Ä—É
        units_mapping = {
            r'\b—à—Ç\.?\b': '—à—Ç—É–∫',
            r'\b–∫–≥\.?\b': '–∫—ñ–ª–æ–≥—Ä–∞–º', 
            r'\b–≥\.?\b': '–≥—Ä–∞–º',
            r'\b–ª\.?\b': '–ª—ñ—Ç—Ä',
            r'\b–º\.?\b': '–º–µ—Ç—Ä',
            r'\b—Å–º\.?\b': '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä',
            r'\b–º–º\.?\b': '–º—ñ–ª—ñ–º–µ—Ç—Ä'
        }
        
        for pattern, replacement in units_mapping.items():
            text = re.sub(pattern, replacement, text)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _generate_content_hash(self, item: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ö–µ—à—É"""
        key_str = f"{item.get('F_TENDERNUMBER', '')}{item.get('EDRPOU', '')}{item.get('F_ITEMNAME', '')}{item.get('F_INDUSTRYNAME', '')}"
        return hashlib.md5(key_str.encode('utf-8')).hexdigest()[:16]  # –ö–æ—Ä–æ—Ç—à–∏–π —Ö–µ—à

    
    def _create_embedding(self, text: str) -> Optional[np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        if not text or not isinstance(text, str):
            return None
        
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        processed_text = self._preprocess_text(text)
        if not processed_text or len(processed_text) < 2:
            return None
        
        try:
            # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –≤–∏–≤—ñ–¥ –ø—Ä–∏ –µ–Ω–∫–æ–¥–∏–Ω–≥—É
            with open(os.devnull, 'w') as devnull:
                old_stdout = sys.stdout
                sys.stdout = devnull
                
                embedding = self.embedding_model.encode(
                    processed_text, 
                    show_progress_bar=False,
                    batch_size=1,
                    normalize_embeddings=True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
                )
                
                sys.stdout = old_stdout
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            if embedding is None:
                return None
            
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding)
            
            if embedding.shape[0] != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –µ–º–±–µ–¥–∏–Ω–≥—É: {embedding.shape[0]}")
                return None
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ NaN —Ç–∞ inf
            if np.isnan(embedding).any() or np.isinf(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å NaN –∞–±–æ inf")
                return None
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —è–∫—â–æ –Ω–µ –±—É–ª–∞ –∑—Ä–æ–±–ª–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            else:
                self.logger.debug(f"‚ö†Ô∏è –ù—É–ª—å–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
                return None
            
            # –ö–µ—à—É–≤–∞–Ω–Ω—è (–æ–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–µ—à—É)
            if len(self.embedding_cache) < 5000:
                self.embedding_cache[text] = embedding
            
            return embedding
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É: {e}")
            return None

    def _prepare_point(self, item: Dict, point_id: Optional[int] = None) -> Optional[models.PointStruct]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –≤ Qdrant –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é"""
        
        # ========== –í–ê–õ–Ü–î–ê–¶–Ü–Ø –í–•–Ü–î–ù–ò–• –î–ê–ù–ò–• ==========
        
        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ö –ø–æ–ª—ñ–≤
        tender_number = item.get('F_TENDERNUMBER', '')
        edrpou = item.get('EDRPOU', '')
        item_name = item.get('F_ITEMNAME', '')
        
        if not tender_number:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_TENDERNUMBER, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–ø–∏—Å")
            return None
        
        if not edrpou:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π EDRPOU –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return None
        
        if not item_name:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_ITEMNAME –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return None
        
        # 2. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—É
        tender_name = item.get('F_TENDERNAME', '')
        detail_name = item.get('F_DETAILNAME', '')
        combined_text = f"{item_name} {tender_name} {detail_name}".strip()
        
        if len(combined_text) < 3:
            self.logger.debug(f"‚ö†Ô∏è –ù–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(combined_text)} —Å–∏–º–≤–æ–ª—ñ–≤) –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return None
        
        # 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é
        try:
            embedding = self._create_embedding(combined_text)
            
            if embedding is None:
                self.logger.debug(f"‚ö†Ô∏è _create_embedding –ø–æ–≤–µ—Ä–Ω—É–≤ None –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if not isinstance(embedding, np.ndarray):
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –Ω–µ —î numpy array –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if embedding.shape[0] != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –µ–º–±–µ–¥–∏–Ω–≥—É: {embedding.shape[0]} –∑–∞–º—ñ—Å—Ç—å 768 –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if np.isnan(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å NaN –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if np.isinf(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å inf –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
                
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
        
        # ========== –°–¢–í–û–†–ï–ù–ù–Ø –ú–ï–¢–ê–î–ê–ù–ò–• ==========
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
            metadata = {
                # –û—Å–Ω–æ–≤–Ω—ñ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
                'tender_number': tender_number,
                'edrpou': edrpou,
                'owner_name': item.get('OWNER_NAME', ''),
                
                # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Ç–æ–≤–∞—Ä/–ø–æ—Å–ª—É–≥—É
                'item_name': item_name,
                'tender_name': tender_name,
                'detail_name': detail_name,
                
                # –ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫
                'supplier_name': item.get('supp_name', ''),
                
                # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
                'industry': item.get('F_INDUSTRYNAME', ''),
                'cpv': int(item.get('CPV', 0)) if item.get('CPV') else 0,
                
                # –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é
                'budget': 0.0,
                'quantity': 0.0,
                'price': 0.0,
                'currency': item.get('F_TENDERCURRENCY', 'UAH'),
                'currency_rate': 1.0,
                
                # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞ –¥–∞—Ç–∏
                'won': bool(item.get('WON', False)),
                'date_end': item.get('DATEEND', ''),
                'extraction_date': item.get('EXTRACTION_DATE', ''),
                
                # –°–∏—Å—Ç–µ–º–Ω—ñ –ø–æ–ª—è
                'original_id': item.get('ID', ''),
                'created_at': datetime.now().isoformat(),
                'content_hash': self._generate_content_hash(item)
            }
            
            # –ë–µ–∑–ø–µ—á–Ω–µ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
            try:
                budget = item.get('ITEM_BUDGET')
                if budget is not None and str(budget).strip():
                    metadata['budget'] = float(budget)
            except (ValueError, TypeError):
                pass
            
            try:
                quantity = item.get('F_qty')
                if quantity is not None and str(quantity).strip():
                    metadata['quantity'] = float(quantity)
            except (ValueError, TypeError):
                pass
            
            try:
                price = item.get('F_price')
                if price is not None and str(price).strip():
                    metadata['price'] = float(price)
            except (ValueError, TypeError):
                pass
            
            try:
                rate = item.get('F_TENDERCURRENCYRATE')
                if rate is not None and str(rate).strip():
                    metadata['currency_rate'] = float(rate)
            except (ValueError, TypeError):
                pass
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è
            group_key_parts = [
                str(edrpou),
                str(item.get('F_INDUSTRYNAME', '')),
                str(item_name),
                str(item.get('OWNER_NAME', '')),
                str(tender_name)
            ]
            
            if item.get('CPV'):
                group_key_parts.append(str(item.get('CPV')))
            
            metadata['group_key'] = '-'.join(group_key_parts)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if hasattr(self, 'category_manager') and self.category_manager:
                try:
                    categories = self.category_manager.categorize_item(item_name)
                    metadata['primary_category'] = categories[0][0] if categories else 'unknown'
                    metadata['all_categories'] = [cat[0] for cat in categories[:3]]
                    metadata['category_confidence'] = categories[0][1] if categories else 0.0
                except Exception as e:
                    self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è {tender_number}: {e}")
                    metadata['primary_category'] = 'unknown'
                    metadata['all_categories'] = []
                    metadata['category_confidence'] = 0.0
            else:
                metadata['primary_category'] = 'unknown'
                metadata['all_categories'] = []
                metadata['category_confidence'] = 0.0
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
        
        # ========== –°–¢–í–û–†–ï–ù–ù–Ø –¢–û–ß–ö–ò ==========
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID
            if point_id is None:
                point_id = int(hashlib.md5(metadata['content_hash'].encode()).hexdigest()[:8], 16)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
            point = models.PointStruct(
                id=point_id,
                vector=embedding.tolist(),
                payload=metadata
            )
            
            # –§—ñ–Ω–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–æ—á–∫–∏
            if not hasattr(point, 'id') or point.id is None:
                self.logger.debug(f"‚ö†Ô∏è –¢–æ—á–∫–∞ –Ω–µ –º–∞—î ID –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if not hasattr(point, 'vector') or not point.vector:
                self.logger.debug(f"‚ö†Ô∏è –¢–æ—á–∫–∞ –Ω–µ –º–∞—î –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if len(point.vector) != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä–∞ —É —Ç–æ—á—Ü—ñ: {len(point.vector)} –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            return point
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
    
    def optimize_collection(self):
        """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó –ø—ñ—Å–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"""
        try:
            # –§–æ—Ä—Å—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            self.client.update_collection(
                collection_name=self.collection_name,
                optimizer_config=models.OptimizersConfigDiff(
                    max_segment_size=500000,
                    memmap_threshold=100000,
                    indexing_threshold=100000
                )
            )
            self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: {e}")


    def index_tenders(self, 
                historical_data: List[Dict], 
                update_mode: bool = True,
                batch_size: int = 1000) -> Dict[str, Any]:
        """
        –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ –∑ –î–ï–¢–ê–õ–¨–ù–ò–ú –ª–æ–≥—É–≤–∞–Ω–Ω—è–º
        """
        print(f"\nüîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(historical_data):,} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"üìã –†–µ–∂–∏–º: {'–û–Ω–æ–≤–ª–µ–Ω–Ω—è' if update_mode else '–ü–æ–≤–Ω–∞ –ø–µ—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è'}")
        print(f"üì¶ –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É: {batch_size}")
        
        start_time = datetime.now()
        results = {
            'total_processed': len(historical_data),
            'indexed_count': 0,
            'updated_count': 0,
            'skipped_count': 0,
            'error_count': 0,
            'processing_time': 0,
            'batch_stats': []  # –î–û–î–ê–ù–û –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –±–∞—Ç—á—ñ–≤
        }
        
        if not update_mode:
            try:
                self.client.delete_collection(self.collection_name)
                self.logger.info("üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é")
                # –ü–µ—Ä–µ—Å—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–µ–∫—Ü—ñ—é
                self._init_collection()
                self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–∞")
            except Exception as e:
                self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                import traceback
                self.logger.debug(f"–ü–æ–≤–Ω–∏–π traceback: {traceback.format_exc()}")


        
        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤
        existing_hashes = set()
        if update_mode:
            load_start = datetime.now()
            existing_hashes = self._get_existing_hashes()
            load_time = (datetime.now() - load_start).total_seconds()
            print(f"‚è±Ô∏è –ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ö–µ—à—ñ–≤: {load_time:.1f} —Å–µ–∫\n")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        new_items = []
        print("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏...")
        
        for item in historical_data:
            content_hash = self._generate_content_hash(item)
            if not update_mode or content_hash not in existing_hashes:
                new_items.append(item)
            else:
                results['skipped_count'] += 1
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏:")
        print(f"   ‚Ä¢ –ù–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {len(new_items):,}")
        print(f"   ‚Ä¢ –î—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {results['skipped_count']:,}")
        
        if not new_items:
            print("\n‚úÖ –í—Å—ñ –∑–∞–ø–∏—Å–∏ –≤–∂–µ —î –≤ –±–∞–∑—ñ")
            results['processing_time'] = (datetime.now() - start_time).total_seconds()
            return results
        
        print(f"\nüöÄ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(new_items):,} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤:")
        
        pbar = tqdm(
            total=len(new_items),
            desc="–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è",
            unit="–∑–∞–ø–∏—Å—ñ–≤",
            ncols=100
        )
        
        # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á–∞–º–∏ –∑ –î–ï–¢–ê–õ–¨–ù–ò–ú –ª–æ–≥—É–≤–∞–Ω–Ω—è–º
        points_to_upsert = []
        batch_num = 0
        point_creation_errors = 0
        
        # –î–û–î–ê–ù–û: —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        points_prepared = 0
        points_added_to_batch = 0
        
        for i, item in enumerate(new_items):
            points_prepared += 1  # –î–û–î–ê–ù–û: —Ä–∞—Ö—É—î–º–æ –≤—Å—ñ –æ–±—Ä–æ–±–ª–µ–Ω—ñ –∑–∞–ø–∏—Å–∏
            try:
                # –î–û–î–ê–ù–û: –¥–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
                point = self._prepare_point(item)
                if point is None:
                    print(f"‚ö†Ô∏è _prepare_point –ø–æ–≤–µ—Ä–Ω—É–≤ None –¥–ª—è –∑–∞–ø–∏—Å—É {i}")
                    continue
                
                points_to_upsert.append(point)
                points_added_to_batch += 1  # –î–û–î–ê–ù–û: —Ä–∞—Ö—É—î–º–æ –¥–æ–¥–∞–Ω—ñ –¥–æ –±–∞—Ç—á—É
                
                # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –±–∞—Ç—á—É
                if len(points_to_upsert) >= batch_size:
                    batch_num += 1
                    
                    # –î–û–î–ê–ù–û: –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–¥ –≤—ñ–¥–ø—Ä–∞–≤–∫–æ—é
                    print(f"\nüîÄ –ë–∞—Ç—á #{batch_num}: –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {points_prepared} –∑–∞–ø–∏—Å—ñ–≤, —É –±–∞—Ç—á—ñ {points_added_to_batch} —Ç–æ—á–æ–∫")
                    
                    batch_start = datetime.now()
                    success_count = self._upsert_batch(points_to_upsert, batch_num)
                    batch_time = (datetime.now() - batch_start).total_seconds()
                    
                    # –î–û–î–ê–ù–û: –¥–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á—É
                    batch_stat = {
                        'batch_num': batch_num,
                        'points_prepared': len(points_to_upsert),
                        'points_uploaded': success_count,
                        'points_failed': len(points_to_upsert) - success_count,
                        'batch_time': batch_time
                    }
                    results['batch_stats'].append(batch_stat)
                    
                    print(f"‚úÖ –ë–∞—Ç—á #{batch_num}: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {success_count}/{len(points_to_upsert)} —Ç–æ—á–æ–∫ –∑–∞ {batch_time:.2f}—Å")
                    
                    results['indexed_count'] += success_count
                    results['error_count'] += len(points_to_upsert) - success_count
                    points_to_upsert = []
                    points_added_to_batch = 0  # –î–û–î–ê–ù–û: —Å–∫–∏–¥–∞—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ –±–∞—Ç—á—É
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–∂–Ω—ñ 10 –±–∞—Ç—á—ñ–≤
                    if batch_num % 10 == 0:
                        elapsed = (datetime.now() - start_time).total_seconds()
                        speed = results['indexed_count'] / elapsed if elapsed > 0 else 0
                        pbar.set_postfix(
                            batches=batch_num,
                            indexed=f"{results['indexed_count']:,}",
                            speed=f"{speed:.0f}/—Å"
                        )
                
                pbar.update(1)
                
            except Exception as e:
                point_creation_errors += 1
                results['error_count'] += 1
                
                # –î–û–î–ê–ù–û: –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–æ–∫
                if point_creation_errors <= 10:
                    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏ –¥–ª—è –∑–∞–ø–∏—Å—É {i}: {e}")
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø—Ä–æ–±–ª–µ–º–Ω–∏–π –∑–∞–ø–∏—Å
                    print(f"   –¢–µ–Ω–¥–µ—Ä: {item.get('F_TENDERNUMBER', 'N/A')}")
                    print(f"   –Ñ–î–†–ü–û–£: {item.get('EDRPOU', 'N/A')}")
                    print(f"   –¢–æ–≤–∞—Ä: {item.get('F_ITEMNAME', 'N/A')[:50]}...")
                
                pbar.update(1)
        
        # –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á
        if points_to_upsert:
            batch_num += 1
            print(f"\nüîÄ –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á #{batch_num}: {len(points_to_upsert)} —Ç–æ—á–æ–∫")
            
            batch_start = datetime.now()
            success_count = self._upsert_batch(points_to_upsert, batch_num)
            batch_time = (datetime.now() - batch_start).total_seconds()
            
            batch_stat = {
                'batch_num': batch_num,
                'points_prepared': len(points_to_upsert),
                'points_uploaded': success_count,
                'points_failed': len(points_to_upsert) - success_count,
                'batch_time': batch_time
            }
            results['batch_stats'].append(batch_stat)
            
            print(f"‚úÖ –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {success_count}/{len(points_to_upsert)} —Ç–æ—á–æ–∫")
            
            results['indexed_count'] += success_count
            results['error_count'] += len(points_to_upsert) - success_count
        
        pbar.close()
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á—ñ–≤
        if results['batch_stats']:
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á—ñ–≤:")
            total_prepared = sum(b['points_prepared'] for b in results['batch_stats'])
            total_uploaded = sum(b['points_uploaded'] for b in results['batch_stats'])
            avg_batch_size = total_prepared / len(results['batch_stats'])
            
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –±–∞—Ç—á—ñ–≤: {len(results['batch_stats'])}")
            print(f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É: {avg_batch_size:.1f}")
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ —Ç–æ—á–æ–∫: {total_prepared:,}")
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç–æ—á–æ–∫: {total_uploaded:,}")
            print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–æ–∫: {point_creation_errors}")
            
            # –ü–µ—Ä—à—ñ –∫—ñ–ª—å–∫–∞ –±–∞—Ç—á—ñ–≤ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            print(f"\nüìã –ü–µ—Ä—à—ñ 5 –±–∞—Ç—á—ñ–≤:")
            for batch in results['batch_stats'][:5]:
                print(f"   –ë–∞—Ç—á {batch['batch_num']}: {batch['points_prepared']}/{batch_size} –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ, "
                    f"{batch['points_uploaded']} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ, {batch['batch_time']:.2f}—Å")
        
        # –§—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è
        results['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats['total_indexed'] = self.get_collection_size()
        self.stats['last_index_time'] = datetime.now().isoformat()
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*60)
        print("‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("="*60)
        print(f"üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –û–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞–ø–∏—Å—ñ–≤: {results['total_processed']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ –Ω–æ–≤–∏—Ö: {results['indexed_count']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {results['skipped_count']:,}")
        print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫ –æ–±—Ä–æ–±–∫–∏: {results['error_count']:,}")
        print(f"   ‚Ä¢ –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {results['processing_time']:.1f} —Å–µ–∫")
        
        if results['indexed_count'] > 0:
            print(f"   ‚Ä¢ –®–≤–∏–¥–∫—ñ—Å—Ç—å: {results['indexed_count']/results['processing_time']:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
        
        print(f"\nüìä –°—Ç–∞–Ω –∫–æ–ª–µ–∫—Ü—ñ—ó:")
        print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ: {self.stats['total_indexed']:,}")
        print("="*60)
        
        return results


    def _format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —á–∞—Å—É –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥"""
        if seconds < 60:
            return f"{seconds:.0f} —Å–µ–∫"
        elif seconds < 3600:
            return f"{seconds/60:.1f} —Ö–≤"
        else:
            return f"{seconds/3600:.1f} –≥–æ–¥"

    def _get_existing_hashes(self) -> set:
        """–°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤ –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É"""
        try:
            hashes = set()
            offset = None
            batch_count = 0
            
            print(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤...")
            
            while True:
                # –û—Ç—Ä–∏–º—É—î–º–æ –±–∞—Ç—á –∑–∞–ø–∏—Å—ñ–≤
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=offset,
                    limit=10000,
                    with_payload=["content_hash"],
                    with_vectors=False
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                    
                # –î–æ–¥–∞—î–º–æ —Ö–µ—à—ñ
                for point in points:
                    if point.payload and 'content_hash' in point.payload:
                        hashes.add(point.payload['content_hash'])
                
                batch_count += 1
                if batch_count % 10 == 0:
                    print(f"   ‚Ä¢ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(hashes):,} —Ö–µ—à—ñ–≤...")
                
                if not next_offset:
                    break
                    
                offset = next_offset
            
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(hashes):,} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ö–µ—à—ñ–≤")
            return hashes
            
        except Exception as e:            
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤: {e}")
            return set()
    
    def _upsert_batch(self, points: List[models.PointStruct], batch_num: int = 0) -> int:
        """–®–≤–∏–¥–∫–∞ –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è"""
        try:
            print(f"üì° –®–≤–∏–¥–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—É #{batch_num} –∑ {len(points)} —Ç–æ—á–æ–∫...")
            
            # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è - —Ç—ñ–ª—å–∫–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å
            if not points:
                print(f"‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π –±–∞—Ç—á #{batch_num}")
                return 0
            
            # üî• –®–í–ò–î–ö–ê –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
            self.client.upsert(
                collection_name=self.collection_name,
                points=points,
                wait=False,  # –ù–ï —á–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è!
                ordering=models.WriteOrdering.WEAK  # –°–ª–∞–±–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
            )
            
            print(f"‚ö° –ë–∞—Ç—á #{batch_num} –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ")
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ —É—Å–ø—ñ—Ö)
            return len(points)
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —à–≤–∏–¥–∫–æ—ó –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –±–∞—Ç—á—É #{batch_num}: {e}")
            print(f"   –¢–∏–ø –ø–æ–º–∏–ª–∫–∏: {type(e).__name__}")
        return 0


    
    def search_similar_tenders(self, 
                             query_item: Dict, 
                             limit: int = 10,
                             similarity_threshold: float = 0.7) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
        """
        self.stats['total_searches'] += 1
        
        item_name = query_item.get('F_ITEMNAME', '')
        if not item_name:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(item_name)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            query_filter = None
            
            # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
            industry = query_item.get('F_INDUSTRYNAME')
            if industry:
                query_filter = models.Filter(
                    must=[
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=industry)
                        )
                    ]
                )
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit * 2,  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
                with_payload=True,
                score_threshold=similarity_threshold
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            similar_tenders = []
            for result in search_results:
                if len(similar_tenders) >= limit:
                    break
                
                # –£–Ω–∏–∫–∞—î–º–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å —Ç–æ–≥–æ –∂ —Ç–µ–Ω–¥–µ—Ä—É
                if (result.payload.get('tender_number') == query_item.get('F_TENDERNUMBER') and
                    result.payload.get('edrpou') == query_item.get('EDRPOU')):
                    continue
                
                similar_tender = {
                    'similarity_score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                
                similar_tenders.append(similar_tender)
            
            return similar_tenders
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤: {e}")
            return []
    
    def search_by_text(self, 
                      query: str, 
                      filters: Optional[Dict] = None,
                      limit: int = 20) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º
        """
        self.stats['total_searches'] += 1
        
        if not query:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(query)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤
            query_filter = None
            if filters:
                filter_conditions = []
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                if 'category' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="primary_category",
                            match=models.MatchValue(value=filters['category'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
                if 'industry' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=filters['industry'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –±—é–¥–∂–µ—Ç—É
                if 'min_budget' in filters or 'max_budget' in filters:
                    range_condition = {}
                    if 'min_budget' in filters:
                        range_condition['gte'] = filters['min_budget']
                    if 'max_budget' in filters:
                        range_condition['lte'] = filters['max_budget']
                    
                    filter_conditions.append(
                        models.FieldCondition(
                            key="budget",
                            range=models.Range(**range_condition)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è—Ö
                if 'won_only' in filters and filters['won_only']:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="won",
                            match=models.MatchValue(value=True)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Ö
                if 'date_from' in filters or 'date_to' in filters:
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ª–æ–≥—ñ–∫–∞ –¥–ª—è –¥–∞—Ç
                    pass
                
                if filter_conditions:
                    query_filter = models.Filter(must=filter_conditions)
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit,
                with_payload=True
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            results = []
            for result in search_results:
                tender_result = {
                    'score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                results.append(tender_result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—à—É–∫—É: {e}")
            return []
    
    def get_collection_size(self) -> int:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            return 0
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            return {
                'points_count': collection_info.points_count,
                'segments_count': len(collection_info.segments) if collection_info.segments else 0,
                'vector_size': collection_info.config.params.vectors.size,
                'distance_metric': collection_info.config.params.vectors.distance.value,
                'index_stats': self.stats,
                'status': collection_info.status.value
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {'error': str(e)}
    
    def delete_tender_records(self, tender_number: str) -> bool:
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="tender_number",
                                match=models.MatchValue(value=tender_number)
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–∏ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_number}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return False
    
    def update_tender_records(self, tender_data: List[Dict]) -> Dict[str, int]:
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        if not tender_data:
            return {'updated': 0, 'errors': 0}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞
        tender_number = tender_data[0].get('F_TENDERNUMBER')
        if not tender_number:
            return {'updated': 0, 'errors': 1}
        
        try:
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ –∑–∞–ø–∏—Å–∏
            self.delete_tender_records(tender_number)
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏
            results = self.index_tenders(tender_data, update_mode=True)
            
            return {
                'updated': results['indexed_count'],
                'errors': results['error_count']
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return {'updated': 0, 'errors': 1}
    
    def cleanup_old_records(self, days_old: int = 365) -> int:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            deleted_count = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="created_at",
                                range=models.DatetimeRange(
                                    lt=cutoff_date.isoformat()
                                )
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {e}")
            return 0
    
    def export_collection_data(self, limit: Optional[int] = None) -> List[Dict]:
        """–ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            all_points = []
            
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit or 10000,
                with_payload=True,
                with_vectors=False  # –ù–µ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            )
            
            points, next_page_offset = scroll_result
            all_points.extend([{
                'id': point.id,
                'payload': point.payload
            } for point in points])
            
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            while next_page_offset and (not limit or len(all_points) < limit):
                remaining = limit - len(all_points) if limit else 10000
                
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=min(remaining, 10000),
                    with_payload=True,
                    with_vectors=False
                )
                
                points, next_page_offset = scroll_result
                all_points.extend([{
                    'id': point.id,
                    'payload': point.payload
                } for point in points])
            
            return all_points
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É –¥–∞–Ω–∏—Ö: {e}")
            return []
    
    def get_database_health(self) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        health_status = {
            'status': 'unknown',
            'collection_exists': False,
            'points_count': 0,
            'last_indexed': self.stats.get('last_index_time'),
            'search_performance': 'unknown',
            'errors': []
        }
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            health_status['collection_exists'] = True
            health_status['points_count'] = collection_info.points_count
            
            # –¢–µ—Å—Ç –ø–æ—à—É–∫—É
            start_time = datetime.now()
            self.client.search(
                collection_name=self.collection_name,
                query_vector=[0.1] * 768,
                limit=1
            )
            search_time = (datetime.now() - start_time).total_seconds()
            
            if search_time < 0.1:
                health_status['search_performance'] = 'excellent'
            elif search_time < 0.5:
                health_status['search_performance'] = 'good'
            else:
                health_status['search_performance'] = 'slow'
            
            health_status['status'] = 'healthy'
            
        except Exception as e:
            health_status['status'] = 'error'
            health_status['errors'].append(str(e))
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è –ë–î: {e}")
        
        return health_status
    
    def search_tenders_by_group(self, group_criteria: Dict, limit: int = 20) -> List[Dict]:
        """–ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ –≥—Ä—É–ø–æ–≤–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏"""
        filter_conditions = []
        
        for field, value in group_criteria.items():
            if value is not None:
                filter_conditions.append(
                    models.FieldCondition(
                        key=field,
                        match=models.MatchValue(value=value)
                    )
                )
        
        query_filter = models.Filter(must=filter_conditions) if filter_conditions else None
        
        try:
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            return [{
                'id': point.id,
                **point.payload
            } for point in results]
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∑–∞ –≥—Ä—É–ø–æ—é: {e}")
            return []

    def get_tender_groups(self, tender_number: str) -> Dict[str, List[Dict]]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –≥—Ä—É–ø –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            query_filter = models.Filter(
                must=[
                    models.FieldCondition(
                        key="tender_number",
                        match=models.MatchValue(value=tender_number)
                    )
                ]
            )
            
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=1000,
                with_payload=True,
                with_vectors=False
            )[0]
            
            groups = defaultdict(list)
            for point in results:
                group_key = point.payload.get('group_key', 'unknown')
                groups[group_key].append(point.payload)
            
            return dict(groups)
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≥—Ä—É–ø —Ç–µ–Ω–¥–µ—Ä–∞: {e}")
            return {}

