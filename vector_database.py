import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import numpy as np
import hashlib

# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import ResponseHandlingException, UnexpectedResponse


class TenderVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω—å
    
    –§—É–Ω–∫—Ü—ñ—ó:
    - –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
    - –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è
    - –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    - –ê–Ω–∞–ª—ñ–∑ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    """
    
    def __init__(self, 
                 embedding_model,
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333,
                 collection_name: str = "tender_offers_history"):
        
        self.logger = logging.getLogger(__name__)
        self.embedding_model = embedding_model
        self.collection_name = collection_name
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant
        try:
            self.client = QdrantClient(host=qdrant_host, port=qdrant_port)
            self.logger.info(f"‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–æ –¥–æ Qdrant: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant: {e}")
            raise
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        self._init_collection()
        
        # –ö–µ—à –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å
        self.embedding_cache = {}
        self.hash_cache = {}  # –î–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_indexed': 0,
            'total_updated': 0,
            'total_searches': 0,
            'last_index_time': None,
            'last_update_time': None
        }
    
    def _init_collection(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó Qdrant"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î")
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É
            self.stats['total_indexed'] = collection_info.points_count
            
        except (ResponseHandlingException, UnexpectedResponse):
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó
            self.logger.info(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó '{self.collection_name}'...")
            
            try:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=768,  # –†–æ–∑–º—ñ—Ä –¥–ª—è paraphrase-multilingual-mpnet-base-v2
                        distance=models.Distance.COSINE
                    ),
                    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
                    optimizers_config=models.OptimizersConfigDiff(
                        default_segment_number=2,
                        max_segment_size=20000,
                        memmap_threshold=50000,
                        indexing_threshold=20000,
                        flush_interval_sec=10,
                        max_optimization_threads=2
                    ),
                    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—ó —Ç–∞ —à–∞—Ä–¥–∏–Ω–≥–∞
                    shard_number=1,
                    replication_factor=1
                )
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
                self._create_payload_indexes()
                
                self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
                
            except Exception as e:
                self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                raise
    
    def _create_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è payload –ø–æ–ª—ñ–≤"""
        indexes_to_create = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("industry", models.PayloadSchemaType.KEYWORD),
            ("primary_category", models.PayloadSchemaType.KEYWORD),
            ("cpv", models.PayloadSchemaType.INTEGER),
            ("budget", models.PayloadSchemaType.FLOAT),
            ("won", models.PayloadSchemaType.BOOL),
            ("date_end", models.PayloadSchemaType.KEYWORD),
            ("created_at", models.PayloadSchemaType.DATETIME)
        ]
        
        for field_name, field_type in indexes_to_create:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type
                )
            except Exception as e:
                # –Ü–Ω–¥–µ–∫—Å –≤–∂–µ –º–æ–∂–µ —ñ—Å–Ω—É–≤–∞—Ç–∏
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤"""
        if not text or not isinstance(text, str):
            return ""
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        text = text.lower().strip()
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤, –∞–ª–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–º—ñ—Å—Ç–æ–≤–Ω–∏—Ö
        text = re.sub(r'[^\w\s\-\.\(\)]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å –≤–∏–º—ñ—Ä—É
        units_mapping = {
            r'\b—à—Ç\.?\b': '—à—Ç—É–∫',
            r'\b–∫–≥\.?\b': '–∫—ñ–ª–æ–≥—Ä–∞–º', 
            r'\b–≥\.?\b': '–≥—Ä–∞–º',
            r'\b–ª\.?\b': '–ª—ñ—Ç—Ä',
            r'\b–º\.?\b': '–º–µ—Ç—Ä',
            r'\b—Å–º\.?\b': '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä',
            r'\b–º–º\.?\b': '–º—ñ–ª—ñ–º–µ—Ç—Ä'
        }
        
        for pattern, replacement in units_mapping.items():
            text = re.sub(pattern, replacement, text)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _generate_content_hash(self, item: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ö–µ—à—É –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—ó —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –∑–∞–ø–∏—Å—ñ–≤"""
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–ª—é—á–æ–≤—ñ –ø–æ–ª—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ —Ö–µ—à—É
        key_fields = [
            item.get('F_TENDERNUMBER', ''),
            item.get('EDRPOU', ''),
            item.get('F_ITEMNAME', ''),
            str(item.get('ITEM_BUDGET', '')),
            item.get('DATEEND', '')
        ]
        
        content_str = '|'.join(str(field) for field in key_fields)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    
    def _create_embedding(self, text: str) -> np.ndarray:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º"""
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–µ—à—É
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ –µ–º–±–µ–¥–∏–Ω–≥—É
        processed_text = self._preprocess_text(text)
        if not processed_text:
            # –ü–æ—Ä–æ–∂–Ω—ñ–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ—Ä–æ–∂–Ω—å–æ–≥–æ —Ç–µ–∫—Å—Ç—É
            embedding = np.zeros(768)
        else:
            embedding = self.embedding_model.encode(processed_text)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –∫–µ—à (–æ–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–µ—à—É)
        if len(self.embedding_cache) < 10000:
            self.embedding_cache[text] = embedding
        
        return embedding
    
    def _prepare_point(self, item: Dict, point_id: Optional[int] = None) -> models.PointStruct:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –≤ Qdrant"""
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É
        item_name = item.get('F_ITEMNAME', '')
        embedding = self._create_embedding(item_name)
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
        metadata = {
            'tender_number': item.get('F_TENDERNUMBER', ''),
            'supplier_name': item.get('supp_name', ''),
            'edrpou': item.get('EDRPOU', ''),
            'item_name': item_name,
            'industry': item.get('F_INDUSTRYNAME', ''),
            'cpv': int(item.get('CPV', 0)) if item.get('CPV') else 0,
            'budget': float(item.get('ITEM_BUDGET', 0)) if item.get('ITEM_BUDGET') else 0.0,
            'quantity': float(item.get('F_qty', 0)) if item.get('F_qty') else 0.0,
            'price': float(item.get('F_price', 0)) if item.get('F_price') else 0.0,
            'won': bool(item.get('WON', False)),
            'date_end': item.get('DATEEND', ''),
            'created_at': datetime.now().isoformat(),
            'content_hash': self._generate_content_hash(item)
        }
        
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π (—è–∫—â–æ —î CategoryManager)
        if hasattr(self, 'category_manager') and self.category_manager:
            categories = self.category_manager.categorize_item(item_name)
            metadata['primary_category'] = categories[0][0] if categories else 'unknown'
            metadata['all_categories'] = [cat[0] for cat in categories[:3]]  # –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
            metadata['category_confidence'] = categories[0][1] if categories else 0.0
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–∞–Ω–æ–≥–æ ID –∞–±–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ
        if point_id is None:
            point_id = int(hashlib.md5(metadata['content_hash'].encode()).hexdigest()[:8], 16)
        
        return models.PointStruct(
            id=point_id,
            vector=embedding.tolist(),
            payload=metadata
        )
    
    def index_tenders(self, 
                     historical_data: List[Dict], 
                     update_mode: bool = False,
                     batch_size: int = 100) -> Dict[str, Any]:
        """
        –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ
        
        Args:
            historical_data: –°–ø–∏—Å–æ–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
            update_mode: True –¥–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è, False –¥–ª—è –ø–æ–≤–Ω–æ—ó –ø–µ—Ä–µ—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
            batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        """
        self.logger.info(f"üîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(historical_data)} –∑–∞–ø–∏—Å—ñ–≤ (update_mode: {update_mode})")
        
        start_time = datetime.now()
        results = {
            'total_processed': len(historical_data),
            'indexed_count': 0,
            'updated_count': 0,
            'skipped_count': 0,
            'error_count': 0,
            'processing_time': 0
        }
        
        # –û—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —è–∫—â–æ –Ω–µ update_mode
        if not update_mode:
            try:
                self.client.delete_collection(self.collection_name)
                self._init_collection()
                self.logger.info("üóëÔ∏è –ö–æ–ª–µ–∫—Ü—ñ—è –æ—á–∏—â–µ–Ω–∞ –¥–ª—è –ø–æ–≤–Ω–æ—ó –ø–µ—Ä–µ—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            except Exception as e:
                self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
        
        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –¥—É–±–ª—é–≤–∞–Ω–Ω—è
        existing_hashes = set()
        if update_mode:
            existing_hashes = self._get_existing_hashes()
            self.logger.info(f"üìã –ó–Ω–∞–π–¥–µ–Ω–æ {len(existing_hashes)} —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
        
        # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á–∞–º–∏
        points_to_upsert = []
        
        for i, item in enumerate(historical_data):
            try:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ
                content_hash = self._generate_content_hash(item)
                
                if update_mode and content_hash in existing_hashes:
                    results['skipped_count'] += 1
                    continue
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
                point = self._prepare_point(item)
                points_to_upsert.append(point)
                
                # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –±–∞—Ç—á—É
                if len(points_to_upsert) >= batch_size:
                    success_count = self._upsert_batch(points_to_upsert)
                    results['indexed_count'] += success_count
                    results['error_count'] += len(points_to_upsert) - success_count
                    points_to_upsert = []
                
                # –ü—Ä–æ–≥—Ä–µ—Å
                if (i + 1) % 1000 == 0:
                    self.logger.info(f"üìà –û–±—Ä–æ–±–ª–µ–Ω–æ {i + 1}/{len(historical_data)} –∑–∞–ø–∏—Å—ñ–≤")
                    
            except Exception as e:
                self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–∞–ø–∏—Å—É {i}: {e}")
                results['error_count'] += 1
                continue
        
        # –û–±—Ä–æ–±–∫–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É
        if points_to_upsert:
            success_count = self._upsert_batch(points_to_upsert)
            results['indexed_count'] += success_count
            results['error_count'] += len(points_to_upsert) - success_count
        
        # –§—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è
        results['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['total_indexed'] = self.get_collection_size()
        self.stats['last_index_time'] = datetime.now().isoformat()
        
        self.logger.info(f"‚úÖ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {results['processing_time']:.2f} —Å–µ–∫")
        self.logger.info(f"üìä –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {results['indexed_count']}, "
                        f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {results['skipped_count']}, "
                        f"–ü–æ–º–∏–ª–æ–∫: {results['error_count']}")
        
        return results
    
    def _get_existing_hashes(self) -> set:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –¥—É–±–ª—é–≤–∞–Ω–Ω—è"""
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ —Ç–æ—á–∫–∏ –∑ content_hash
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=10000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ª—ñ–º—ñ—Ç
                with_payload=["content_hash"]
            )
            
            hashes = set()
            points = scroll_result[0]
            
            for point in points:
                if point.payload and 'content_hash' in point.payload:
                    hashes.add(point.payload['content_hash'])
            
            # –Ø–∫—â–æ —î —â–µ —Ç–æ—á–∫–∏, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥
            next_page_offset = scroll_result[1]
            while next_page_offset:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=10000,
                    with_payload=["content_hash"]
                )
                
                points = scroll_result[0]
                for point in points:
                    if point.payload and 'content_hash' in point.payload:
                        hashes.add(point.payload['content_hash'])
                
                next_page_offset = scroll_result[1]
            
            return hashes
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤: {e}")
            return set()
    
    def _upsert_batch(self, points: List[models.PointStruct]) -> int:
        """–í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É —Ç–æ—á–æ–∫"""
        try:
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            return len(points)
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –±–∞—Ç—á—É: {e}")
            return 0
    
    def search_similar_tenders(self, 
                             query_item: Dict, 
                             limit: int = 10,
                             similarity_threshold: float = 0.7) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
        """
        self.stats['total_searches'] += 1
        
        item_name = query_item.get('F_ITEMNAME', '')
        if not item_name:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(item_name)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            query_filter = None
            
            # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
            industry = query_item.get('F_INDUSTRYNAME')
            if industry:
                query_filter = models.Filter(
                    must=[
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=industry)
                        )
                    ]
                )
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit * 2,  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
                with_payload=True,
                score_threshold=similarity_threshold
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            similar_tenders = []
            for result in search_results:
                if len(similar_tenders) >= limit:
                    break
                
                # –£–Ω–∏–∫–∞—î–º–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å —Ç–æ–≥–æ –∂ —Ç–µ–Ω–¥–µ—Ä—É
                if (result.payload.get('tender_number') == query_item.get('F_TENDERNUMBER') and
                    result.payload.get('edrpou') == query_item.get('EDRPOU')):
                    continue
                
                similar_tender = {
                    'similarity_score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                
                similar_tenders.append(similar_tender)
            
            return similar_tenders
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤: {e}")
            return []
    
    def search_by_text(self, 
                      query: str, 
                      filters: Optional[Dict] = None,
                      limit: int = 20) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º
        """
        self.stats['total_searches'] += 1
        
        if not query:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(query)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤
            query_filter = None
            if filters:
                filter_conditions = []
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                if 'category' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="primary_category",
                            match=models.MatchValue(value=filters['category'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
                if 'industry' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=filters['industry'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –±—é–¥–∂–µ—Ç—É
                if 'min_budget' in filters or 'max_budget' in filters:
                    range_condition = {}
                    if 'min_budget' in filters:
                        range_condition['gte'] = filters['min_budget']
                    if 'max_budget' in filters:
                        range_condition['lte'] = filters['max_budget']
                    
                    filter_conditions.append(
                        models.FieldCondition(
                            key="budget",
                            range=models.Range(**range_condition)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è—Ö
                if 'won_only' in filters and filters['won_only']:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="won",
                            match=models.MatchValue(value=True)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Ö
                if 'date_from' in filters or 'date_to' in filters:
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ª–æ–≥—ñ–∫–∞ –¥–ª—è –¥–∞—Ç
                    pass
                
                if filter_conditions:
                    query_filter = models.Filter(must=filter_conditions)
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit,
                with_payload=True
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            results = []
            for result in search_results:
                tender_result = {
                    'score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                results.append(tender_result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—à—É–∫—É: {e}")
            return []
    
    def get_collection_size(self) -> int:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            return 0
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            return {
                'points_count': collection_info.points_count,
                'segments_count': len(collection_info.segments) if collection_info.segments else 0,
                'vector_size': collection_info.config.params.vectors.size,
                'distance_metric': collection_info.config.params.vectors.distance.value,
                'index_stats': self.stats,
                'status': collection_info.status.value
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {'error': str(e)}
    
    def delete_tender_records(self, tender_number: str) -> bool:
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="tender_number",
                                match=models.MatchValue(value=tender_number)
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–∏ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_number}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return False
    
    def update_tender_records(self, tender_data: List[Dict]) -> Dict[str, int]:
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        if not tender_data:
            return {'updated': 0, 'errors': 0}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞
        tender_number = tender_data[0].get('F_TENDERNUMBER')
        if not tender_number:
            return {'updated': 0, 'errors': 1}
        
        try:
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ –∑–∞–ø–∏—Å–∏
            self.delete_tender_records(tender_number)
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏
            results = self.index_tenders(tender_data, update_mode=True)
            
            return {
                'updated': results['indexed_count'],
                'errors': results['error_count']
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return {'updated': 0, 'errors': 1}
    
    def cleanup_old_records(self, days_old: int = 365) -> int:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            deleted_count = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="created_at",
                                range=models.DatetimeRange(
                                    lt=cutoff_date.isoformat()
                                )
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {e}")
            return 0
    
    def export_collection_data(self, limit: Optional[int] = None) -> List[Dict]:
        """–ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            all_points = []
            
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit or 10000,
                with_payload=True,
                with_vectors=False  # –ù–µ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            )
            
            points, next_page_offset = scroll_result
            all_points.extend([{
                'id': point.id,
                'payload': point.payload
            } for point in points])
            
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            while next_page_offset and (not limit or len(all_points) < limit):
                remaining = limit - len(all_points) if limit else 10000
                
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=min(remaining, 10000),
                    with_payload=True,
                    with_vectors=False
                )
                
                points, next_page_offset = scroll_result
                all_points.extend([{
                    'id': point.id,
                    'payload': point.payload
                } for point in points])
            
            return all_points
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É –¥–∞–Ω–∏—Ö: {e}")
            return []
    
    def get_database_health(self) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        health_status = {
            'status': 'unknown',
            'collection_exists': False,
            'points_count': 0,
            'last_indexed': self.stats.get('last_index_time'),
            'search_performance': 'unknown',
            'errors': []
        }
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            health_status['collection_exists'] = True
            health_status['points_count'] = collection_info.points_count
            
            # –¢–µ—Å—Ç –ø–æ—à—É–∫—É
            start_time = datetime.now()
            self.client.search(
                collection_name=self.collection_name,
                query_vector=[0.1] * 768,
                limit=1
            )
            search_time = (datetime.now() - start_time).total_seconds()
            
            if search_time < 0.1:
                health_status['search_performance'] = 'excellent'
            elif search_time < 0.5:
                health_status['search_performance'] = 'good'
            else:
                health_status['search_performance'] = 'slow'
            
            health_status['status'] = 'healthy'
            
        except Exception as e:
            health_status['status'] = 'error'
            health_status['errors'].append(str(e))
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è –ë–î: {e}")
        
        return health_status