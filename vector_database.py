import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import numpy as np
import hashlib
from tqdm import tqdm
import sys
import os
from sentence_transformers import SentenceTransformer
# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import ResponseHandlingException, UnexpectedResponse


class TenderVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω—å
    
    –§—É–Ω–∫—Ü—ñ—ó:
    - –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
    - –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è
    - –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    - –ê–Ω–∞–ª—ñ–∑ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    """
    
    def __init__(self, 
                 embedding_model,
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333,
                 collection_name: str = "tender_vectors"):
        
        self.logger = logging.getLogger(__name__)
        self.embedding_model = embedding_model
        import transformers
        import datasets
        transformers.logging.set_verbosity_error()
        datasets.logging.set_verbosity_error()
        if hasattr(self.embedding_model, 'tokenizer') and self.embedding_model.tokenizer:
            self.embedding_model.tokenizer.verbose = False

        self.collection_name = collection_name
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant
        try:
            self.client = QdrantClient(host=qdrant_host, port=qdrant_port)
            self.logger.info(f"‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–æ –¥–æ Qdrant: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant: {e}")
            raise
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        self._init_collection()
        
        # –ö–µ—à –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å
        self.embedding_cache = {}
        self.hash_cache = {}  # –î–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_indexed': 0,
            'total_updated': 0,
            'total_searches': 0,
            'last_index_time': None,
            'last_update_time': None
        }
    
    def _init_collection(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–±–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó Qdrant"""
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î")
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É
            self.stats['total_indexed'] = collection_info.points_count
            
        except (ResponseHandlingException, UnexpectedResponse):
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó
            self.logger.info(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó '{self.collection_name}'...")
            
            try:
                self.client.create_collection(
                    collection_name=self.collection_name,  # –î–û–î–ê–ù–û!
                    vectors_config=models.VectorParams(
                        size=768,
                        distance=models.Distance.COSINE,                    
                        on_disk=True
                    ),
                    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
                    optimizers_config=models.OptimizersConfigDiff(
                        default_segment_number=4,
                        max_segment_size=100000,
                        memmap_threshold=20000,
                        indexing_threshold=50000,
                        flush_interval_sec=30,
                        max_optimization_threads=1
                    ),
                    hnsw_config=models.HnswConfigDiff(
                        m=16,  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏—Ö 64
                        ef_construct=100,  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏—Ö 200
                        full_scan_threshold=10000,
                        max_indexing_threads=1,
                        on_disk=True,  # HNSW —ñ–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫—É
                        payload_m=16
                    ),
                    quantization_config=models.ScalarQuantization(
                        scalar=models.ScalarQuantizationConfig(
                            type=models.ScalarType.INT8,
                            quantile=0.99,
                            always_ram=False
                        )
                    ),
                    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—ó —Ç–∞ —à–∞—Ä–¥–∏–Ω–≥–∞
                    shard_number=1,
                    replication_factor=1
                )
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
                self._create_payload_indexes()
                
                self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
                
            except Exception as e:
                self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                raise
    
    def _create_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–ª—ñ–≤"""
        indexes_to_create = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("won", models.PayloadSchemaType.BOOL),
            ("industry", models.PayloadSchemaType.KEYWORD),
            # –ù–ï —ñ–Ω–¥–µ–∫—Å—É—î–º–æ F_ITEMNAME - —Ü–µ —Ç–µ–∫—Å—Ç–æ–≤–µ –ø–æ–ª–µ, –∑–∞–π–º–∞—î –±–∞–≥–∞—Ç–æ –º—ñ—Å—Ü—è
        ]
        
        for field_name, field_type in indexes_to_create:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type,
                    wait=False  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
                )
            except Exception as e:
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤"""
        if not text or not isinstance(text, str):
            return ""
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        text = text.lower().strip()
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤, –∞–ª–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–º—ñ—Å—Ç–æ–≤–Ω–∏—Ö
        text = re.sub(r'[^\w\s\-\.\(\)]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å –≤–∏–º—ñ—Ä—É
        units_mapping = {
            r'\b—à—Ç\.?\b': '—à—Ç—É–∫',
            r'\b–∫–≥\.?\b': '–∫—ñ–ª–æ–≥—Ä–∞–º', 
            r'\b–≥\.?\b': '–≥—Ä–∞–º',
            r'\b–ª\.?\b': '–ª—ñ—Ç—Ä',
            r'\b–º\.?\b': '–º–µ—Ç—Ä',
            r'\b—Å–º\.?\b': '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä',
            r'\b–º–º\.?\b': '–º—ñ–ª—ñ–º–µ—Ç—Ä'
        }
        
        for pattern, replacement in units_mapping.items():
            text = re.sub(pattern, replacement, text)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _generate_content_hash(self, item: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ö–µ—à—É"""
        key_str = f"{item.get('F_TENDERNUMBER', '')}{item.get('EDRPOU', '')}{item.get('F_ITEMNAME', '')}{item.get('F_INDUSTRYNAME', '')}"
        return hashlib.md5(key_str.encode('utf-8')).hexdigest()[:16]  # –ö–æ—Ä–æ—Ç—à–∏–π —Ö–µ—à

    
    def _create_embedding(self, text: str) -> np.ndarray:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é"""
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        processed_text = self._preprocess_text(text)
        if not processed_text:
            embedding = np.zeros(768)
        else:
            # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –≤–∏–≤—ñ–¥ –ø—Ä–∏ –µ–Ω–∫–æ–¥–∏–Ω–≥—É
            with open(os.devnull, 'w') as devnull:
                old_stdout = sys.stdout
                sys.stdout = devnull
                embedding = self.embedding_model.encode(processed_text, show_progress_bar=False)
                sys.stdout = old_stdout
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–ª—è –∫—Ä–∞—â–æ—ó –∫–æ–º–ø—Ä–µ—Å—ñ—ó
            embedding = embedding / np.linalg.norm(embedding)
        
        if len(self.embedding_cache) < 5000:
            self.embedding_cache[text] = embedding
        
        return embedding

    
    # def _create_payload_indexes(self):
    #     """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è payload –ø–æ–ª—ñ–≤"""
    #     indexes_to_create = [
    #         ("tender_number", models.PayloadSchemaType.KEYWORD),
    #         ("edrpou", models.PayloadSchemaType.KEYWORD),
    #         ("owner_name", models.PayloadSchemaType.KEYWORD),  # –î–û–î–ê–ù–û
    #         ("industry", models.PayloadSchemaType.KEYWORD),
    #         ("primary_category", models.PayloadSchemaType.KEYWORD),
    #         ("cpv", models.PayloadSchemaType.INTEGER),
    #         ("budget", models.PayloadSchemaType.FLOAT),
    #         ("won", models.PayloadSchemaType.BOOL),
    #         ("date_end", models.PayloadSchemaType.KEYWORD),
    #         ("group_key", models.PayloadSchemaType.KEYWORD),  # –î–û–î–ê–ù–û
    #         ("created_at", models.PayloadSchemaType.DATETIME)
    #     ]


    def _prepare_point(self, item: Dict, point_id: Optional[int] = None) -> models.PointStruct:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –≤ Qdrant –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º OWNER_NAME"""
        
        # –ó–ú–Ü–ù–ï–ù–û: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–±—ñ–Ω–æ–≤–∞–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—É
        combined_text = f"{item.get('F_ITEMNAME', '')} {item.get('F_TENDERNAME', '')} {item.get('F_DETAILNAME', '')}"
        embedding = self._create_embedding(combined_text)
        
        # –ó–ú–Ü–ù–ï–ù–û: –†–æ–∑—à–∏—Ä–µ–Ω—ñ –º–µ—Ç–∞–¥–∞–Ω—ñ
        metadata = {
            # –û—Å–Ω–æ–≤–Ω—ñ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
            'tender_number': item.get('F_TENDERNUMBER', ''),
            'edrpou': item.get('EDRPOU', ''),
            'owner_name': item.get('OWNER_NAME', ''),  # –î–û–î–ê–ù–û
            
            # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Ç–æ–≤–∞—Ä/–ø–æ—Å–ª—É–≥—É
            'item_name': item.get('F_ITEMNAME', ''),
            'tender_name': item.get('F_TENDERNAME', ''),  # –î–û–î–ê–ù–û
            'detail_name': item.get('F_DETAILNAME', ''),  # –î–û–î–ê–ù–û
            
            # –ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫
            'supplier_name': item.get('supp_name', ''),
            
            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
            'industry': item.get('F_INDUSTRYNAME', ''),
            'cpv': int(item.get('CPV', 0)) if item.get('CPV') else 0,
            
            # –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏
            'budget': float(item.get('ITEM_BUDGET', 0)) if item.get('ITEM_BUDGET') else 0.0,
            'quantity': float(item.get('F_qty', 0)) if item.get('F_qty') else 0.0,
            'price': float(item.get('F_price', 0)) if item.get('F_price') else 0.0,
            'currency': item.get('F_TENDERCURRENCY', 'UAH'),  # –î–û–î–ê–ù–û
            'currency_rate': float(item.get('F_TENDERCURRENCYRATE', 1.0)),  # –î–û–î–ê–ù–û
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞ –¥–∞—Ç–∏
            'won': bool(item.get('WON', False)),
            'date_end': item.get('DATEEND', ''),
            'extraction_date': item.get('EXTRACTION_DATE', ''),  # –î–û–î–ê–ù–û
            
            # –°–∏—Å—Ç–µ–º–Ω—ñ –ø–æ–ª—è
            'original_id': item.get('ID', ''),  # –î–û–î–ê–ù–û
            'created_at': datetime.now().isoformat(),
            'content_hash': self._generate_content_hash(item)
        }
        
        # –î–û–î–ê–ù–û: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è
        group_key_parts = [
            str(item.get('EDRPOU', '')),
            str(item.get('F_INDUSTRYNAME', '')),
            str(item.get('F_ITEMNAME', '')),
            str(item.get('OWNER_NAME', '')),
            str(item.get('F_TENDERNAME', ''))
        ]
        
        if item.get('CPV'):
            group_key_parts.append(str(item.get('CPV')))
        
        metadata['group_key'] = '-'.join(group_key_parts)
        
        # –Ü—Å–Ω—É—é—á–∏–π –∫–æ–¥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è
        if hasattr(self, 'category_manager') and self.category_manager:
            categories = self.category_manager.categorize_item(item.get('F_ITEMNAME', ''))
            metadata['primary_category'] = categories[0][0] if categories else 'unknown'
            metadata['all_categories'] = [cat[0] for cat in categories[:3]]
            metadata['category_confidence'] = categories[0][1] if categories else 0.0
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–∞–Ω–æ–≥–æ ID –∞–±–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–æ–≥–æ
        if point_id is None:
            point_id = int(hashlib.md5(metadata['content_hash'].encode()).hexdigest()[:8], 16)
        
        return models.PointStruct(
            id=point_id,
            vector=embedding.tolist(),
            payload=metadata
        )
    
    def optimize_collection(self):
        """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó –ø—ñ—Å–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"""
        try:
            # –§–æ—Ä—Å—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            self.client.update_collection(
                collection_name=self.collection_name,
                optimizer_config=models.OptimizersConfigDiff(
                    max_segment_size=500000,
                    memmap_threshold=100000,
                    indexing_threshold=100000
                )
            )
            self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: {e}")


    def index_tenders(self, 
                 historical_data: List[Dict], 
                 update_mode: bool = False,
                 batch_size: int = 1000) -> Dict[str, Any]:
        """
        –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ –∑ —î–¥–∏–Ω–∏–º –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä–æ–º
        """
        self.logger.info(f"üîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(historical_data)} –∑–∞–ø–∏—Å—ñ–≤ (update_mode: {update_mode})")
        
        start_time = datetime.now()
        results = {
            'total_processed': len(historical_data),
            'indexed_count': 0,
            'updated_count': 0,
            'skipped_count': 0,
            'error_count': 0,
            'processing_time': 0
        }
        
        # –û—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —è–∫—â–æ –Ω–µ update_mode
        if not update_mode:
            try:
                self.client.delete_collection(self.collection_name)
                self._init_collection()
                self.logger.info("üóëÔ∏è –ö–æ–ª–µ–∫—Ü—ñ—è –æ—á–∏—â–µ–Ω–∞ –¥–ª—è –ø–æ–≤–Ω–æ—ó –ø–µ—Ä–µ—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            except Exception as e:
                self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
        
        # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤
        existing_hashes = set()
        if update_mode:
            self.logger.info("üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤...")
            existing_hashes = self._get_existing_hashes()
            self.logger.info(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(existing_hashes)} —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É
        total_items = len(historical_data)
        pbar = tqdm(
            total=total_items,
            desc="üöÄ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è",
            unit="–∑–∞–ø–∏—Å—ñ–≤",
            ncols=120,
            bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate} –∑–∞–ø–∏—Å—ñ–≤/—Å]'
        )
        
        # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á–∞–º–∏
        points_to_upsert = []
        processed_count = 0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≤–∏–≤–æ–¥—É
        stats_update_interval = 1000
        last_stats_update = 0
        
        for i, item in enumerate(historical_data):
            try:
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ
                content_hash = self._generate_content_hash(item)
                
                if update_mode and content_hash in existing_hashes:
                    results['skipped_count'] += 1
                    pbar.set_postfix({
                        '–Ü–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ': results['indexed_count'],
                        '–ü—Ä–æ–ø—É—â–µ–Ω–æ': results['skipped_count'],
                        '–ü–æ–º–∏–ª–æ–∫': results['error_count']
                    }, refresh=False)
                    pbar.update(1)
                    continue
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
                point = self._prepare_point(item)
                points_to_upsert.append(point)
                
                # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –±–∞—Ç—á—É
                if len(points_to_upsert) >= batch_size:
                    success_count = self._upsert_batch(points_to_upsert)
                    results['indexed_count'] += success_count
                    results['error_count'] += len(points_to_upsert) - success_count
                    points_to_upsert = []
                    
                    # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—ñ
                    pbar.set_postfix({
                        '–Ü–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ': results['indexed_count'],
                        '–ü—Ä–æ–ø—É—â–µ–Ω–æ': results['skipped_count'],
                        '–ü–æ–º–∏–ª–æ–∫': results['error_count'],
                        '–®–≤–∏–¥–∫—ñ—Å—Ç—å': f"{pbar.format_dict['rate_fmt']}"
                    }, refresh=True)
                
                processed_count += 1
                pbar.update(1)
                
                # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–∂–Ω—ñ N –∑–∞–ø–∏—Å—ñ–≤
                if processed_count - last_stats_update >= stats_update_interval:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    speed = processed_count / elapsed if elapsed > 0 else 0
                    eta = (total_items - processed_count) / speed if speed > 0 else 0

                    pbar.set_postfix({
                        '–Ü–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ': results['indexed_count'],
                        '–ü—Ä–æ–ø—É—â–µ–Ω–æ': results['skipped_count'],
                        '–ü–æ–º–∏–ª–æ–∫': results['error_count'],
                        '–®–≤–∏–¥–∫—ñ—Å—Ç—å': f"{speed:.0f}/—Å–µ–∫",
                        '–ó–∞–ª–∏—à–∏–ª–æ—Å—å': str(timedelta(seconds=int(eta)))
                    }, refresh=True)
                    last_stats_update = processed_count
                    
            except Exception as e:
                results['error_count'] += 1
                if results['error_count'] <= 10:  # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ 10 –ø–æ–º–∏–ª–æ–∫
                    tqdm.write(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –∑–∞–ø–∏—Å—É {i}: {e}")
                pbar.update(1)
                continue
        
        # –û–±—Ä–æ–±–∫–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É
        if points_to_upsert:
            pbar.set_description("üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É")
            success_count = self._upsert_batch(points_to_upsert)
            results['indexed_count'] += success_count
            results['error_count'] += len(points_to_upsert) - success_count
        
        pbar.close()
        
        # –§—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è
        results['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['total_indexed'] = self.get_collection_size()
        self.stats['last_index_time'] = datetime.now().isoformat()
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*60)
        print("‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("="*60)
        print(f"üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –û–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞–ø–∏—Å—ñ–≤: {results['total_processed']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {results['indexed_count']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {results['skipped_count']:,}")
        print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {results['error_count']:,}")
        print(f"   ‚Ä¢ –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {timedelta(seconds=int(results['processing_time']))}")
        print(f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å: {results['total_processed']/results['processing_time']:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
        print(f"   ‚Ä¢ –†–æ–∑–º—ñ—Ä –∫–æ–ª–µ–∫—Ü—ñ—ó: {self.stats['total_indexed']:,} –∑–∞–ø–∏—Å—ñ–≤")
        print("="*60)
        self.optimize_collection()  
        return results

    def _format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —á–∞—Å—É –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥"""
        if seconds < 60:
            return f"{seconds:.0f} —Å–µ–∫"
        elif seconds < 3600:
            return f"{seconds/60:.1f} —Ö–≤"
        else:
            return f"{seconds/3600:.1f} –≥–æ–¥"

    def _get_existing_hashes(self) -> set:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤ –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –¥—É–±–ª—é–≤–∞–Ω–Ω—è"""
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ —Ç–æ—á–∫–∏ –∑ content_hash
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=10000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ª—ñ–º—ñ—Ç
                with_payload=["content_hash"]
            )
            
            hashes = set()
            points = scroll_result[0]
            
            for point in points:
                if point.payload and 'content_hash' in point.payload:
                    hashes.add(point.payload['content_hash'])
            
            # –Ø–∫—â–æ —î —â–µ —Ç–æ—á–∫–∏, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥
            next_page_offset = scroll_result[1]
            while next_page_offset:
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=10000,
                    with_payload=["content_hash"]
                )
                
                points = scroll_result[0]
                for point in points:
                    if point.payload and 'content_hash' in point.payload:
                        hashes.add(point.payload['content_hash'])
                
                next_page_offset = scroll_result[1]
            
            return hashes
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤: {e}")
            return set()
    
    def _upsert_batch(self, points: List[models.PointStruct]) -> int:
        """–í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É —Ç–æ—á–æ–∫"""
        try:
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            return len(points)
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –±–∞—Ç—á—É: {e}")
            return 0
    
    def search_similar_tenders(self, 
                             query_item: Dict, 
                             limit: int = 10,
                             similarity_threshold: float = 0.7) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
        """
        self.stats['total_searches'] += 1
        
        item_name = query_item.get('F_ITEMNAME', '')
        if not item_name:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(item_name)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            query_filter = None
            
            # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
            industry = query_item.get('F_INDUSTRYNAME')
            if industry:
                query_filter = models.Filter(
                    must=[
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=industry)
                        )
                    ]
                )
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit * 2,  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
                with_payload=True,
                score_threshold=similarity_threshold
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            similar_tenders = []
            for result in search_results:
                if len(similar_tenders) >= limit:
                    break
                
                # –£–Ω–∏–∫–∞—î–º–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å —Ç–æ–≥–æ –∂ —Ç–µ–Ω–¥–µ—Ä—É
                if (result.payload.get('tender_number') == query_item.get('F_TENDERNUMBER') and
                    result.payload.get('edrpou') == query_item.get('EDRPOU')):
                    continue
                
                similar_tender = {
                    'similarity_score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                
                similar_tenders.append(similar_tender)
            
            return similar_tenders
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤: {e}")
            return []
    
    def search_by_text(self, 
                      query: str, 
                      filters: Optional[Dict] = None,
                      limit: int = 20) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º
        """
        self.stats['total_searches'] += 1
        
        if not query:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(query)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤
            query_filter = None
            if filters:
                filter_conditions = []
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                if 'category' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="primary_category",
                            match=models.MatchValue(value=filters['category'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
                if 'industry' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=filters['industry'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –±—é–¥–∂–µ—Ç—É
                if 'min_budget' in filters or 'max_budget' in filters:
                    range_condition = {}
                    if 'min_budget' in filters:
                        range_condition['gte'] = filters['min_budget']
                    if 'max_budget' in filters:
                        range_condition['lte'] = filters['max_budget']
                    
                    filter_conditions.append(
                        models.FieldCondition(
                            key="budget",
                            range=models.Range(**range_condition)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è—Ö
                if 'won_only' in filters and filters['won_only']:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="won",
                            match=models.MatchValue(value=True)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Ö
                if 'date_from' in filters or 'date_to' in filters:
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ª–æ–≥—ñ–∫–∞ –¥–ª—è –¥–∞—Ç
                    pass
                
                if filter_conditions:
                    query_filter = models.Filter(must=filter_conditions)
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit,
                with_payload=True
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            results = []
            for result in search_results:
                tender_result = {
                    'score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                results.append(tender_result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—à—É–∫—É: {e}")
            return []
    
    def get_collection_size(self) -> int:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            return 0
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            return {
                'points_count': collection_info.points_count,
                'segments_count': len(collection_info.segments) if collection_info.segments else 0,
                'vector_size': collection_info.config.params.vectors.size,
                'distance_metric': collection_info.config.params.vectors.distance.value,
                'index_stats': self.stats,
                'status': collection_info.status.value
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {'error': str(e)}
    
    def delete_tender_records(self, tender_number: str) -> bool:
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="tender_number",
                                match=models.MatchValue(value=tender_number)
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–∏ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_number}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return False
    
    def update_tender_records(self, tender_data: List[Dict]) -> Dict[str, int]:
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        if not tender_data:
            return {'updated': 0, 'errors': 0}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞
        tender_number = tender_data[0].get('F_TENDERNUMBER')
        if not tender_number:
            return {'updated': 0, 'errors': 1}
        
        try:
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ –∑–∞–ø–∏—Å–∏
            self.delete_tender_records(tender_number)
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏
            results = self.index_tenders(tender_data, update_mode=True)
            
            return {
                'updated': results['indexed_count'],
                'errors': results['error_count']
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return {'updated': 0, 'errors': 1}
    
    def cleanup_old_records(self, days_old: int = 365) -> int:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            deleted_count = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="created_at",
                                range=models.DatetimeRange(
                                    lt=cutoff_date.isoformat()
                                )
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {e}")
            return 0
    
    def export_collection_data(self, limit: Optional[int] = None) -> List[Dict]:
        """–ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            all_points = []
            
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit or 10000,
                with_payload=True,
                with_vectors=False  # –ù–µ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            )
            
            points, next_page_offset = scroll_result
            all_points.extend([{
                'id': point.id,
                'payload': point.payload
            } for point in points])
            
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            while next_page_offset and (not limit or len(all_points) < limit):
                remaining = limit - len(all_points) if limit else 10000
                
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=min(remaining, 10000),
                    with_payload=True,
                    with_vectors=False
                )
                
                points, next_page_offset = scroll_result
                all_points.extend([{
                    'id': point.id,
                    'payload': point.payload
                } for point in points])
            
            return all_points
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É –¥–∞–Ω–∏—Ö: {e}")
            return []
    
    def get_database_health(self) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        health_status = {
            'status': 'unknown',
            'collection_exists': False,
            'points_count': 0,
            'last_indexed': self.stats.get('last_index_time'),
            'search_performance': 'unknown',
            'errors': []
        }
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            health_status['collection_exists'] = True
            health_status['points_count'] = collection_info.points_count
            
            # –¢–µ—Å—Ç –ø–æ—à—É–∫—É
            start_time = datetime.now()
            self.client.search(
                collection_name=self.collection_name,
                query_vector=[0.1] * 768,
                limit=1
            )
            search_time = (datetime.now() - start_time).total_seconds()
            
            if search_time < 0.1:
                health_status['search_performance'] = 'excellent'
            elif search_time < 0.5:
                health_status['search_performance'] = 'good'
            else:
                health_status['search_performance'] = 'slow'
            
            health_status['status'] = 'healthy'
            
        except Exception as e:
            health_status['status'] = 'error'
            health_status['errors'].append(str(e))
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è –ë–î: {e}")
        
        return health_status
    
    def search_tenders_by_group(self, group_criteria: Dict, limit: int = 20) -> List[Dict]:
        """–ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ –≥—Ä—É–ø–æ–≤–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏"""
        filter_conditions = []
        
        for field, value in group_criteria.items():
            if value is not None:
                filter_conditions.append(
                    models.FieldCondition(
                        key=field,
                        match=models.MatchValue(value=value)
                    )
                )
        
        query_filter = models.Filter(must=filter_conditions) if filter_conditions else None
        
        try:
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            return [{
                'id': point.id,
                **point.payload
            } for point in results]
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∑–∞ –≥—Ä—É–ø–æ—é: {e}")
            return []

    def get_tender_groups(self, tender_number: str) -> Dict[str, List[Dict]]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –≥—Ä—É–ø –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            query_filter = models.Filter(
                must=[
                    models.FieldCondition(
                        key="tender_number",
                        match=models.MatchValue(value=tender_number)
                    )
                ]
            )
            
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=1000,
                with_payload=True,
                with_vectors=False
            )[0]
            
            groups = defaultdict(list)
            for point in results:
                group_key = point.payload.get('group_key', 'unknown')
                groups[group_key].append(point.payload)
            
            return dict(groups)
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≥—Ä—É–ø —Ç–µ–Ω–¥–µ—Ä–∞: {e}")
            return {}

