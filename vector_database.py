import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import numpy as np
import hashlib
from tqdm import tqdm
import sys
import os
from sentence_transformers import SentenceTransformer
# Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import ResponseHandlingException, UnexpectedResponse
import json


class TenderVectorDB:
    """
    –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω—å
    
    –§—É–Ω–∫—Ü—ñ—ó:
    - –ó–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
    - –Ü–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ñ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è
    - –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
    - –ê–Ω–∞–ª—ñ–∑ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    """
    
    def __init__(self, 
                 embedding_model,
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333,
                 collection_name: str = "tender_vectors"):
        
        self.logger = logging.getLogger(__name__)
        self.embedding_model = embedding_model
        import transformers
        import datasets
        transformers.logging.set_verbosity_error()
        datasets.logging.set_verbosity_error()
        if hasattr(self.embedding_model, 'tokenizer') and self.embedding_model.tokenizer:
            self.embedding_model.tokenizer.verbose = False

        self.collection_name = collection_name
        
        # –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant
        try:
            self.client = QdrantClient(host=qdrant_host, port=qdrant_port)
            self.logger.info(f"‚úÖ –ü—ñ–¥–∫–ª—é—á–µ–Ω–æ –¥–æ Qdrant: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ Qdrant: {e}")
            raise
        

        
        # –ö–µ—à –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –æ–±—á–∏—Å–ª–µ–Ω—å
        self.embedding_cache = {}
        self.hash_cache = {}  # –î–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_indexed': 0,
            'total_updated': 0,
            'total_searches': 0,
            'last_index_time': None,
            'last_update_time': None
        }
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
        self._init_collection()

    def _create_minimal_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ú–Ü–ù–Ü–ú–ê–õ–¨–ù–ò–• —ñ–Ω–¥–µ–∫—Å—ñ–≤ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–ª—ñ–≤"""
        minimal_indexes = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("won", models.PayloadSchemaType.BOOL),
        ]
        
        for field_name, field_type in minimal_indexes:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type,
                    wait=False
                )
            except Exception as e:
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")



    def _init_collection(self):        
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"""
        collections = self.client.get_collections().collections
        collection_names = [col.name for col in collections]

        if self.collection_name in collection_names:
            collection_info = self.client.get_collection(self.collection_name)
            self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{self.collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î")
            self.stats['total_indexed'] = collection_info.points_count
        else:
            self.logger.info(f"üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{self.collection_name}' –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó...")
            try:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–µ–∫—Ü—ñ—é –±–µ–∑ HNSW —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=768,
                        distance=models.Distance.COSINE,
                        on_disk=True  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞ –¥–∏—Å–∫—É
                    ),
                    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
                    optimizers_config=models.OptimizersConfigDiff(
                        default_segment_number=4,  # –ú—ñ–Ω—ñ–º—É–º —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                        max_segment_size=3000000,  # –î—É–∂–µ –≤–µ–ª–∏–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ (5M –∑–∞–ø–∏—Å—ñ–≤)
                        memmap_threshold=100000,   # –í–µ–ª–∏–∫–∏–π –ø–æ—Ä—ñ–≥ –¥–ª—è memmap
                        indexing_threshold=99999999,  # –í–Ü–î–ö–õ–Æ–ß–ê–Ñ–ú–û –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—É —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é
                        flush_interval_sec=300,    # –†—ñ–¥—à–µ —Å–∫–∏–¥–∞–Ω–Ω—è –Ω–∞ –¥–∏—Å–∫ (5 —Ö–≤)
                        max_optimization_threads=8 # –ú—ñ–Ω—ñ–º—É–º –ø–æ—Ç–æ–∫—ñ–≤
                    ),
                    # –ù–ï –ø–µ—Ä–µ–¥–∞—î–º–æ hnsw_config –≤–∑–∞–≥–∞–ª—ñ - –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
                    # —è–∫—ñ –±—É–¥—É—Ç—å –∑–º—ñ–Ω–µ–Ω—ñ –ø—Ä–∏ —É–≤—ñ–º–∫–Ω–µ–Ω–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
                    shard_number=2,
                    replication_factor=1
                )
                
                # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç—ñ–ª—å–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏
                self._create_minimal_payload_indexes()
                self.logger.info(f"‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞ –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è")
            except Exception as e:
                self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                raise


    def _create_payload_indexes(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–ª—ñ–≤"""
        indexes_to_create = [
            ("tender_number", models.PayloadSchemaType.KEYWORD),
            ("edrpou", models.PayloadSchemaType.KEYWORD),
            ("won", models.PayloadSchemaType.BOOL),
            ("industry", models.PayloadSchemaType.KEYWORD),
            # –ù–ï —ñ–Ω–¥–µ–∫—Å—É—î–º–æ F_ITEMNAME - —Ü–µ —Ç–µ–∫—Å—Ç–æ–≤–µ –ø–æ–ª–µ, –∑–∞–π–º–∞—î –±–∞–≥–∞—Ç–æ –º—ñ—Å—Ü—è
        ]
        
        for field_name, field_type in indexes_to_create:
            try:
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name=field_name,
                    field_schema=field_type,
                    wait=False  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
                )
            except Exception as e:
                self.logger.debug(f"–Ü–Ω–¥–µ–∫—Å –¥–ª—è {field_name}: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—ñ–≤"""
        if not text or not isinstance(text, str):
            return ""
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        text = text.lower().strip()
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤, –∞–ª–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–º—ñ—Å—Ç–æ–≤–Ω–∏—Ö
        text = re.sub(r'[^\w\s\-\.\(\)]', ' ', text)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–¥–∏–Ω–∏—Ü—å –≤–∏–º—ñ—Ä—É
        units_mapping = {
            r'\b—à—Ç\.?\b': '—à—Ç—É–∫',
            r'\b–∫–≥\.?\b': '–∫—ñ–ª–æ–≥—Ä–∞–º', 
            r'\b–≥\.?\b': '–≥—Ä–∞–º',
            r'\b–ª\.?\b': '–ª—ñ—Ç—Ä',
            r'\b–º\.?\b': '–º–µ—Ç—Ä',
            r'\b—Å–º\.?\b': '—Å–∞–Ω—Ç–∏–º–µ—Ç—Ä',
            r'\b–º–º\.?\b': '–º—ñ–ª—ñ–º–µ—Ç—Ä'
        }
        
        for pattern, replacement in units_mapping.items():
            text = re.sub(pattern, replacement, text)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –º–Ω–æ–∂–∏–Ω–Ω–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _generate_content_hash(self, item: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ö–µ—à—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—Å—ñ—Ö –ø–æ–ª—ñ–≤ (–∫—Ä—ñ–º —Å–ª—É–∂–±–æ–≤–∏—Ö), null –∑–∞–º—ñ–Ω—é—é—Ç—å—Å—è –Ω–∞ ''"""
        # –ö–æ–ø—ñ—è item –±–µ–∑ —Å–ª—É–∂–±–æ–≤–∏—Ö –ø–æ–ª—ñ–≤
        exclude_fields = {'content_hash', 'created_at'}
        filtered = {k: (v if v is not None else '') for k, v in item.items() if k not in exclude_fields}
        # –°–µ—Ä—ñ–∞–ª—ñ–∑—É—î–º–æ —É json-—Ä—è–¥–æ–∫ –∑ —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è–º –∫–ª—é—á—ñ–≤
        key_str = json.dumps(filtered, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(key_str.encode('utf-8')).hexdigest()[:16]  # –ö–æ—Ä–æ—Ç—à–∏–π —Ö–µ—à

    
    def _create_embedding(self, text: str) -> Optional[np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤—Ö—ñ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        if not text or not isinstance(text, str):
            return None
        
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        processed_text = self._preprocess_text(text)
        if not processed_text or len(processed_text) < 2:
            return None
        
        try:
            # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –≤–∏–≤—ñ–¥ –ø—Ä–∏ –µ–Ω–∫–æ–¥–∏–Ω–≥—É
            with open(os.devnull, 'w') as devnull:
                old_stdout = sys.stdout
                sys.stdout = devnull
                
                embedding = self.embedding_model.encode(
                    processed_text, 
                    show_progress_bar=False,
                    batch_size=1,
                    normalize_embeddings=True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
                )
                
                sys.stdout = old_stdout
            
            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            if embedding is None:
                return None
            
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding)
            
            if embedding.shape[0] != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –µ–º–±–µ–¥–∏–Ω–≥—É: {embedding.shape[0]}")
                return None
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ NaN —Ç–∞ inf
            if np.isnan(embedding).any() or np.isinf(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å NaN –∞–±–æ inf")
                return None
            
            # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —è–∫—â–æ –Ω–µ –±—É–ª–∞ –∑—Ä–æ–±–ª–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
            else:
                self.logger.debug(f"‚ö†Ô∏è –ù—É–ª—å–æ–≤–∏–π –≤–µ–∫—Ç–æ—Ä –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
                return None
            
            # –ö–µ—à—É–≤–∞–Ω–Ω—è (–æ–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–µ—à—É)
            if len(self.embedding_cache) < 5000:
                self.embedding_cache[text] = embedding
            
            return embedding
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É: {e}")
            return None

    def _prepare_point(self, item: Dict, point_id: Optional[int] = None) -> Optional[models.PointStruct]:
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–∫–∏ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –≤ Qdrant –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é"""
        
        # ========== –í–ê–õ–Ü–î–ê–¶–Ü–Ø –í–•–Ü–î–ù–ò–• –î–ê–ù–ò–• ==========
        
        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ö –ø–æ–ª—ñ–≤
        tender_number = item.get('F_TENDERNUMBER', '')
        edrpou = item.get('EDRPOU', '')
        item_name = item.get('F_ITEMNAME', '')
        
        if not tender_number:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_TENDERNUMBER, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–ø–∏—Å")
            print (f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_TENDERNUMBER –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return ""
        
        if not edrpou:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π EDRPOU –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            print (f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π EDRPOU –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return ""
        
        if not item_name:
            self.logger.debug(f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_ITEMNAME –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            print (f"‚ö†Ô∏è –ü—É—Å—Ç–∏–π F_ITEMNAME –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return ""
        
        # 2. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è –µ–º–±–µ–¥–∏–Ω–≥—É
        item_name = item.get('F_ITEMNAME') or ''
        tender_name = item.get('F_TENDERNAME') or ''
        detail_name = item.get('F_DETAILNAME') or ''
        industry_name = item.get('F_INDUSTRYNAME') or ''
        combined_text = f"{item_name} {edrpou} {tender_number}{industry_name}".strip()
        
        if len(combined_text) < 3:
            self.logger.debug(f"‚ö†Ô∏è –ù–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç ({len(combined_text)} —Å–∏–º–≤–æ–ª—ñ–≤) –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
            return None
        
        # 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é
        try:
            embedding = self._create_embedding(combined_text)
            
            if embedding is None:
                self.logger.debug(f"‚ö†Ô∏è _create_embedding –ø–æ–≤–µ—Ä–Ω—É–≤ None –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if not isinstance(embedding, np.ndarray):
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –Ω–µ —î numpy array –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if embedding.shape[0] != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –µ–º–±–µ–¥–∏–Ω–≥—É: {embedding.shape[0]} –∑–∞–º—ñ—Å—Ç—å 768 –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if np.isnan(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å NaN –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if np.isinf(embedding).any():
                self.logger.debug(f"‚ö†Ô∏è –ï–º–±–µ–¥–∏–Ω–≥ –º—ñ—Å—Ç–∏—Ç—å inf –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
                
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
        
        # ========== –°–¢–í–û–†–ï–ù–ù–Ø –ú–ï–¢–ê–î–ê–ù–ò–• ==========
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
            metadata = {
                # –û—Å–Ω–æ–≤–Ω—ñ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏
                'tender_number': tender_number,
                'edrpou': edrpou,
                'owner_name': item.get('OWNER_NAME') or '',
                
                # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Ç–æ–≤–∞—Ä/–ø–æ—Å–ª—É–≥—É
                'item_name': item_name,
                'tender_name': tender_name,
                'detail_name': detail_name,
                
                # –ü–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫
                'supplier_name': item.get('supp_name') or '',
                
                # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è
                'industry': item.get('F_INDUSTRYNAME') or '',
                'cpv': int(item.get('CPV', 0)) if item.get('CPV') else 0,
                
                # –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –∑ –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é
                'budget': 0.0,
                'quantity': 0.0,
                'price': 0.0,
                'currency': item.get('F_TENDERCURRENCY') or 'UAH',
                'currency_rate': 1.0,
                
                # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞ –¥–∞—Ç–∏
                'won': bool(item.get('WON', False)),
                'date_end': item.get('DATEEND') or '',
                'extraction_date': item.get('EXTRACTION_DATE') or '',
                
                # –°–∏—Å—Ç–µ–º–Ω—ñ –ø–æ–ª—è
                'original_id': item.get('ID') or '',
                'created_at': datetime.now().isoformat(),
                'content_hash': self._generate_content_hash(item)
            }
            
            # –ë–µ–∑–ø–µ—á–Ω–µ –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞–Ω–Ω—è —á–∏—Å–ª–æ–≤–∏—Ö –∑–Ω–∞—á–µ–Ω—å
            try:
                budget = item.get('ITEM_BUDGET')
                if budget is not None and str(budget).strip():
                    metadata['budget'] = float(budget)
            except (ValueError, TypeError):
                pass
            
            try:
                quantity = item.get('F_qty')
                if quantity is not None and str(quantity).strip():
                    metadata['quantity'] = float(quantity)
            except (ValueError, TypeError):
                pass
            
            try:
                price = item.get('F_price')
                if price is not None and str(price).strip():
                    metadata['price'] = float(price)
            except (ValueError, TypeError):
                pass
            
            try:
                rate = item.get('F_TENDERCURRENCYRATE')
                if rate is not None and str(rate).strip():
                    metadata['currency_rate'] = float(rate)
            except (ValueError, TypeError):
                pass
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è
            group_key_parts = [
                str(edrpou),
                str(item.get('F_INDUSTRYNAME', '')),
                str(item_name),
                str(item.get('OWNER_NAME', '')),
                str(tender_name)
            ]
            
            if item.get('CPV'):
                group_key_parts.append(str(item.get('CPV')))
            
            metadata['group_key'] = '-'.join(group_key_parts)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if hasattr(self, 'category_manager') and self.category_manager:
                try:
                    categories = self.category_manager.categorize_item(item_name)
                    metadata['primary_category'] = categories[0][0] if categories else 'unknown'
                    metadata['all_categories'] = [cat[0] for cat in categories[:3]]
                    metadata['category_confidence'] = categories[0][1] if categories else 0.0
                except Exception as e:
                    self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è {tender_number}: {e}")
                    metadata['primary_category'] = 'unknown'
                    metadata['all_categories'] = []
                    metadata['category_confidence'] = 0.0
            else:
                metadata['primary_category'] = 'unknown'
                metadata['all_categories'] = []
                metadata['category_confidence'] = 0.0
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
        
        # ========== –°–¢–í–û–†–ï–ù–ù–Ø –¢–û–ß–ö–ò ==========
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID
            if point_id is None:
                point_id = int(hashlib.md5(metadata['content_hash'].encode()).hexdigest()[:8], 16)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏
            point = models.PointStruct(
                id=point_id,
                vector=embedding.tolist(),
                payload=metadata
            )
            
            # –§—ñ–Ω–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–æ—á–∫–∏
            if not hasattr(point, 'id') or point.id is None:
                self.logger.debug(f"‚ö†Ô∏è –¢–æ—á–∫–∞ –Ω–µ –º–∞—î ID –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if not hasattr(point, 'vector') or not point.vector:
                self.logger.debug(f"‚ö†Ô∏è –¢–æ—á–∫–∞ –Ω–µ –º–∞—î –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            if len(point.vector) != 768:
                self.logger.debug(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä–∞ —É —Ç–æ—á—Ü—ñ: {len(point.vector)} –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}")
                return None
            
            return point
            
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return None
    
    def optimize_collection(self):
        """–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó –ø—ñ—Å–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó"""
        try:
            # –§–æ—Ä—Å—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            self.client.update_collection(
                collection_name=self.collection_name,
                optimizer_config=models.OptimizersConfigDiff(
                    max_segment_size=500000,
                    memmap_threshold=100000,
                    indexing_threshold=100000
                )
            )
            self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞")
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: {e}")


    def _create_embeddings_batch(self, texts: List[str], batch_size: int = 128) -> List[Optional[np.ndarray]]:
        """Batch-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤ –¥–ª—è —Å–ø–∏—Å–∫—É —Ç–µ–∫—Å—Ç—ñ–≤ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞ GPU)"""
        results = []
        if not texts:
            return results
        try:
            # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –≤–∏–≤—ñ–¥ –ø—Ä–∏ –µ–Ω–∫–æ–¥–∏–Ω–≥—É
            with open(os.devnull, 'w') as devnull:
                old_stdout = sys.stdout
                sys.stdout = devnull
                embeddings = self.embedding_model.encode(
                    texts,
                    show_progress_bar=False,
                    batch_size=batch_size,
                    normalize_embeddings=True
                )
                sys.stdout = old_stdout
            # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –Ω–∞ —Å–ø–∏—Å–æ–∫ np.ndarray
            if isinstance(embeddings, np.ndarray):
                results = [embeddings[i] for i in range(embeddings.shape[0])]
            else:
                results = list(embeddings)
        except Exception as e:
            self.logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ batch-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤: {e}")
            results = [None] * len(texts)
        return results

    def index_tenders(self, 
                historical_data: List[Dict], 
                update_mode: bool = True,
                batch_size: int = 1000,
                embedding_batch_size: int = 128,
                prepare_pool_size: int = 100_000) -> Dict[str, Any]:
        """
        –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤ —É –≤–µ–∫—Ç–æ—Ä–Ω—ñ–π –±–∞–∑—ñ –∑ batch-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤
        –¢–µ–ø–µ—Ä –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ points –π–¥–µ –±–ª–æ–∫–∞–º–∏ –ø–æ prepare_pool_size, –∞ –≤—Å—Ç–∞–≤–∫–∞ —É Qdrant –ø–æ batch_size
        """
        print(f"\nüîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(historical_data):,} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"üìã –†–µ–∂–∏–º: {'–û–Ω–æ–≤–ª–µ–Ω–Ω—è' if update_mode else '–ü–æ–≤–Ω–∞ –ø–µ—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è'}")
        print(f"üì¶ –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É (Qdrant): {batch_size}")
        print(f"‚ö° Batch-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤: {embedding_batch_size}")
        print(f"üßÆ –ü—É–ª –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ points: {prepare_pool_size}")
        
        start_time = datetime.now()
        results = {
            'total_processed': len(historical_data),
            'indexed_count': 0,
            'updated_count': 0,
            'skipped_count': 0,
            'error_count': 0,
            'processing_time': 0,
            'batch_stats': []
        }
        
        if not update_mode:
            try:
                self.client.delete_collection(self.collection_name)
                self.logger.info("üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é")
                self._init_collection()
                self.logger.info("‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–∞")
            except Exception as e:
                self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                import traceback
                self.logger.debug(f"–ü–æ–≤–Ω–∏–π traceback: {traceback.format_exc()}")


        
        existing_hashes = set()
        if update_mode:
            load_start = datetime.now()
            existing_hashes = self._get_existing_hashes()
            load_time = (datetime.now() - load_start).total_seconds()
            print(f"‚è±Ô∏è –ß–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ö–µ—à—ñ–≤: {load_time:.1f} —Å–µ–∫\n")
        
        new_items = []
        print("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏...")
        
        for item in historical_data:
            content_hash = self._generate_content_hash(item)
            if not update_mode or content_hash not in existing_hashes:
                new_items.append(item)
            else:
                results['skipped_count'] += 1
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏:")
        print(f"   ‚Ä¢ –ù–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {len(new_items):,}")
        print(f"   ‚Ä¢ –î—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {results['skipped_count']:,}")
        
        if not new_items:
            print("\n‚úÖ –í—Å—ñ –∑–∞–ø–∏—Å–∏ –≤–∂–µ —î –≤ –±–∞–∑—ñ")
            results['processing_time'] = (datetime.now() - start_time).total_seconds()
            return results
        
        print(f"\nüöÄ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è {len(new_items):,} –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤:")
        
        pbar = tqdm(
            total=len(new_items),
            desc="–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è",
            unit="–∑–∞–ø–∏—Å—ñ–≤",
            ncols=100
        )
        
        points_to_upsert = []
        batch_num = 0
        point_creation_errors = 0
        
        # –î–û–î–ê–ù–û: —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        points_prepared = 0
        points_added_to_batch = 0
        
        i = 0
        while i < len(new_items):
            # –§–æ—Ä–º—É—î–º–æ –ø—ñ–¥–±–∞—Ç—á –¥–ª—è –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤
            batch_items = new_items[i:i+embedding_batch_size]
            texts = []
            valid_indices = []
            for idx, item in enumerate(batch_items):
                tender_number = item.get('F_TENDERNUMBER', '')
                edrpou = item.get('EDRPOU', '')
                item_name = item.get('F_ITEMNAME', '')
                tender_name = item.get('F_TENDERNAME', '')
                detail_name = item.get('F_DETAILNAME', '')
                combined_text = f"{item_name} {tender_name} {detail_name}".strip()
                if not tender_number or not edrpou or not item_name or len(combined_text) < 3:
                    continue
                texts.append(combined_text)
                valid_indices.append(idx)
            # Batch-—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –µ–º–±–µ–¥–¥–∏–Ω–≥—ñ–≤
            embeddings = self._create_embeddings_batch(texts, batch_size=embedding_batch_size)
            for rel_idx, emb in enumerate(embeddings):
                item = batch_items[valid_indices[rel_idx]]
                points_prepared += 1
                if emb is None or not isinstance(emb, np.ndarray) or emb.shape[0] != 768 or np.isnan(emb).any() or np.isinf(emb).any():
                    point_creation_errors += 1
                    if point_creation_errors <= 10:
                        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Å—É {i+valid_indices[rel_idx]}")
                    continue
                # ...—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —è–∫ —É _prepare_point...
                try:
                    metadata = {
                        'tender_number': item.get('F_TENDERNUMBER', ''),
                        'edrpou': item.get('EDRPOU', ''),
                        'owner_name': item.get('OWNER_NAME') or '',
                        'item_name': item.get('F_ITEMNAME', ''),
                        'tender_name': item.get('F_TENDERNAME', ''),
                        'detail_name': item.get('F_DETAILNAME', ''),
                        'supplier_name': item.get('supp_name') or '',
                        'industry': item.get('F_INDUSTRYNAME') or '',
                        'cpv': int(item.get('CPV', 0)) if item.get('CPV') else 0,
                        'budget': 0.0,
                        'quantity': 0.0,
                        'price': 0.0,
                        'currency': item.get('F_TENDERCURRENCY') or 'UAH',
                        'currency_rate': 1.0,
                        'won': bool(item.get('WON', False)),
                        'date_end': item.get('DATEEND') or '',
                        'extraction_date': item.get('EXTRACTION_DATE') or '',
                        'original_id': item.get('ID') or '',
                        'created_at': datetime.now().isoformat(),
                        'content_hash': self._generate_content_hash(item)
                    }
                    try:
                        budget = item.get('ITEM_BUDGET')
                        if budget is not None and str(budget).strip():
                            metadata['budget'] = float(budget)
                    except (ValueError, TypeError):
                        pass
                    try:
                        quantity = item.get('F_qty')
                        if quantity is not None and str(quantity).strip():
                            metadata['quantity'] = float(quantity)
                    except (ValueError, TypeError):
                        pass
                    try:
                        price = item.get('F_price')
                        if price is not None and str(price).strip():
                            metadata['price'] = float(price)
                    except (ValueError, TypeError):
                        pass
                    try:
                        rate = item.get('F_TENDERCURRENCYRATE')
                        if rate is not None and str(rate).strip():
                            metadata['currency_rate'] = float(rate)
                    except (ValueError, TypeError):
                        pass
                    group_key_parts = [
                        str(metadata['edrpou']),
                        str(metadata['industry']),
                        str(metadata['item_name']),
                        str(metadata['owner_name']),
                        str(metadata['tender_name'])
                    ]
                    if item.get('CPV'):
                        group_key_parts.append(str(item.get('CPV')))
                    metadata['group_key'] = '-'.join(group_key_parts)
                    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
                    if hasattr(self, 'category_manager') and self.category_manager:
                        try:
                            categories = self.category_manager.categorize_item(metadata['item_name'])
                            metadata['primary_category'] = categories[0][0] if categories else 'unknown'
                            metadata['all_categories'] = [cat[0] for cat in categories[:3]]
                            metadata['category_confidence'] = categories[0][1] if categories else 0.0
                        except Exception as e:
                            self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è {metadata['tender_number']}: {e}")
                            metadata['primary_category'] = 'unknown'
                            metadata['all_categories'] = []
                            metadata['category_confidence'] = 0.0
                    else:
                        metadata['primary_category'] = 'unknown'
                        metadata['all_categories'] = []
                        metadata['category_confidence'] = 0.0
                    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ID
                    point_id = int(hashlib.md5(metadata['content_hash'].encode()).hexdigest()[:8], 16)
                    point = models.PointStruct(
                        id=point_id,
                        vector=emb.tolist(),
                        payload=metadata
                    )
                    points_to_upsert.append(point)
                    points_added_to_batch += 1
                except Exception as e:
                    point_creation_errors += 1
                    if point_creation_errors <= 10:
                        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–∫–∏ –¥–ª—è –∑–∞–ø–∏—Å—É {i+valid_indices[rel_idx]}: {e}")
                    continue
            # –Ø–∫—â–æ –Ω–∞–∫–æ–ø–∏—á–∏–ª–∏ –ø—É–ª –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            if len(points_to_upsert) >= prepare_pool_size:
                # –í—Å—Ç–∞–≤–ª—è—î–º–æ —É Qdrant –ø–æ batch_size
                while len(points_to_upsert) >= batch_size:
                    batch_num += 1
                    batch_points = points_to_upsert[:batch_size]
                    print(f"\nüîÄ –ë–∞—Ç—á #{batch_num}: –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {points_prepared} –∑–∞–ø–∏—Å—ñ–≤, —É –±–∞—Ç—á—ñ {len(batch_points)} —Ç–æ—á–æ–∫")
                    batch_start = datetime.now()
                    success_count = self._upsert_batch(batch_points, batch_num)
                    batch_time = (datetime.now() - batch_start).total_seconds()
                    batch_stat = {
                        'batch_num': batch_num,
                        'points_prepared': len(batch_points),
                        'points_uploaded': success_count,
                        'points_failed': len(batch_points) - success_count,
                        'batch_time': batch_time
                    }
                    results['batch_stats'].append(batch_stat)
                    print(f"‚úÖ –ë–∞—Ç—á #{batch_num}: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {success_count}/{len(batch_points)} —Ç–æ—á–æ–∫ –∑–∞ {batch_time:.2f}—Å")
                    results['indexed_count'] += success_count
                    results['error_count'] += len(batch_points) - success_count
                    # –î–æ–¥–∞—î–º–æ –ª–æ–≥ –ø–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ
                    current_count = self.get_collection_size()
                    print(f"üìä –ü–æ—Ç–æ—á–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ –ø—ñ—Å–ª—è –±–∞—Ç—á—É #{batch_num}: {current_count}")
                    points_to_upsert = points_to_upsert[batch_size:]
                    points_added_to_batch = len(points_to_upsert)
                    if batch_num % 10 == 0:
                        elapsed = (datetime.now() - start_time).total_seconds()
                        speed = results['indexed_count'] / elapsed if elapsed > 0 else 0
                        pbar.set_postfix(
                            batches=batch_num,
                            indexed=f"{results['indexed_count']:,}",
                            speed=f"{speed:.0f}/—Å"
                        )
            pbar.update(len(batch_items))
            i += embedding_batch_size
        # –í—Å—Ç–∞–≤–∫–∞ –∑–∞–ª–∏—à–∫—É
        if points_to_upsert:
            while len(points_to_upsert) > 0:
                batch_num += 1
                batch_points = points_to_upsert[:batch_size]
                print(f"\nüîÄ –ë–∞—Ç—á #{batch_num}: {len(batch_points)} —Ç–æ—á–æ–∫ (–∑–∞–ª–∏—à–æ–∫)")
                batch_start = datetime.now()
                success_count = self._upsert_batch(batch_points, batch_num)
                batch_time = (datetime.now() - batch_start).total_seconds()
                batch_stat = {
                    'batch_num': batch_num,
                    'points_prepared': len(batch_points),
                    'points_uploaded': success_count,
                    'points_failed': len(batch_points) - success_count,
                    'batch_time': batch_time
                }
                results['batch_stats'].append(batch_stat)
                print(f"‚úÖ –ë–∞—Ç—á #{batch_num}: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {success_count}/{len(batch_points)} —Ç–æ—á–æ–∫")
                results['indexed_count'] += success_count
                results['error_count'] += len(batch_points) - success_count
                points_to_upsert = points_to_upsert[batch_size:]
        pbar.close()
        
        # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á—ñ–≤
        if results['batch_stats']:
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á—ñ–≤:")
            total_prepared = sum(b['points_prepared'] for b in results['batch_stats'])
            total_uploaded = sum(b['points_uploaded'] for b in results['batch_stats'])
            avg_batch_size = total_prepared / len(results['batch_stats'])
            
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –±–∞—Ç—á—ñ–≤: {len(results['batch_stats'])}")
            print(f"   ‚Ä¢ –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É: {avg_batch_size:.1f}")
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ —Ç–æ—á–æ–∫: {total_prepared:,}")
            print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç–æ—á–æ–∫: {total_uploaded:,}")
            print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ—á–æ–∫: {point_creation_errors}")
            
            # –ü–µ—Ä—à—ñ –∫—ñ–ª—å–∫–∞ –±–∞—Ç—á—ñ–≤ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            print(f"\nüìã –ü–µ—Ä—à—ñ 5 –±–∞—Ç—á—ñ–≤:")
            for batch in results['batch_stats'][:5]:
                print(f"   –ë–∞—Ç—á {batch['batch_num']}: {batch['points_prepared']}/{batch_size} –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ, "
                    f"{batch['points_uploaded']} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ, {batch['batch_time']:.2f}—Å")
        
        # –§—ñ–Ω–∞–ª—ñ–∑–∞—Ü—ñ—è
        results['processing_time'] = (datetime.now() - start_time).total_seconds()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats['total_indexed'] = self.get_collection_size()
        self.stats['last_index_time'] = datetime.now().isoformat()
        
        # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*60)
        print("‚úÖ –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("="*60)
        print(f"üìä –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   ‚Ä¢ –û–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞–ø–∏—Å—ñ–≤: {results['total_processed']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ –Ω–æ–≤–∏—Ö: {results['indexed_count']:,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {results['skipped_count']:,}")
        print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫ –æ–±—Ä–æ–±–∫–∏: {results['error_count']:,}")
        print(f"   ‚Ä¢ –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {results['processing_time']:.1f} —Å–µ–∫")
        
        if results['indexed_count'] > 0:
            print(f"   ‚Ä¢ –®–≤–∏–¥–∫—ñ—Å—Ç—å: {results['indexed_count']/results['processing_time']:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
        
        print(f"\nüìä –°—Ç–∞–Ω –∫–æ–ª–µ–∫—Ü—ñ—ó:")
        print(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ: {self.stats['total_indexed']:,}")
        print("="*60)
        
        return results


    def _format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —á–∞—Å—É –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥"""
        if seconds < 60:
            return f"{seconds:.0f} —Å–µ–∫"
        elif seconds < 3600:
            return f"{seconds/60:.1f} —Ö–≤"
        else:
            return f"{seconds/3600:.1f} –≥–æ–¥"

    def _get_existing_hashes(self) -> set:
        """–°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤ –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É"""
        try:
            hashes = set()
            offset = None
            batch_count = 0
            
            print(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤...")
            
            while True:
                # –û—Ç—Ä–∏–º—É—î–º–æ –±–∞—Ç—á –∑–∞–ø–∏—Å—ñ–≤
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=offset,
                    limit=10000,
                    with_payload=["content_hash"],
                    with_vectors=False
                )
                
                points, next_offset = scroll_result
                
                if not points:
                    break
                    
                # –î–æ–¥–∞—î–º–æ —Ö–µ—à—ñ
                for point in points:
                    if point.payload and 'content_hash' in point.payload:
                        hashes.add(point.payload['content_hash'])
                
                batch_count += 1
                if batch_count % 10 == 0:
                    print(f"   ‚Ä¢ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(hashes):,} —Ö–µ—à—ñ–≤...")
                
                if not next_offset:
                    break
                    
                offset = next_offset
            
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(hashes):,} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ö–µ—à—ñ–≤")
            return hashes
            
        except Exception as e:            
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ö–µ—à—ñ–≤: {e}")
            return set()
    
    def _upsert_batch(self, points: List[models.PointStruct], batch_num: int = 0) -> int:
        """–®–≤–∏–¥–∫–∞ –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è"""
        try:
            print(f"üì° –®–≤–∏–¥–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—É #{batch_num} –∑ {len(points)} —Ç–æ—á–æ–∫...")
            
            # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è - —Ç—ñ–ª—å–∫–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å
            if not points:
                print(f"‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π –±–∞—Ç—á #{batch_num}")
                return 0
            
            # üî• –®–í–ò–î–ö–ê –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
            self.client.upsert(
                collection_name=self.collection_name,
                points=points,
                wait=False,  # –ù–ï —á–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è!
                ordering=models.WriteOrdering.WEAK  # –°–ª–∞–±–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
            )
            
            print(f"‚ö° –ë–∞—Ç—á #{batch_num} –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ")
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ —É—Å–ø—ñ—Ö)
            return len(points)
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —à–≤–∏–¥–∫–æ—ó –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –±–∞—Ç—á—É #{batch_num}: {e}")
            print(f"   –¢–∏–ø –ø–æ–º–∏–ª–∫–∏: {type(e).__name__}")
        return 0


    
    def search_similar_tenders(self, 
                             query_item: Dict, 
                             limit: int = 10,
                             similarity_threshold: float = 0.7) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é —Å—Ö–æ–∂—ñ—Å—Ç—é
        """
        self.stats['total_searches'] += 1
        
        item_name = query_item.get('F_ITEMNAME', '')
        if not item_name:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(item_name)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            query_filter = None
            
            # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
            industry = query_item.get('F_INDUSTRYNAME')
            if industry:
                query_filter = models.Filter(
                    must=[
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=industry)
                        )
                    ]
                )
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit * 2,  # –ë–µ—Ä–µ–º–æ –±—ñ–ª—å—à–µ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
                with_payload=True,
                score_threshold=similarity_threshold
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            similar_tenders = []
            for result in search_results:
                if len(similar_tenders) >= limit:
                    break
                
                # –£–Ω–∏–∫–∞—î–º–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω—å —Ç–æ–≥–æ –∂ —Ç–µ–Ω–¥–µ—Ä–∞
                if (result.payload.get('tender_number') == query_item.get('F_TENDERNUMBER') and
                    result.payload.get('edrpou') == query_item.get('EDRPOU')):
                    continue
                
                similar_tender = {
                    'similarity_score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                
                similar_tenders.append(similar_tender)
            
            return similar_tenders
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö —Ç–µ–Ω–¥–µ—Ä—ñ–≤: {e}")
            return []
    
    def search_by_text(self, 
                      query: str, 
                      filters: Optional[Dict] = None,
                      limit: int = 20) -> List[Dict]:
        """
        –ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏–º –∑–∞–ø–∏—Ç–æ–º
        """
        self.stats['total_searches'] += 1
        
        if not query:
            return []
        
        try:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥–∏–Ω–≥—É –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self._create_embedding(query)
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä—ñ–≤
            query_filter = None
            if filters:
                filter_conditions = []
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó
                if 'category' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="primary_category",
                            match=models.MatchValue(value=filters['category'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—ó
                if 'industry' in filters:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="industry",
                            match=models.MatchValue(value=filters['industry'])
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –±—é–¥–∂–µ—Ç—É
                if 'min_budget' in filters or 'max_budget' in filters:
                    range_condition = {}
                    if 'min_budget' in filters:
                        range_condition['gte'] = filters['min_budget']
                    if 'max_budget' in filters:
                        range_condition['lte'] = filters['max_budget']
                    
                    filter_conditions.append(
                        models.FieldCondition(
                            key="budget",
                            range=models.Range(**range_condition)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è—Ö
                if 'won_only' in filters and filters['won_only']:
                    filter_conditions.append(
                        models.FieldCondition(
                            key="won",
                            match=models.MatchValue(value=True)
                        )
                    )
                
                # –§—ñ–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞—Ö
                if 'date_from' in filters or 'date_to' in filters:
                    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ª–æ–≥—ñ–∫–∞ –¥–ª—è –¥–∞—Ç
                    pass
                
                if filter_conditions:
                    query_filter = models.Filter(must=filter_conditions)
            
            # –ü–æ—à—É–∫
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding.tolist(),
                query_filter=query_filter,
                limit=limit,
                with_payload=True
            )
            
            # –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            results = []
            for result in search_results:
                tender_result = {
                    'score': float(result.score),
                    'tender_number': result.payload.get('tender_number', ''),
                    'supplier_name': result.payload.get('supplier_name', ''),
                    'edrpou': result.payload.get('edrpou', ''),
                    'item_name': result.payload.get('item_name', ''),
                    'industry': result.payload.get('industry', ''),
                    'category': result.payload.get('primary_category', 'unknown'),
                    'budget': result.payload.get('budget', 0),
                    'won': result.payload.get('won', False),
                    'date_end': result.payload.get('date_end', ''),
                    'cpv': result.payload.get('cpv', 0)
                }
                results.append(tender_result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ—à—É–∫—É: {e}")
            return []
    
    def get_collection_size(self) -> int:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return collection_info.points_count
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            return 0
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            collection_info = self.client.get_collection(self.collection_name)
            
            return {
                'points_count': collection_info.points_count,
                'segments_count': len(collection_info.segments) if collection_info.segments else 0,
                'vector_size': collection_info.config.params.vectors.size,
                'distance_metric': collection_info.config.params.vectors.distance.value,
                'index_stats': self.stats,
                'status': collection_info.status.value
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {'error': str(e)}
    
    def delete_tender_records(self, tender_number: str) -> bool:
        """–í–∏–¥–∞–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="tender_number",
                                match=models.MatchValue(value=tender_number)
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–∏ —Ç–µ–Ω–¥–µ—Ä–∞: {tender_number}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return False
    
    def update_tender_records(self, tender_data: List[Dict]) -> Dict[str, int]:
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ø–∏—Å—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        if not tender_data:
            return {'updated': 0, 'errors': 0}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞
        tender_number = tender_data[0].get('F_TENDERNUMBER')
        if not tender_number:
            return {'updated': 0, 'errors': 1}
        
        try:
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ –∑–∞–ø–∏—Å–∏
            self.delete_tender_records(tender_number)
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏
            results = self.index_tenders(tender_data, update_mode=True)
            
            return {
                'updated': results['indexed_count'],
                'errors': results['error_count']
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}: {e}")
            return {'updated': 0, 'errors': 1}
    
    def cleanup_old_records(self, days_old: int = 365) -> int:
        """–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        try:
            deleted_count = self.client.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="created_at",
                                range=models.DatetimeRange(
                                    lt=cutoff_date.isoformat()
                                )
                            )
                        ]
                    )
                )
            )
            
            self.logger.info(f"üßπ –í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {e}")
            return 0
    
    def export_collection_data(self, limit: Optional[int] = None) -> List[Dict]:
        """–ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        try:
            all_points = []
            
            scroll_result = self.client.scroll(
                collection_name=self.collection_name,
                limit=limit or 10000,
                with_payload=True,
                with_vectors=False  # –ù–µ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –≤–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            )
            
            points, next_page_offset = scroll_result
            all_points.extend([{
                'id': point.id,
                'payload': point.payload
            } for point in points])
            
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ —Å–∫—Ä–æ–ª–∏–Ω–≥ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            while next_page_offset and (not limit or len(all_points) < limit):
                remaining = limit - len(all_points) if limit else 10000
                
                scroll_result = self.client.scroll(
                    collection_name=self.collection_name,
                    offset=next_page_offset,
                    limit=min(remaining, 10000),
                    with_payload=True,
                    with_vectors=False
                )
                
                points, next_page_offset = scroll_result
                all_points.extend([{
                    'id': point.id,
                    'payload': point.payload
                } for point in points])
            
            return all_points
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É –¥–∞–Ω–∏—Ö: {e}")
            return []
    
    def get_database_health(self) -> Dict[str, Any]:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        health_status = {
            'status': 'unknown',
            'collection_exists': False,
            'points_count': 0,
            'last_indexed': self.stats.get('last_index_time'),
            'search_performance': 'unknown',
            'errors': []
        }
        
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
            collection_info = self.client.get_collection(self.collection_name)
            health_status['collection_exists'] = True
            health_status['points_count'] = collection_info.points_count
            
            # –¢–µ—Å—Ç –ø–æ—à—É–∫—É
            start_time = datetime.now()
            self.client.search(
                collection_name=self.collection_name,
                query_vector=[0.1] * 768,
                limit=1
            )
            search_time = (datetime.now() - start_time).total_seconds()
            
            if search_time < 0.1:
                health_status['search_performance'] = 'excellent'
            elif search_time < 0.5:
                health_status['search_performance'] = 'good'
            else:
                health_status['search_performance'] = 'slow'
            
            health_status['status'] = 'healthy'
            
        except Exception as e:
            health_status['status'] = 'error'
            health_status['errors'].append(str(e))
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è –ë–î: {e}")
        
        return health_status
    
    def search_tenders_by_group(self, group_criteria: Dict, limit: int = 20) -> List[Dict]:
        """–ü–æ—à—É–∫ —Ç–µ–Ω–¥–µ—Ä—ñ–≤ –∑–∞ –≥—Ä—É–ø–æ–≤–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏"""
        filter_conditions = []
        
        for field, value in group_criteria.items():
            if value is not None:
                filter_conditions.append(
                    models.FieldCondition(
                        key=field,
                        match=models.MatchValue(value=value)
                    )
                )
        
        query_filter = models.Filter(must=filter_conditions) if filter_conditions else None
        
        try:
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )[0]
            
            return [{
                'id': point.id,
                **point.payload
            } for point in results]
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –∑–∞ –≥—Ä—É–ø–æ—é: {e}")
            return []

    def get_tender_groups(self, tender_number: str) -> Dict[str, List[Dict]]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –≥—Ä—É–ø –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–¥–µ—Ä–∞"""
        try:
            query_filter = models.Filter(
                must=[
                    models.FieldCondition(
                        key="tender_number",
                        match=models.MatchValue(value=tender_number)
                    )
                ]
            )
            
            results = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=query_filter,
                limit=1000,
                with_payload=True,
                with_vectors=False
            )[0]
            
            groups = defaultdict(list)
            for point in results:
                group_key = point.payload.get('group_key', 'unknown')
                groups[group_key].append(point.payload)
            
            return dict(groups)
            
        except Exception as e:
            self.logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –≥—Ä—É–ø —Ç–µ–Ω–¥–µ—Ä–∞: {e}")
            return {}

    def get_all_supplier_ids(self) -> list:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –≤—Å—ñ —É–Ω—ñ–∫–∞–ª—å–Ω—ñ EDRPOU (—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏ –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫—ñ–≤) –∑ –∫–æ–ª–µ–∫—Ü—ñ—ó Qdrant.
        """
        client = self.client  # ‚Üê –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Ç—É—Ç
        collection = self.collection_name if hasattr(self, 'collection_name') else "tender_vectors"

        edrpou_set = set()
        offset = None

        while True:
            response = client.scroll(
                collection_name=collection,
                limit=1000,
                offset=offset,
                with_payload=True,
                scroll_filter=None  # ‚Üê –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ —Ç—É—Ç
            )
            points = response[0]
            if not points:
                break
            for point in points:
                edrpou = point.payload.get("edrpou")
                if edrpou:
                    edrpou_set.add(edrpou)
            if len(points) < 1000:
                break
            offset = points[-1].id

        return list(edrpou_set)

