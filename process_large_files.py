import json
import logging
from pathlib import Path
from datetime import datetime
import gc
import sys
from tender_analysis_system import TenderAnalysisSystem

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def process_large_files(file_paths: list, 
                       categories_file: str = "categories.jsonl",
                       mapping_file: str = "category_mappings.json",
                       batch_size: int = 1000,
                       max_records_per_file: int = None):
    """
    –û–±—Ä–æ–±–∫–∞ –≤–µ–ª–∏–∫–∏—Ö JSONL —Ñ–∞–π–ª—ñ–≤ –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—î—é –ø–∞–º'—è—Ç—ñ
    
    Args:
        file_paths: —Å–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤ –¥–æ JSONL —Ñ–∞–π–ª—ñ–≤
        categories_file: —Ñ–∞–π–ª –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
        mapping_file: —Ñ–∞–π–ª –∑ –º–∞–ø–ø—ñ–Ω–≥–æ–º –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        max_records_per_file: –º–∞–∫—Å–∏–º—É–º –∑–∞–ø–∏—Å—ñ–≤ –∑ –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª—É (–¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)
    """
    
    # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
    logger.info("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏...")
    system = TenderAnalysisSystem(categories_file=categories_file)
    
    if not system.initialize_system():
        logger.error("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏")
        return
    
    # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–∞–ø–ø—ñ–Ω–≥—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    if Path(mapping_file).exists():
        logger.info(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–∞–ø–ø—ñ–Ω–≥—É –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑ {mapping_file}")
        system.categories_manager.load_category_mappings(mapping_file)
    
    # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_stats = {
        'total_processed': 0,
        'total_indexed': 0,
        'total_errors': 0,
        'processing_time': 0,
        'file_stats': {}
    }
    
    overall_start = datetime.now()
    
    # 4. –û–±—Ä–æ–±–∫–∞ –∫–æ–∂–Ω–æ–≥–æ —Ñ–∞–π–ª—É
    for file_idx, file_path in enumerate(file_paths):
        if not Path(file_path).exists():
            logger.error(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            continue
        
        file_size_gb = Path(file_path).stat().st_size / (1024**3)
        logger.info(f"\nüìÅ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É {file_idx + 1}/{len(file_paths)}: {file_path}")
        logger.info(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size_gb:.2f} –ì–ë")
        
        file_start = datetime.now()
        file_stats = {
            'records_read': 0,
            'records_indexed': 0,
            'errors': 0,
            'batches': 0
        }
        
        # –ë–∞—Ç—á–µ–≤–∞ –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É
        batch_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    # –û–±–º–µ–∂–µ–Ω–Ω—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
                    if max_records_per_file and line_num > max_records_per_file:
                        logger.info(f"‚èπÔ∏è –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç {max_records_per_file} –∑–∞–ø–∏—Å—ñ–≤")
                        break
                    
                    try:
                        # –ü–∞—Ä—Å–∏–Ω–≥ JSON
                        record = json.loads(line.strip())
                        batch_data.append(record)
                        file_stats['records_read'] += 1
                        
                        # –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—É
                        if len(batch_data) >= batch_size:
                            logger.info(f"üîÑ –û–±—Ä–æ–±–∫–∞ –±–∞—Ç—á—É {file_stats['batches'] + 1} "
                                      f"({len(batch_data)} –∑–∞–ø–∏—Å—ñ–≤)...")
                            
                            # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É
                            index_stats = system.vector_db.index_tenders(
                                batch_data,
                                update_mode=True,  # –î–æ–¥–∞—î–º–æ –¥–æ —ñ—Å–Ω—É—é—á–æ—ó –±–∞–∑–∏
                                batch_size=100  # –ú–µ–Ω—à–∏–π –±–∞—Ç—á –¥–ª—è Qdrant
                            )
                            
                            file_stats['records_indexed'] += index_stats['indexed_count']
                            file_stats['errors'] += index_stats['error_count']
                            file_stats['batches'] += 1
                            
                            # –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
                            batch_data.clear()
                            gc.collect()
                            
                            # –ü—Ä–æ–≥—Ä–µ—Å
                            if line_num % 10000 == 0:
                                elapsed = (datetime.now() - file_start).total_seconds()
                                speed = line_num / elapsed if elapsed > 0 else 0
                                logger.info(f"üìà –û–±—Ä–æ–±–ª–µ–Ω–æ {line_num:,} —Ä—è–¥–∫—ñ–≤ "
                                          f"({speed:.0f} —Ä—è–¥–∫—ñ–≤/—Å–µ–∫)")
                    
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ JSON –≤ —Ä—è–¥–∫—É {line_num}: {e}")
                        file_stats['errors'] += 1
                    except Exception as e:
                        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ä—è–¥–∫–∞ {line_num}: {e}")
                        file_stats['errors'] += 1
                
                # –û–±—Ä–æ–±–∫–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É
                if batch_data:
                    logger.info(f"üîÑ –û–±—Ä–æ–±–∫–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ç—á—É ({len(batch_data)} –∑–∞–ø–∏—Å—ñ–≤)...")
                    index_stats = system.vector_db.index_tenders(
                        batch_data,
                        update_mode=True,
                        batch_size=100
                    )
                    file_stats['records_indexed'] += index_stats['indexed_count']
                    file_stats['errors'] += index_stats['error_count']
        
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ —Ñ–∞–π–ª—É: {e}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª—É
        file_time = (datetime.now() - file_start).total_seconds()
        file_stats['processing_time'] = file_time
        
        logger.info(f"\n‚úÖ –§–∞–π–ª –æ–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞ {file_time:.1f} —Å–µ–∫")
        logger.info(f"üìä –ü—Ä–æ—á–∏—Ç–∞–Ω–æ: {file_stats['records_read']:,}")
        logger.info(f"üìä –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {file_stats['records_indexed']:,}")
        logger.info(f"üìä –ü–æ–º–∏–ª–æ–∫: {file_stats['errors']:,}")
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_stats['file_stats'][file_path] = file_stats
        total_stats['total_processed'] += file_stats['records_read']
        total_stats['total_indexed'] += file_stats['records_indexed']
        total_stats['total_errors'] += file_stats['errors']
        
        # –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ –º—ñ–∂ —Ñ–∞–π–ª–∞–º–∏
        gc.collect()
    
    # 5. –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_stats['processing_time'] = (datetime.now() - overall_start).total_seconds()
    
    logger.info("\n" + "="*50)
    logger.info("üìä –ü–Ü–î–°–£–ú–ö–û–í–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    logger.info(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {total_stats['processing_time']:.1f} —Å–µ–∫")
    logger.info(f"üìÑ –û–±—Ä–æ–±–ª–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤: {len(file_paths)}")
    logger.info(f"üìù –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_stats['total_processed']:,}")
    logger.info(f"‚úÖ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {total_stats['total_indexed']:,}")
    logger.info(f"‚ùå –ü–æ–º–∏–ª–æ–∫: {total_stats['total_errors']:,}")
    
    # 6. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
    db_stats = system.vector_db.get_collection_stats()
    logger.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏:")
    logger.info(f"üóÑÔ∏è –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {db_stats['points_count']:,}")
    
    # 7. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏
    logger.info("\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏...")
    system.save_system("tender_system_large.pkl")
    
    return total_stats

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    # –í–∞—à—ñ —Ñ–∞–π–ª–∏
    files = [
        "tender_data_part1.jsonl",  # 3.5 GB
        "tender_data_part2.jsonl"   # 3.5 GB
    ]
    
    # –°–ø–æ—á–∞—Ç–∫—É —Å—Ç–≤–æ—Ä—ñ—Ç—å –º–∞–ø–ø—ñ–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
    from category_mapping import analyze_categories_and_create_mapping
    analyze_categories_and_create_mapping("categories.jsonl")
    
    # –ü–æ—Ç—ñ–º –∑–∞–ø—É—Å—Ç—ñ—Ç—å –æ–±—Ä–æ–±–∫—É
    stats = process_large_files(
        file_paths=files,
        batch_size=1000,  # –ú–æ–∂–Ω–∞ –∑–±—ñ–ª—å—à–∏—Ç–∏ —è–∫—â–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ RAM
        max_records_per_file=None  # –î–ª—è —Ç–µ—Å—Ç—É –º–æ–∂–Ω–∞ –ø–æ—Å—Ç–∞–≤–∏—Ç–∏ 10000
    )