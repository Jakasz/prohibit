import json
import numpy as np
import pandas as pd
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_auc_score, precision_recall_curve, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.utils.class_weight import compute_sample_weight
from typing import Dict, List, Tuple, Optional, Any
import pickle
import joblib
from datetime import datetime
import matplotlib.pyplot as plt

from profiles.supplier_profiler import SupplierProfile

# –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    from scipy.optimize import minimize
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# –õ–æ–∫–∞–ª—å–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏ (–¥–æ–¥–∞—Ç–∏ –ø—ñ—Å–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤)
from features.feature_extractor import FeatureExtractor
from features.feature_processor import AdvancedFeatureProcessor
from AI.model_monitor import ModelMonitor

class PredictionEngine:
    """–û—Å–Ω–æ–≤–Ω–∏–π –¥–≤–∏–≥—É–Ω –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ç–µ–Ω–¥–µ—Ä—ñ–≤"""

    def __init__(self, supplier_profiler, categories_manager):
        self.supplier_profiler = supplier_profiler        
        self.categories_manager = categories_manager
        self.feature_extractor = FeatureExtractor(categories_manager)
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.model_performance = {}
        self.monitor = ModelMonitor()        
        self.logger = logging.getLogger(__name__)

        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π
        self.model_configs = {
            'random_forest': {
                'class': RandomForestClassifier,
                'params': {
                    'n_estimators': 200,
                    'max_depth': 15,
                    'min_samples_split': 20,
                    'min_samples_leaf': 15,
                    'max_features': 'sqrt',
                    'n_jobs': -1,
                    'random_state': 42
                }
            },
            'gradient_boosting': {
                'class': GradientBoostingClassifier,
                'params': {
                    'n_estimators': 250,
                    'learning_rate': 0.05,
                    'max_depth': 5,
                    'subsample': 0.7,
                    'random_state': 42
                }
            }
        }
        if XGBOOST_AVAILABLE:
            self.model_configs['xgboost'] = {
                'class': XGBClassifier,
                'params': {
                    'n_estimators': 300,
                    'max_depth': 8,
                    'learning_rate': 0.05,
                    'subsample': 0.7,
                    'colsample_bytree': 0.8,
                    'min_child_weight': 10,
                    'gamma': 0.1,
                    'random_state': 42,
                    'n_jobs': -1,
                    'eval_metric': 'logloss',
                    'tree_method': 'hist'
                }
            }


        self.ensemble_weights = {}
        self.calibrated_models = {}

    def analyze_with_shap(self, X_sample=None, sample_size=1000, save_plots=True):
        """
        –ü–æ–≤–Ω–∏–π SHAP –∞–Ω–∞–ª—ñ–∑ –º–æ–¥–µ–ª—ñ
        
        Args:
            X_sample: –¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É (—è–∫—â–æ None - –≤—ñ–∑—å–º–µ –∑ training_data)
            sample_size: –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            save_plots: —á–∏ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫–∏
        """
        try:
            import shap
            
            
            self.logger.info("üîç –ü–æ—á–∞—Ç–æ–∫ SHAP –∞–Ω–∞–ª—ñ–∑—É...")
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
            if X_sample is None:
                if not hasattr(self, 'training_data') or self.training_data is None:
                    raise ValueError("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É. –°–ø–æ—á–∞—Ç–∫—É –Ω–∞—Ç—Ä–µ–Ω—É–π—Ç–µ –º–æ–¥–µ–ª—å.")
                X, y = self.training_data
                # –ë–µ—Ä–µ–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—É –≤–∏–±—ñ—Ä–∫—É
                if len(X) > sample_size:
                    indices = np.random.choice(len(X), sample_size, replace=False)
                    X_sample = X.iloc[indices] if hasattr(X, 'iloc') else X[indices]
                    y_sample = y.iloc[indices] if hasattr(y, 'iloc') else y[indices]
                else:
                    X_sample = X
                    y_sample = y
            
            # –û–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ feature processor —è–∫—â–æ —î
            if hasattr(self, 'feature_processor'):
                X_sample_processed = self.feature_processor.transform(X_sample)
                X_sample_processed = self.feature_extractor.create_interaction_features(X_sample_processed)
            else:
                X_sample_processed = X_sample
            
            # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            X_sample_scaled = self.scalers['main'].transform(X_sample_processed)
            
            # –ê–Ω–∞–ª—ñ–∑ –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            shap_results = {}
            
            for model_name, model in self.models.items():
                self.logger.info(f"\nüìä SHAP –∞–Ω–∞–ª—ñ–∑ –¥–ª—è {model_name}...")
                
                try:
                    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—è—Å–Ω—é–≤–∞—á–∞
                    if model_name == 'random_forest':
                        explainer = shap.TreeExplainer(model)
                    elif model_name == 'gradient_boosting':
                        explainer = shap.TreeExplainer(model)
                    elif model_name == 'xgboost' and hasattr(model, 'get_booster'):
                        explainer = shap.TreeExplainer(model)
                    else:
                        # –î–ª—è —ñ–Ω—à–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ KernelExplainer
                        explainer = shap.KernelExplainer(
                            model.predict_proba, 
                            shap.sample(X_sample_scaled, 100)
                        )
                    
                    # –û–±—á–∏—Å–ª–µ–Ω–Ω—è SHAP values
                    shap_values = explainer.shap_values(X_sample_scaled)
                    
                    # –î–ª—è –±—ñ–Ω–∞—Ä–Ω–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –±–µ—Ä–µ–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–ª–∞—Å—É 1
                    if isinstance(shap_values, list) and len(shap_values) == 2:
                        shap_values = shap_values[1]
                    
                    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
                    shap_results[model_name] = {
                        'shap_values': shap_values,
                        'explainer': explainer,
                        'expected_value': explainer.expected_value[1] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value
                    }
                    
                    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π
                    if save_plots:
                        self._create_shap_plots(
                            shap_values, 
                            X_sample_processed, 
                            model_name,
                            explainer.expected_value[1] if isinstance(explainer.expected_value, np.ndarray) else explainer.expected_value
                        )
                    
                    # –ê–Ω–∞–ª—ñ–∑ experience_type —è–∫—â–æ —î
                    if 'experience_type' in self.feature_names:
                        self._analyze_experience_type_shap(shap_values, X_sample_processed, model_name)
                    
                except Exception as e:
                    self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ SHAP –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è {model_name}: {e}")
                    continue
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ SHAP —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            self.shap_results = shap_results
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
            self._create_shap_report(shap_results, X_sample_processed)
            
            self.logger.info("‚úÖ SHAP –∞–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            
            return shap_results
            
        except ImportError:
            self.logger.error("‚ùå SHAP –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í–∏–∫–æ–Ω–∞–π—Ç–µ: pip install shap")
            raise

    def _create_shap_plots(self, shap_values, X_sample, model_name, expected_value):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è SHAP –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π"""
        import shap
        from pathlib import Path
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
        plots_dir = Path("shap_plots")
        plots_dir.mkdir(exist_ok=True)
        
        # 1. Summary plot - –∑–∞–≥–∞–ª—å–Ω–∏–π –æ–≥–ª—è–¥
        plt.figure(figsize=(10, 8))
        shap.summary_plot(
            shap_values, 
            X_sample, 
            feature_names=self.feature_names,
            show=False
        )
        plt.title(f'SHAP Summary Plot - {model_name}')
        plt.tight_layout()
        plt.savefig(plots_dir / f'shap_summary_{model_name}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Feature importance bar plot
        plt.figure(figsize=(10, 8))
        shap.summary_plot(
            shap_values, 
            X_sample, 
            feature_names=self.feature_names,
            plot_type="bar",
            show=False
        )
        plt.title(f'SHAP Feature Importance - {model_name}')
        plt.tight_layout()
        plt.savefig(plots_dir / f'shap_importance_{model_name}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Dependence plots –¥–ª—è —Ç–æ–ø-5 –æ–∑–Ω–∞–∫
        feature_importance = np.abs(shap_values).mean(axis=0)
        top_features_idx = np.argsort(feature_importance)[-5:][::-1]
        
        for idx in top_features_idx:
            plt.figure(figsize=(8, 6))
            shap.dependence_plot(
                idx, 
                shap_values, 
                X_sample,
                feature_names=self.feature_names,
                show=False
            )
            feature_name = self.feature_names[idx]
            plt.title(f'SHAP Dependence Plot - {feature_name} ({model_name})')
            plt.tight_layout()
            plt.savefig(plots_dir / f'shap_dependence_{model_name}_{feature_name}.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info(f"üìà –ì—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {plots_dir}/")

    def _analyze_experience_type_shap(self, shap_values, X_sample, model_name):
        """–°–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–ª—è experience_type"""
        if 'experience_type' not in self.feature_names:
            return
        
        exp_type_idx = self.feature_names.index('experience_type')
        
        print(f"\n=== –ê–Ω–∞–ª—ñ–∑ experience_type –¥–ª—è {model_name} ===")
        
        # –ì—Ä—É–ø—É—î–º–æ SHAP values –ø–æ —Ç–∏–ø–∞—Ö –¥–æ—Å–≤—ñ–¥—É
        X_df = pd.DataFrame(X_sample, columns=self.feature_names)
        
        for exp_type in [1, 2, 3]:
            mask = X_df['experience_type'] == exp_type
            if mask.any():
                shap_mean = shap_values[mask, exp_type_idx].mean()
                shap_std = shap_values[mask, exp_type_idx].std()
                count = mask.sum()
                
                type_names = {1: "–ü—Ä—è–º–∏–π –¥–æ—Å–≤—ñ–¥", 2: "–ö–ª–∞—Å—Ç–µ—Ä–Ω–∏–π –¥–æ—Å–≤—ñ–¥", 3: "–ó–∞–≥–∞–ª—å–Ω–∏–π"}
                print(f"\n{type_names[exp_type]}:")
                print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å: {count}")
                print(f"  –°–µ—Ä–µ–¥–Ω—ñ–π SHAP: {shap_mean:+.4f}")
                print(f"  Std SHAP: {shap_std:.4f}")
                print(f"  –í–ø–ª–∏–≤: {'–ü–æ–∑–∏—Ç–∏–≤–Ω–∏–π' if shap_mean > 0 else '–ù–µ–≥–∞—Ç–∏–≤–Ω–∏–π'}")

    def test_prediction_with_explanation(self, edrpou, item_name, category):
        """–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –ø—Ä–æ–≥–Ω–æ–∑—É –∑ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º"""
        
        test_data = {
            "EDRPOU": edrpou,
            "F_ITEMNAME": item_name,
            "F_TENDERNAME": f"–¢–µ—Å—Ç–æ–≤–∞ –∑–∞–∫—É–ø—ñ–≤–ª—è {category}",
            "F_INDUSTRYNAME": category
        }
        
        print(f"\nüß™ –¢–ï–°–¢ –ü–†–û–ì–ù–û–ó–£")
        print("="*60)
        
        try:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø—Ä–æ—Ñ—ñ–ª—å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞
            supplier_profile = None
            if hasattr(self, 'supplier_analyzer'):
                supplier_profile = self.supplier_analyzer.get_supplier_profile(edrpou)
                if supplier_profile:
                    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Ñ—ñ–ª—å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞:")
                    print(f"   –ó–∞–≥–∞–ª—å–Ω–∏–π win rate: {supplier_profile.metrics.win_rate:.2%}")
                    print(f"   –î–æ—Å–≤—ñ–¥: {supplier_profile.metrics.total_tenders} —Ç–µ–Ω–¥–µ—Ä—ñ–≤")
                else:
                    print(f"‚ö†Ô∏è –ü—Ä–æ—Ñ—ñ–ª—å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            
            # –†–æ–±–∏–º–æ –ø—Ä–æ–≥–Ω–æ–∑
            result = self.explain_single_prediction(test_data, supplier_profile, show_plot=False)
            
            return result
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return None





    def _create_shap_report(self, shap_results, X_sample):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ SHAP –∑–≤—ñ—Ç—É"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'sample_size': len(X_sample),
            'models_analyzed': list(shap_results.keys()),
            'feature_analysis': {}
        }
        
        # –ê–Ω–∞–ª—ñ–∑ –ø–æ –∫–æ–∂–Ω—ñ–π –æ–∑–Ω–∞—Ü—ñ
        for feature_idx, feature_name in enumerate(self.feature_names):
            feature_report = {}
            
            for model_name, results in shap_results.items():
                shap_vals = results['shap_values'][:, feature_idx]
                
                feature_report[model_name] = {
                    'mean_abs_shap': float(np.abs(shap_vals).mean()),
                    'mean_shap': float(shap_vals.mean()),
                    'std_shap': float(shap_vals.std()),
                    'positive_impact_ratio': float((shap_vals > 0).mean()),
                    'feature_importance_rank': None  # –ó–∞–ø–æ–≤–Ω–∏–º–æ –ø—ñ–∑–Ω—ñ—à–µ
                }
            
            report['feature_analysis'][feature_name] = feature_report
        
        # –î–æ–¥–∞—î–º–æ —Ä–∞–Ω–≥–∏ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ
        for model_name in shap_results.keys():
            # –°–æ—Ä—Ç—É—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
            importance_list = [
                (feat, data[model_name]['mean_abs_shap']) 
                for feat, data in report['feature_analysis'].items()
            ]
            importance_list.sort(key=lambda x: x[1], reverse=True)
            
            # –ü—Ä–∏—Å–≤–æ—é—î–º–æ —Ä–∞–Ω–≥–∏
            for rank, (feat, _) in enumerate(importance_list, 1):
                report['feature_analysis'][feat][model_name]['feature_importance_rank'] = rank
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤—ñ—Ç
        with open('shap_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info("üìÑ SHAP –∑–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ shap_analysis_report.json")
        
        # –í–∏–≤–æ–¥–∏–º–æ —Ç–æ–ø-10 –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫
        print("\nüìä –¢–û–ü-10 –ù–ê–ô–í–ê–ñ–õ–ò–í–Ü–®–ò–• –û–ó–ù–ê–ö (–∑–∞ SHAP):")
        print("="*70)
        
        for model_name in shap_results.keys():
            print(f"\n{model_name}:")
            top_features = sorted(
                report['feature_analysis'].items(),
                key=lambda x: x[1][model_name]['mean_abs_shap'],
                reverse=True
            )[:10]
            
            for feat_name, feat_data in top_features:
                model_data = feat_data[model_name]
                direction = "‚Üë" if model_data['mean_shap'] > 0 else "‚Üì"
                print(f"  {feat_name:35} | SHAP: {model_data['mean_abs_shap']:.4f} | –ù–∞–ø—Ä—è–º: {direction}")

    def explain_single_prediction(self, input_data, supplier_profile=None, show_plot=True):
        """
        –ü–æ—è—Å–Ω–µ–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑—É
        
        Args:
            input_data: —Å–∏—Ä—ñ –¥–∞–Ω—ñ (dict –∞–±–æ DataFrame –∑ –ø–æ–ª—è–º–∏ EDRPOU, F_ITEMNAME —Ç–æ—â–æ)
            supplier_profile: –ø—Ä–æ—Ñ—ñ–ª—å –ø–æ—Å—Ç–∞—á–∞–ª—å–Ω–∏–∫–∞ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            show_plot: —á–∏ –ø–æ–∫–∞–∑—É–≤–∞—Ç–∏ waterfall plot
        """
        import shap
        
        if not hasattr(self, 'shap_results'):
            raise ValueError("–°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å analyze_with_shap()")
        
        # 1. –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
        if isinstance(input_data, pd.DataFrame):
            input_dict = input_data.iloc[0].to_dict()
        else:
            input_dict = input_data
        
        # 2. –û—Ç—Ä–∏–º—É—î–º–æ EDRPOU –¥–ª—è –ø–æ—à—É–∫—É –ø—Ä–æ—Ñ—ñ–ª—é
        edrpou = input_dict.get('EDRPOU', '')
        
        # 3. –Ø–∫—â–æ –ø—Ä–æ—Ñ—ñ–ª—å –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∏–π, –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –∑–Ω–∞–π—Ç–∏
        if supplier_profile is None and edrpou and hasattr(self, 'supplier_analyzer'):
            supplier_profile = self.supplier_analyzer.get_supplier_profile(edrpou)
        
        # 4. –í–∏—Ç—è–≥—É—î–º–æ –æ–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ feature_extractor
        features = self.feature_extractor.extract_features(input_dict, supplier_profile)
        
        # 5. –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –Ω–∞–∑–≤–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫
        X_single = pd.DataFrame([features], columns=self.feature_names)
        
        # 6. –û–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö
        if hasattr(self, 'feature_processor'):
            X_processed = self.feature_processor.transform(X_single)
            X_processed = self.feature_extractor.create_interaction_features(X_processed)
        else:
            X_processed = X_single
        
        X_scaled = self.scalers['main'].transform(X_processed)
        
        # 7. –ü—Ä–æ–≥–Ω–æ–∑
        predictions = {}
        for model_name, model in self.models.items():
            pred = model.predict_proba(X_scaled)[0][1]
            predictions[model_name] = pred
        
        ensemble_pred = np.mean(list(predictions.values()))
        
        print(f"\nüéØ –ü—Ä–æ–≥–Ω–æ–∑: {ensemble_pred:.2%} –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–º–æ–≥–∏")
        print("="*70)
        
        # 8. –í–∏–≤–æ–¥–∏–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        print("\nüìã –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:")
        print(f"  EDRPOU: {edrpou}")
        print(f"  –ù–∞–∑–≤–∞ –ø–æ–∑–∏—Ü—ñ—ó: {input_dict.get('F_ITEMNAME', 'N/A')}")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {input_dict.get('F_INDUSTRYNAME', 'N/A')}")
        
        print("\nüìä –í–∏—Ç—è–≥–Ω—É—Ç—ñ –æ–∑–Ω–∞–∫–∏ (—Ç–æ–ø-10):")
        important_features = ['supplier_category_win_rate', 'supplier_win_rate', 
                            'experience_type', 'has_category_experience',
                            'supplier_experience', 'category_win_probability',
                            'has_brand', 'supplier_stability', 
                            'competitive_strength', 'supplier_vs_market_avg']
        
        for feat in important_features:
            if feat in features:
                print(f"  {feat:35}: {features[feat]:.4f}")
        
        # 9. –ü–æ—è—Å–Ω–µ–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        for model_name, results in self.shap_results.items():
            print(f"\nüìä –ü–æ—è—Å–Ω–µ–Ω–Ω—è –≤—ñ–¥ {model_name}:")
            print(f"–ü—Ä–æ–≥–Ω–æ–∑ –º–æ–¥–µ–ª—ñ: {predictions[model_name]:.2%}")
            
            # –û—Ç—Ä–∏–º—É—î–º–æ SHAP values –¥–ª—è —Ü—å–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É
            explainer = results['explainer']
            shap_values = explainer.shap_values(X_scaled)
            
            if isinstance(shap_values, list):
                shap_values = shap_values[1]  # –î–ª—è –∫–ª–∞—Å—É 1
            
            # –°–æ—Ä—Ç—É—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑–∞ –≤–ø–ª–∏–≤–æ–º
            feature_impact = list(zip(self.feature_names, shap_values[0], X_processed.iloc[0]))
            feature_impact.sort(key=lambda x: abs(x[1]), reverse=True)
            
            print("\n–¢–æ–ø-10 —Ñ–∞–∫—Ç–æ—Ä—ñ–≤:")
            for feat_name, impact, feat_value in feature_impact[:10]:
                direction = "–∑–±—ñ–ª—å—à—É—î" if impact > 0 else "–∑–º–µ–Ω—à—É—î"
                print(f"  {feat_name:30} = {feat_value:8.3f} | SHAP: {impact:+.4f} ({direction})")
            
            # Waterfall plot –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
            if show_plot and model_name == 'random_forest':  # –¢—ñ–ª—å–∫–∏ –¥–ª—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ
                try:
                    import matplotlib.pyplot as plt
                    shap.plots.waterfall(
                        shap.Explanation(
                            values=shap_values[0],
                            base_values=results['expected_value'],
                            data=X_processed.iloc[0],
                            feature_names=self.feature_names
                        ),
                        max_display=15
                    )
                    plt.tight_layout()
                    plt.show()
                except Exception as e:
                    print(f"–ù–µ –≤–¥–∞–ª–æ—Å—å —Å—Ç–≤–æ—Ä–∏—Ç–∏ waterfall plot: {e}")
        
        return {
            'prediction': ensemble_pred,
            'model_predictions': predictions,
            'features': features,
            'top_factors': feature_impact[:10]
        }





    def train_model(self, validation_split: float = 0.2, cv_folds: int = 5) -> Dict[str, Any]:
        """
        –û–±–≥–æ—Ä—Ç–∫–∞ –¥–ª—è train_models –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é –Ω–∞–∑–≤–æ—é –º–µ—Ç–æ–¥—É
        """
        # –°–ø–æ—á–∞—Ç–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ
        if not hasattr(self, 'training_data') or self.training_data is None:
            raise ValueError("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è. –°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–ª–∏—á—Ç–µ prepare_training_data()")
        
        # –†–æ–∑–ø–∞–∫—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö
        X, y = self.training_data
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        performance = self.train_models(X, y, test_size=validation_split)
        
        # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        return {
            'performance_metrics': {
                'auc_score': performance.get('ensemble', {}).get('test_auc', 0),
                'model_scores': {
                    name: metrics.get('test_auc', 0) 
                    for name, metrics in performance.items() 
                    if name != 'ensemble'
                }
            },
            'feature_importance': self.feature_importance,
            'training_samples': len(X),
            'models_trained': list(self.models.keys())
        }

    def prepare_training_data_from_history(self, historical_data: List[Dict], supplier_profiles: Dict[str, SupplierProfile]):
        """
        –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ –ø—Ä–æ—Ñ—ñ–ª—ñ–≤
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –≤ —Å–ª–æ–≤–Ω–∏–∫–∏
        profiles_dict = {}
        for edrpou, profile in supplier_profiles.items():
            if isinstance(profile, SupplierProfile):
                profiles_dict[edrpou] = profile.to_dict()
            else:
                profiles_dict[edrpou] = profile
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        X, y = self.prepare_training_data(historical_data, profiles_dict)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        self.training_data = (X, y)
        
        return X, y

    def export_state(self) -> Dict[str, Any]:
        """–ï–∫—Å–ø–æ—Ä—Ç —Å—Ç–∞–Ω—É –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞"""
        return {
            'models': self.models,
            'scalers': self.scalers,
            'feature_importance': self.feature_importance,
            'model_performance': self.model_performance,
            'ensemble_weights': self.ensemble_weights,
            'feature_names': self.feature_names
        }

    def load_state(self, state_data: Dict[str, Any]):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–∞–Ω—É –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞"""
        self.models = state_data.get('models', {})
        self.scalers = state_data.get('scalers', {})
        self.feature_importance = state_data.get('feature_importance', {})
        self.model_performance = state_data.get('model_performance', {})
        self.ensemble_weights = state_data.get('ensemble_weights', {})
        self.feature_names = state_data.get('feature_names', [])
        
        # –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–∞–ø–æ—Ä—Ü—è –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ—Å—Ç—ñ
        self.is_trained = len(self.models) > 0


    def update_actual_outcomes(self, outcomes: List[Dict[str, Any]]):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É"""
        for outcome in outcomes:
            self.monitor.update_actual_outcome(
                tender_id=outcome['tender_number'],
                actual=outcome['won']
            )
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        should_retrain, info = self.monitor.should_retrain()
        if should_retrain:
            self.logger.warning(f"–†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {info['reasons']}")
        
        return info

    def prepare_training_data(self, historical_data: List[Dict], supplier_profiles: Dict) -> Tuple[pd.DataFrame, pd.Series]:
        self.logger.info("–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")
        features_list = []
        targets = []
        for item in historical_data:
            edrpou = item.get('EDRPOU')
            supplier_profile = supplier_profiles.get(edrpou)
            features = self.feature_extractor.extract_features(item, supplier_profile)
            features_list.append(features)
            targets.append(int(item.get('WON', 0)))
        X = pd.DataFrame(features_list)
        y = pd.Series(targets)
        self.feature_names = list(X.columns)
        X = X.fillna(0)
        self.logger.info(f"–ü—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –∑—Ä–∞–∑–∫—ñ–≤ –∑ {len(X.columns)} —Ñ—ñ—á–∞–º–∏")
        self.logger.info(f"–†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤: {y.value_counts().to_dict()}")
        return X, y
    

    def _optimize_ensemble_weights_advanced(self, X_test, y_test):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ –∞–Ω—Å–∞–º–±–ª—é"""
        try:
            from scipy.optimize import minimize
            
            # –ó–±–∏—Ä–∞—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏ –≤—ñ–¥ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
            predictions = {}
            X_test_scaled = self.scalers['main'].transform(X_test)
            
            for name, model in self.models.items():
                predictions[name] = model.predict_proba(X_test_scaled)[:, 1]
            
            # –§—É–Ω–∫—Ü—ñ—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            def objective(weights):
                weighted_pred = np.zeros(len(y_test))
                for i, name in enumerate(self.models.keys()):
                    weighted_pred += weights[i] * predictions[name]
                return -roc_auc_score(y_test, weighted_pred)
            
            # –û–±–º–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            constraints = {'type': 'eq', 'fun': lambda w: 1 - sum(w)}
            bounds = [(0, 1) for _ in range(len(self.models))]
            initial_weights = [1/len(self.models)] * len(self.models)
            
            # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
            result = minimize(objective, initial_weights, method='SLSQP', 
                            bounds=bounds, constraints=constraints)
            
            if result.success:
                self.ensemble_weights = dict(zip(self.models.keys(), result.x))
                self.logger.info(f"–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –≤–∞–≥–∏: {self.ensemble_weights}")
            else:
                # Fallback –¥–æ —Ä—ñ–≤–Ω–∏—Ö –≤–∞–≥
                self._optimize_ensemble_weights(X_test, y_test)
                
        except ImportError:
            self.logger.warning("scipy –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä—ñ–≤–Ω—ñ –≤–∞–≥–∏")
            self._optimize_ensemble_weights(X_test, y_test)    

    def _calculate_sample_weights(self, y: pd.Series) -> np.ndarray:
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥ –¥–ª—è –∑–±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤"""
        from sklearn.utils.class_weight import compute_sample_weight
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥
        sample_weights = compute_sample_weight(
            class_weight='balanced',
            y=y
        )
        
        return sample_weights

    def _optimize_ensemble_weights_advanced(self, X_test: pd.DataFrame, y_test: pd.Series):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ –∞–Ω—Å–∞–º–±–ª—é"""
        try:
            from scipy.optimize import minimize
            
            # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            X_test_scaled = self.scalers['main'].transform(X_test)
            
            # –ó–±–∏—Ä–∞—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏ –≤—ñ–¥ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
            predictions = {}
            for name, model in self.models.items():
                predictions[name] = model.predict_proba(X_test_scaled)[:, 1]
            
            # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            def objective(weights):
                weighted_pred = np.zeros(len(y_test))
                for i, name in enumerate(self.models.keys()):
                    weighted_pred += weights[i] * predictions[name]
                return -roc_auc_score(y_test, weighted_pred)
            
            # –û–±–º–µ–∂–µ–Ω–Ω—è: —Å—É–º–∞ –≤–∞–≥ = 1, –≤—Å—ñ –≤–∞–≥–∏ >= 0
            constraints = {'type': 'eq', 'fun': lambda w: 1 - sum(w)}
            bounds = [(0, 1) for _ in range(len(self.models))]
            initial_weights = [1/len(self.models)] * len(self.models)
            
            # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
            result = minimize(objective, initial_weights, method='SLSQP', 
                            bounds=bounds, constraints=constraints)
            
            if result.success:
                self.ensemble_weights = dict(zip(self.models.keys(), result.x))
                self.logger.info(f"–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –≤–∞–≥–∏ –∞–Ω—Å–∞–º–±–ª—é: {self.ensemble_weights}")
            else:
                self.logger.warning("–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –Ω–µ –≤–¥–∞–ª–∞—Å—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä—ñ–≤–Ω—ñ –≤–∞–≥–∏")
                self._optimize_ensemble_weights(X_test, y_test)
                
        except ImportError:
            self.logger.warning("scipy –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä—ñ–≤–Ω—ñ –≤–∞–≥–∏")
            self._optimize_ensemble_weights(X_test, y_test)
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó: {e}")
            self._optimize_ensemble_weights(X_test, y_test)


    def train_models(self, X: pd.DataFrame, y: pd.Series, test_size: float = 0.2, use_calibration: bool = True):
        self.logger.info("–ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
        
        # [1] –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test (–ë–ï–ó –ó–ú–Ü–ù)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # ===== [2] –ù–û–í–ï: Feature Processing =====
        # –Ü–º–ø–æ—Ä—Ç –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É: from .feature_processor import AdvancedFeatureProcessor
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞
        self.feature_processor = AdvancedFeatureProcessor()
        X_train_processed = self.feature_processor.fit_transform(X_train, y_train)
        X_test_processed = self.feature_processor.transform(X_test)
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è interaction features
        X_train_processed = self.feature_extractor.create_interaction_features(X_train_processed)
        X_test_processed = self.feature_extractor.create_interaction_features(X_test_processed)
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–ø–∏—Å–∫—É –æ–∑–Ω–∞–∫
        self.feature_names = list(X_train_processed.columns)
        
        # ===== [3] –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (–í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û –û–ë–†–û–ë–õ–ï–ù–Ü –î–ê–ù–Ü) =====
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_processed)  
        X_test_scaled = scaler.transform(X_test_processed)        
        self.scalers['main'] = scaler

        # ===== [4] –¶–∏–∫–ª —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π =====
        for model_name, config in self.model_configs.items():
            self.logger.info(f"–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model_name}")
            
            # [5] –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            model_class = config['class']
            model_params = config['params'].copy()  # –ö–æ–ø—ñ—è –¥–ª—è –º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
            
            # ===== [6]: –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π =====
            if model_name == 'xgboost' and XGBOOST_AVAILABLE:
                # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥–∏ –¥–ª—è –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤
                scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
                model_params['scale_pos_weight'] = scale_pos_weight
                self.logger.info(f"XGBoost scale_pos_weight: {scale_pos_weight:.2f}")
                
            elif model_name == 'gradient_boosting':
                # –î–ª—è GradientBoosting –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ sample_weight –ø—Ä–∏ fit
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—ñ–∑–Ω—ñ—à–µ
                self._gb_sample_weights = self._calculate_sample_weights(y_train)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –æ–Ω–æ–≤–ª–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            model = model_class(**model_params)
            
            # [7] –ö–∞–ª—ñ–±—Ä–∞—Ü—ñ—è (–ë–ï–ó –ó–ú–Ü–ù)
            if use_calibration:
                model = CalibratedClassifierCV(model, cv=3)
            
            # ===== [8] –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (–ó –£–†–ê–•–£–í–ê–ù–ù–Ø–ú –û–°–û–ë–õ–ò–í–û–°–¢–ï–ô) =====
            if model_name == 'gradient_boosting' and hasattr(self, '_gb_sample_weights'):
                # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è GradientBoosting
                if use_calibration:
                    # CalibratedClassifierCV –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î sample_weight –Ω–∞–ø—Ä—è–º—É
                    model.fit(X_train_scaled, y_train)
                else:
                    model.fit(X_train_scaled, y_train, sample_weight=self._gb_sample_weights)
            else:
                # –ó–≤–∏—á–∞–π–Ω–µ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
                model.fit(X_train_scaled, y_train)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            self.models[model_name] = model
            
            # –û—Ü—ñ–Ω–∫–∞
            perf = self._evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, model_name)
            self.model_performance[model_name] = perf
            
            # –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
            self._analyze_feature_importance(model, self.feature_names, model_name)
            
            # ===== –ù–û–í–ï: –õ–æ–≥—É–≤–∞–Ω–Ω—è –≤ –º–æ–Ω—ñ—Ç–æ—Ä =====
            if hasattr(self, 'monitor'):
                self.monitor.performance_history.append({
                    'model': model_name,
                    'timestamp': datetime.now().isoformat(),
                    'metrics': perf
                })

        # ===== [9] –ó–ê–ú–Ü–ù–ê: –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –≤–∞–≥ –∞–Ω—Å–∞–º–±–ª—é =====      
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ:
        self._optimize_ensemble_weights_advanced(X_test_processed, y_test)  
        
        # –û—Ü—ñ–Ω–∫–∞ –∞–Ω—Å–∞–º–±–ª—é
        ensemble_pred = self.predict_proba_ensemble(X_test_processed) 
        ensemble_auc = roc_auc_score(y_test, ensemble_pred)
        self.logger.info(f"Ensemble AUC: {ensemble_auc:.4f}")
        self.model_performance['ensemble'] = {'test_auc': ensemble_auc}

        # ===== –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–æ–¥–∞—Ç–∫–æ–≤–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó =====
        self.training_info = {
            'date': datetime.now().isoformat(),
            'n_samples': len(X_train),
            'n_features': len(self.feature_names),
            'class_distribution': {
                'train': dict(y_train.value_counts()),
                'test': dict(y_test.value_counts())
            }
        }
        
        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–µ—Ä—Å—ñ—ó –º–æ–¥–µ–ª—ñ
        self.model_performance['version'] = self.model_performance.get('version', '1.0')
        
        return self.model_performance

    def _evaluate_model(self, model, X_train, y_train, X_test, y_test, model_name: str) -> Dict:
        y_train_pred = model.predict_proba(X_train)[:, 1]
        y_test_pred = model.predict_proba(X_test)[:, 1]
        train_auc = roc_auc_score(y_train, y_train_pred)
        test_auc = roc_auc_score(y_test, y_test_pred)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
        precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)
        best_threshold_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_threshold_idx] if best_threshold_idx < len(thresholds) else 0.5
        y_test_binary = (y_test_pred >= best_threshold).astype(int)
        report = classification_report(y_test, y_test_binary, output_dict=True)
        performance = {
            'train_auc': train_auc,
            'test_auc': test_auc,
            'cv_auc_mean': cv_scores.mean(),
            'cv_auc_std': cv_scores.std(),
            'best_threshold': best_threshold,
            'precision': report['1']['precision'],
            'recall': report['1']['recall'],
            'f1_score': report['1']['f1-score']
        }
        return performance

    def _analyze_feature_importance(self, model, feature_names, model_name: str):
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'base_estimator_') and hasattr(model.base_estimator_, 'feature_importances_'):
            importances = model.base_estimator_.feature_importances_
        else:
            importances = None
        if importances is not None:
            self.feature_importance[model_name] = dict(zip(feature_names, importances))

    def _optimize_ensemble_weights(self, X_test, y_test):
        # –ü—Ä–æ—Å—Ç–∞ —Ä—ñ–≤–Ω–∞ –≤–∞–≥–∞ –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
        n_models = len(self.models)
        if n_models == 0:
            self.ensemble_weights = {}
            return
        self.ensemble_weights = {name: 1.0 / n_models for name in self.models}

    def predict_proba(self, X: pd.DataFrame, model_name: str = 'ensemble') -> np.ndarray:
        if model_name == 'ensemble':
            return self.predict_proba_ensemble(X)
        model = self.models.get(model_name)
        scaler = self.scalers.get('main')
        if model is None or scaler is None:
            raise ValueError("Model or scaler not found")
        X_scaled = scaler.transform(X)
        return model.predict_proba(X_scaled)[:, 1]

    def predict_proba_ensemble(self, X: pd.DataFrame) -> np.ndarray:
        """–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª–µ–º –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö"""
        # –û–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ feature processor
        if hasattr(self, 'feature_processor'):
            X_processed = self.feature_processor.transform(X)
            # –î–æ–¥–∞–≤–∞–Ω–Ω—è interaction features
            X_processed = self.feature_extractor.create_interaction_features(X_processed)
        else:
            X_processed = X
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
        scaler = self.scalers.get('main')
        if scaler is None:
            raise ValueError("Scaler not found")
        X_scaled = scaler.transform(X_processed)
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        preds = np.zeros(X.shape[0])
        for model_name, model in self.models.items():
            weight = self.ensemble_weights.get(model_name, 1.0)
            preds += weight * model.predict_proba(X_scaled)[:, 1]
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        preds /= sum(self.ensemble_weights.values())
        
        return preds

    def predict_tender(self, tender_items: List[Dict], supplier_profiles: Dict) -> List[Dict]:
        features_list = []
        for item in tender_items:
            edrpou = item.get('EDRPOU')
            supplier_profile = supplier_profiles.get(edrpou)
            features = self.feature_extractor.extract_features(item, supplier_profile)
            features_list.append(features)
        X = pd.DataFrame(features_list).fillna(0)

        # –î–æ–¥–∞—î–º–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –æ–∑–Ω–∞–∫–∏, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        for col in self.feature_names:
            if col not in X.columns:
                X[col] = 0
        X = X[self.feature_names]  # –≤–ø–æ—Ä—è–¥–∫—É–≤–∞—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏

        probs = self.predict_proba(X)
        results = []
        for i, item in enumerate(tender_items):
            results.append({
                'tender_number': item.get('F_TENDERNUMBER'),
                'edrpou': item.get('EDRPOU'),
                'probability': probs[i],
                'confidence': self._calculate_prediction_confidence(X.iloc[i], probs[i]),
                'risk_factors': self._identify_risk_factors(X.iloc[i], probs[i])
            })
            for i, result in enumerate(results):
                self.monitor.log_prediction(
                    tender_id=result['tender_number'],
                    features=features_list[i],  # –ó–±–µ—Ä–µ–≥—Ç–∏ features_list
                    prediction=result['probability'],
                    model_version=f"ensemble_v{self.model_performance.get('version', '1.0')}"
                )
        return results

    def _calculate_prediction_confidence(self, features: pd.Series, probability: float) -> str:
        if probability > 0.8:
            return "high"
        elif probability > 0.6:
            return "medium"
        else:
            return "low"

    def _identify_risk_factors(self, features: pd.Series, probability: float) -> List[str]:
        risks = []
        if probability < 0.5:
            risks.append("–ù–∏–∑—å–∫–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–º–æ–≥–∏")
        if features.get('competition_intensity', 0) > 0.7:
            risks.append("–í–∏—Å–æ–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—è")
        if features.get('entry_barrier', 0) > 0.6:
            risks.append("–í–∏—Å–æ–∫–∏–π –±–∞—Ä'—î—Ä –≤—Ö–æ–¥—É")
        return risks

    def get_feature_analysis(self) -> Dict[str, pd.DataFrame]:
        return self.feature_importance

    def save_models(self, filepath: str):
        """–ó–±–µ—Ä–µ–≥—Ç–∏ –≤—Å—ñ –º–æ–¥–µ–ª—ñ, —Å–∫–µ–π–ª–µ—Ä–∏ —Ç–∞ –≤–∞–≥–∏ –∞–Ω—Å–∞–º–±–ª—é —É —Ñ–∞–π–ª"""
        state = {
            'models': self.models,
            'scalers': self.scalers,
            'ensemble_weights': self.ensemble_weights,
            'feature_names': getattr(self, 'feature_names', []),
            'feature_importance': self.feature_importance,
            'model_performance': self.model_performance
        }
        with open(filepath, "wb") as f:
            pickle.dump(state, f)
        self.logger.info(f"–ú–æ–¥–µ–ª—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {filepath}")

    def load_models(self, filepath: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤—Å—ñ –º–æ–¥–µ–ª—ñ, —Å–∫–µ–π–ª–µ—Ä–∏ —Ç–∞ –≤–∞–≥–∏ –∞–Ω—Å–∞–º–±–ª—é –∑ —Ñ–∞–π–ª—É"""
        with open(filepath, "rb") as f:
            state = pickle.load(f)
        self.models = state.get('models', {})
        self.scalers = state.get('scalers', {})
        self.ensemble_weights = state.get('ensemble_weights', {})
        self.feature_names = state.get('feature_names', [])
        self.feature_importance = state.get('feature_importance', {})
        self.model_performance = state.get('model_performance', {})
        self.logger.info(f"–ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ {filepath}")