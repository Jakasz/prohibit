#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
"""

import logging
import json
from pathlib import Path
from tender_analysis_system import TenderAnalysisSystem
from qdrant_client import QdrantClient
from tqdm import tqdm  # –î–æ–¥–∞–π—Ç–µ —ñ–º–ø–æ—Ä—Ç –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def monitor_progress(system, start_count, interval=50000):
    """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—É —Ç–∞ —Ä–æ–∑–º—ñ—Ä—É"""
    current_count = system.vector_db.get_collection_size()
    if current_count - start_count >= interval:
        stats = system.vector_db.get_storage_info()
        logging.info(f"""
        üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
        - –ó–∞–ø–∏—Å—ñ–≤: {current_count:,}
        - –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤: {stats['vectors_size_gb']} –ì–ë
        - –†–æ–∑–º—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {stats['payload_size_gb']} –ì–ë
        - –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {stats['estimated_total_gb']} –ì–ë
        """)
        return current_count
    return start_count

def process_file_with_stats(file_path, system, batch_size=1000):
    print(f"\nüìÅ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file_path}")
    file_size = Path(file_path).stat().st_size / (1024**3)
    print(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size:.2f} –ì–ë")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤
    print("üìù –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤...")
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                pass  # –ú–æ–∂–Ω–∞ —Ä–∞—Ö—É–≤–∞—Ç–∏ –ø–æ–º–∏–ª–∫–∏ —Ç—É—Ç
    
    # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    stats = system.vector_db.index_tenders(
        data,
        update_mode=True,
        batch_size=batch_size
    )
    
    return stats

def create_optimized_vector_database(
    jsonl_files: list,
    categories_file: str = "categories.jsonl",
    collection_name: str = "tender_vectors",
    batch_size: int = 2000,  # –ó–±—ñ–ª—å—à–µ–Ω–æ
    max_records: int = None,
    monitor_interval: int = 50000
):
    """
    –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
    """
    
    print("="*60)
    print("üöÄ –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ê –í–ï–ö–¢–û–†–ù–ê –ë–ê–ó–ê –¢–ï–ù–î–ï–†–Ü–í")
    print("="*60)
    
    # 1. –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó (—è–∫—â–æ —î)
    try:
        client = QdrantClient(host="localhost", port=6333)
        client.delete_collection(collection_name)
        print(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}'")
    except:
        print(f"‚ÑπÔ∏è –ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –Ω–µ —ñ—Å–Ω—É–≤–∞–ª–∞")
    
    # 2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
    print("\nüì¶ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏...")
    
    # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è –≤—ñ–¥ transformers
    import logging
    logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
    
    system = TenderAnalysisSystem(
        categories_file=categories_file,
        qdrant_host="localhost",
        qdrant_port=6333
    )
    
    if not system.initialize_system():
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó!")
        return False
    
    # –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó
    system.vector_db.collection_name = collection_name
    
    # 3. –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
    total_indexed = 0
    total_skipped = 0
    total_errors = 0

    for idx, jsonl_file in enumerate(jsonl_files, 1):
        print(f"\nüìÅ –§–∞–π–ª {idx}/{len(jsonl_files)}: {Path(jsonl_file).name}")
        if not Path(jsonl_file).exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {jsonl_file}")
            continue

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤—É —Ñ—É–Ω–∫—Ü—ñ—é
        stats = process_file_with_stats(jsonl_file, system, batch_size=batch_size)
        total_indexed += stats.get('indexed_count', 0)
        total_skipped += stats.get('skipped_count', 0)
        total_errors += stats.get('error_count', 0)

        print(f"‚úÖ –§–∞–π–ª –æ–±—Ä–æ–±–ª–µ–Ω–æ. –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {stats.get('indexed_count', 0):,}")

    # 4. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
    print("\nüîß –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó...")
    system.vector_db.optimize_collection()
    
    # 5. –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*60)
    print("üìä –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    
    final_stats = system.vector_db.get_collection_stats()
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {final_stats['points_count']:,}")
    print(f"üì¶ –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤: {final_stats['vectors_size_gb']} –ì–ë")
    print(f"üì¶ –†–æ–∑–º—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {final_stats['payload_size_gb']} –ì–ë")
    print(f"üíæ –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {final_stats['estimated_total_gb']} –ì–ë")
    print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {total_skipped:,}")
    print(f"‚ùå –ü–æ–º–∏–ª–æ–∫: {total_errors:,}")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    if final_stats['points_count'] > 0:
        bytes_per_point = (final_stats['estimated_total_gb'] * 1024**3) / final_stats['points_count']
        print(f"üìè –ë–∞–π—Ç –Ω–∞ –∑–∞–ø–∏—Å: {bytes_per_point:.0f}")
        
        # –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è —è–∫—â–æ —Ä–æ–∑–º—ñ—Ä –≤–µ–ª–∏–∫–∏–π
        if bytes_per_point > 5000:
            print("‚ö†Ô∏è –£–í–ê–ì–ê: –†–æ–∑–º—ñ—Ä –Ω–∞ –∑–∞–ø–∏—Å –∑–∞–≤–µ–ª–∏–∫–∏–π! –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é.")
    
    print("="*60)
    
    return True


if __name__ == "__main__":
    import json
    
    # ===== –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø =====
    
    # –í–∞—à—ñ —Ñ–∞–π–ª–∏
    FILES = [
        "out_10_nodup.jsonl",
        "out_12_nodup.jsonl"
    ]
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
    COLLECTION_NAME = "tender_vectors"  # –ù–æ–≤–∞ –Ω–∞–∑–≤–∞!
    BATCH_SIZE = 1500  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    MONITOR_INTERVAL = 100000  # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∫–æ–∂–Ω—ñ 100–∫ –∑–∞–ø–∏—Å—ñ–≤
    
    # –î–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    MAX_RECORDS = None  # None = –≤—Å—ñ –∑–∞–ø–∏—Å–∏, –∞–±–æ —á–∏—Å–ª–æ –¥–ª—è —Ç–µ—Å—Ç—É
    
    # ===== –ó–ê–ü–£–°–ö =====
    
    print("üîß –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ê –í–ï–†–°–Ü–Ø")
    print(f"\nüìÅ –§–∞–π–ª–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏:")
    for f in FILES:
        print(f"  - {f}")
    
    print(f"\n‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    print(f"  - –ö–æ–ª–µ–∫—Ü—ñ—è: {COLLECTION_NAME}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - Monitor interval: {MONITOR_INTERVAL:,}")
    print(f"  - Max records: {MAX_RECORDS or '–í—Å—ñ'}")
    
    response = input("\n–ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏? (y/n): ")
    if response.lower() == 'y':
        create_optimized_vector_database(
            jsonl_files=FILES,
            collection_name=COLLECTION_NAME,
            batch_size=BATCH_SIZE,
            max_records=MAX_RECORDS,
            monitor_interval=MONITOR_INTERVAL
        )
    else:
        print("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞")