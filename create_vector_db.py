
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
"""

import logging
import json
from pathlib import Path
from tender_analysis_system import TenderAnalysisSystem
from qdrant_client import QdrantClient
from tqdm import tqdm
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def monitor_progress(system, start_count, interval=50000):
    """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—É —Ç–∞ —Ä–æ–∑–º—ñ—Ä—É"""
    current_count = system.vector_db.get_collection_size()
    if current_count - start_count >= interval:
        stats = system.vector_db.get_storage_info()
        logging.info(f"""
        üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
        - –ó–∞–ø–∏—Å—ñ–≤: {current_count:,}
        - –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤: {stats['vectors_size_gb']} –ì–ë
        - –†–æ–∑–º—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {stats['payload_size_gb']} –ì–ë
        - –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {stats['estimated_total_gb']} –ì–ë
        """)
        return current_count
    return start_count

def process_file_with_stats(file_path, system, batch_size=1000, update_mode=True):
    """
    –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
    
    Args:
        file_path: —à–ª—è—Ö –¥–æ JSONL —Ñ–∞–π–ª—É
        system: –µ–∫–∑–µ–º–ø–ª—è—Ä TenderAnalysisSystem
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        update_mode: True - –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤, False - –ø–æ–≤–Ω–∞ –ø–µ—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    """
    print(f"\nüìÅ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file_path}")
    file_size = Path(file_path).stat().st_size / (1024**3)
    print(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size:.2f} –ì–ë")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤
    print("üìù –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤...")
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    errors = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                errors += 1
                if errors <= 10:  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 10 –ø–æ–º–∏–ª–æ–∫
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}")
    
    if errors > 0:
        print(f"‚ö†Ô∏è –í—Å—å–æ–≥–æ –ø–æ–º–∏–ª–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {errors}")
    
    # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–µ–∂–∏–º—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    print(f"\nüîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ —Ä–µ–∂–∏–º—ñ: {'–æ–Ω–æ–≤–ª–µ–Ω–Ω—è' if update_mode else '–ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è'}")
    stats = system.vector_db.index_tenders(
        data,
        update_mode=update_mode,  # –ü–µ—Ä–µ–¥–∞—î–º–æ —Ä–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
        batch_size=batch_size
    )
    
    return stats

def check_existing_collection(host="localhost", port=6333, collection_name="tender_vectors"):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
    
    Returns:
        tuple: (exists: bool, info: dict)
    """
    try:
        client = QdrantClient(host=host, port=port)
        collection_info = client.get_collection(collection_name)
        
        info = {
            'exists': True,
            'points_count': collection_info.points_count,
            'status': collection_info.status,
            'vectors_size': collection_info.config.params.vectors.size,
            'segments_count': collection_info.segments_count if collection_info.segments_count else 0
        }
        
        return True, info
    except:
        return False, {'exists': False}

def create_optimized_vector_database(
    jsonl_files: list,
    categories_file: str = "categories.jsonl",
    collection_name: str = "tender_vectors",
    batch_size: int = 2000,
    max_records: int = None,
    monitor_interval: int = 50000,
    update_mode: bool = None,  # None = –∑–∞–ø–∏—Ç–∞—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    force_recreate: bool = False  # –ü—Ä–∏–º—É—Å–æ–≤–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
):
    """
    –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏
    
    Args:
        jsonl_files: —Å–ø–∏—Å–æ–∫ JSONL —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        categories_file: —Ñ–∞–π–ª –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
        collection_name: –Ω–∞–∑–≤–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó –≤ Qdrant
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        max_records: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ (–¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)
        monitor_interval: —ñ–Ω—Ç–µ—Ä–≤–∞–ª –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –ø—Ä–æ–≥—Ä–µ—Å—É
        update_mode: True - –æ–Ω–æ–≤–ª–µ–Ω–Ω—è, False - –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è, None - –∑–∞–ø–∏—Ç–∞—Ç–∏
        force_recreate: –ø—Ä–∏–º—É—Å–æ–≤–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–µ–∑ –∑–∞–ø–∏—Ç—É
    """
    
    print("="*60)
    print("üöÄ –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–ê –í–ï–ö–¢–û–†–ù–ê –ë–ê–ó–ê –¢–ï–ù–î–ï–†–Ü–í")
    print("="*60)
    
    # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
    exists, collection_info = check_existing_collection(
        collection_name=collection_name
    )
    
    if exists:
        print(f"\n‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î:")
        print(f"   ‚Ä¢ –ó–∞–ø–∏—Å—ñ–≤: {collection_info['points_count']:,}")
        print(f"   ‚Ä¢ –°—Ç–∞—Ç—É—Å: {collection_info['status']}")
        print(f"   ‚Ä¢ –°–µ–≥–º–µ–Ω—Ç—ñ–≤: {collection_info['segments_count']}")
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–µ–∂–∏–º—É —Ä–æ–±–æ—Ç–∏
        if force_recreate:
            update_mode = False
            print("\n‚ö†Ô∏è –£–í–ê–ì–ê: –ü—Ä–∏–º—É—Å–æ–≤–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó!")
        elif update_mode is None:
            print("\nü§î –©–æ —Ä–æ–±–∏—Ç–∏ –∑ —ñ—Å–Ω—É—é—á–æ—é –∫–æ–ª–µ–∫—Ü—ñ—î—é?")
            print("1. –û–Ω–æ–≤–∏—Ç–∏ (–¥–æ–¥–∞—Ç–∏ –Ω–æ–≤—ñ –∑–∞–ø–∏—Å–∏)")
            print("2. –í–∏–¥–∞–ª–∏—Ç–∏ —ñ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∑–∞–Ω–æ–≤–æ")
            print("3. –°–∫–∞—Å—É–≤–∞—Ç–∏ –æ–ø–µ—Ä–∞—Ü—ñ—é")
            
            choice = input("\n–í–∞—à –≤–∏–±—ñ—Ä (1/2/3): ")
            
            if choice == '1':
                update_mode = True
                print("‚úÖ –†–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è")
            elif choice == '2':
                update_mode = False
                print("‚ö†Ô∏è –†–µ–∂–∏–º –ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è")
            else:
                print("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞")
                return False
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if not update_mode:
            try:
                client = QdrantClient(host="localhost", port=6333)
                client.delete_collection(collection_name)
                print(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}'")
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
                return False
    else:
        print(f"\n‚ÑπÔ∏è –ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –Ω–µ —ñ—Å–Ω—É—î, –±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–∞ –Ω–æ–≤–∞")
        update_mode = False
    
    # 2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
    print("\nüì¶ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏...")
    
    # –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è –≤—ñ–¥ transformers
    import logging
    logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
    
    system = TenderAnalysisSystem(
        categories_file=categories_file,
        qdrant_host="localhost",
        qdrant_port=6333
    )
    
    if not system.initialize_system():
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó!")
        return False
    
    # –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
    system.vector_db.collection_name = collection_name
    
    # 3. –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    initial_count = system.vector_db.get_collection_size() if update_mode else 0
    print(f"\nüìä –ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤: {initial_count:,}")
    
    # 4. –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
    total_indexed = 0
    total_skipped = 0
    total_errors = 0
    start_time = datetime.now()

    for idx, jsonl_file in enumerate(jsonl_files, 1):
        print(f"\n{'='*60}")
        print(f"üìÅ –§–∞–π–ª {idx}/{len(jsonl_files)}: {Path(jsonl_file).name}")
        print(f"{'='*60}")
        
        if not Path(jsonl_file).exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {jsonl_file}")
            continue

        # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–µ–∂–∏–º—É
        stats = process_file_with_stats(
            jsonl_file, 
            system, 
            batch_size=batch_size,
            update_mode=update_mode
        )
        
        total_indexed += stats.get('indexed_count', 0)
        total_skipped += stats.get('skipped_count', 0)
        total_errors += stats.get('error_count', 0)

        print(f"\n‚úÖ –§–∞–π–ª –æ–±—Ä–æ–±–ª–µ–Ω–æ:")
        print(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {stats.get('indexed_count', 0):,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {stats.get('skipped_count', 0):,}")
        print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {stats.get('error_count', 0):,}")

    # 5. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó
    if total_indexed > 0:
        print("\nüîß –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–ª–µ–∫—Ü—ñ—ó...")
        system.vector_db.optimize_collection()
    
    # 6. –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    final_count = system.vector_db.get_collection_size()
    processing_time = (datetime.now() - start_time).total_seconds()
    
    print("\n" + "="*60)
    print("üìä –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("="*60)
    
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ: {final_count:,}")
    print(f"üìà –î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {final_count - initial_count:,}")
    print(f"üì¶ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {total_indexed:,}")
    print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {total_skipped:,}")
    print(f"‚ùå –ü–æ–º–∏–ª–æ–∫: {total_errors:,}")
    print(f"‚è±Ô∏è –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {processing_time:.1f} —Å–µ–∫")
    print(f"üöÄ –®–≤–∏–¥–∫—ñ—Å—Ç—å: {(total_indexed + total_skipped) / processing_time:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
    final_stats = system.vector_db.get_collection_stats()
    if 'vectors_size_gb' in final_stats:
        print(f"\nüíæ –†–æ–∑–º—ñ—Ä –±–∞–∑–∏ –¥–∞–Ω–∏—Ö:")
        print(f"   ‚Ä¢ –í–µ–∫—Ç–æ—Ä–∏: {final_stats['vectors_size_gb']} –ì–ë")
        print(f"   ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω—ñ: {final_stats['payload_size_gb']} –ì–ë")
        print(f"   ‚Ä¢ –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {final_stats['estimated_total_gb']} –ì–ë")
        
        if final_stats['points_count'] > 0:
            bytes_per_point = (final_stats['estimated_total_gb'] * 1024**3) / final_stats['points_count']
            print(f"   ‚Ä¢ –ë–∞–π—Ç –Ω–∞ –∑–∞–ø–∏—Å: {bytes_per_point:.0f}")
    
    print("="*60)
    
    return True


if __name__ == "__main__":
    # ===== –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø =====
    
    # –í–∞—à—ñ —Ñ–∞–π–ª–∏
    FILES = [
        "out_10_nodup.jsonl",
        "out_12_nodup.jsonl"
    ]
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
    COLLECTION_NAME = "tender_vectors"  # –ù–∞–∑–≤–∞ –∫–æ–ª–µ–∫—Ü—ñ—ó
    BATCH_SIZE = 1500                   # –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É
    MONITOR_INTERVAL = 100000           # –Ü–Ω—Ç–µ—Ä–≤–∞–ª –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
    
    # –†–µ–∂–∏–º–∏ —Ä–æ–±–æ—Ç–∏ (—Ä–æ–∑–∫–æ–º–µ–Ω—Ç—É–π—Ç–µ –ø–æ—Ç—Ä—ñ–±–Ω–∏–π)
    UPDATE_MODE = None      # –ó–∞–ø–∏—Ç–∞—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    # UPDATE_MODE = True    # –ó–∞–≤–∂–¥–∏ –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏
    # UPDATE_MODE = False   # –ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏
    
    # –î–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    MAX_RECORDS = None  # None = –≤—Å—ñ –∑–∞–ø–∏—Å–∏, –∞–±–æ —á–∏—Å–ª–æ –¥–ª—è —Ç–µ—Å—Ç—É
    
    # ===== –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø =====
    
    print("üîß –°–ò–°–¢–ï–ú–ê –í–ï–ö–¢–û–†–ù–û–á –ë–ê–ó–ò –¢–ï–ù–î–ï–†–Ü–í")
    print("üìÖ –í–µ—Ä—Å—ñ—è: 2.0 (–∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è)")
    
    print(f"\nüìÅ –§–∞–π–ª–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏:")
    for f in FILES:
        if Path(f).exists():
            size = Path(f).stat().st_size / (1024**3)
            print(f"  ‚úÖ {f} ({size:.2f} –ì–ë)")
        else:
            print(f"  ‚ùå {f} (–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ)")
    
    print(f"\n‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    print(f"  - –ö–æ–ª–µ–∫—Ü—ñ—è: {COLLECTION_NAME}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - Monitor interval: {MONITOR_INTERVAL:,}")
    print(f"  - Max records: {MAX_RECORDS or '–í—Å—ñ'}")
    
    if UPDATE_MODE is None:
        print(f"  - –†–µ–∂–∏–º: –±—É–¥–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ")
    elif UPDATE_MODE:
        print(f"  - –†–µ–∂–∏–º: –æ–Ω–æ–≤–ª–µ–Ω–Ω—è (–¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö)")
    else:
        print(f"  - –†–µ–∂–∏–º: –ø–æ–≤–Ω–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è")
    
    # ===== –ó–ê–ü–£–°–ö =====
    
    response = input("\nüöÄ –ü–æ—á–∞—Ç–∏ –æ–±—Ä–æ–±–∫—É? (y/n): ")
    if response.lower() == 'y':
        success = create_optimized_vector_database(
            jsonl_files=FILES,
            collection_name=COLLECTION_NAME,
            batch_size=BATCH_SIZE,
            max_records=MAX_RECORDS,
            monitor_interval=MONITOR_INTERVAL,
            update_mode=UPDATE_MODE
        )
        
        if success:
            print("\n‚úÖ –û–ø–µ—Ä–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
        else:
            print("\n‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏")
    else:
        print("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞")