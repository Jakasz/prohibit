
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
"""

import logging
import json
from pathlib import Path
# from tender_analysis_system import TenderAnalysisSystem # Replaced by system_provider
from system_provider import get_system, is_system_initialized
from qdrant_client import QdrantClient
from tqdm import tqdm
from datetime import datetime
from qdrant_client.http import models

# Assuming TenderVectorDB is still needed for specific DB operations not covered by TenderAnalysisSystem's vector_db attribute directly
# However, TenderAnalysisSystem itself initializes and holds a TenderVectorDB instance.
# We should use system.vector_db instead of creating a separate one if possible.
# from vector_database import TenderVectorDB # This might be redundant if system.vector_db is used

# Configure logging if not already done by other modules
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


# This function seems to be a method of TenderVectorDB, not a standalone one.
# It's not used in this file's global scope. If it's a helper for TenderVectorDB, it should be there.
# For now, commenting out as it's not directly called in the main logic of create_vector_db.py
# def fast_upsert_batch(self, points, batch_num: int = 0) -> int:
#     """–®–≤–∏–¥–∫–∞ –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è"""
#     try:
#         print(f"üì° –®–≤–∏–¥–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—É #{batch_num} –∑ {len(points)} —Ç–æ—á–æ–∫...")
#
#         if not points:
#             print(f"‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π –±–∞—Ç—á #{batch_num}")
#             return 0
#
#         # üî• –®–í–ò–î–ö–ê –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
#         self.client.upsert(
#             collection_name=self.collection_name,
#             points=points,
#             wait=True,  # –ù–ï —á–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è!
#             ordering=models.WriteOrdering.WEAK  # –°–ª–∞–±–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
#         )
#
#         print(f"‚ö° –ë–∞—Ç—á #{batch_num} –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ")
#         return len(points)
#
#     except Exception as e:
#         print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —à–≤–∏–¥–∫–æ—ó –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –±–∞—Ç—á—É #{batch_num}: {e}")
#         return 0


def monitor_progress(system, start_count, interval=50000):
    """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—É —Ç–∞ —Ä–æ–∑–º—ñ—Ä—É"""
    current_count = system.vector_db.get_collection_size()
    if current_count - start_count >= interval:
        stats = system.vector_db.get_storage_info()
        logger.info(f"""
        üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
        - –ó–∞–ø–∏—Å—ñ–≤: {current_count:,}
        - –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤: {stats['vectors_size_gb']} –ì–ë
        - –†–æ–∑–º—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {stats['payload_size_gb']} –ì–ë
        - –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {stats['estimated_total_gb']} –ì–ë
        """)
        return current_count
    return start_count

def process_file_with_stats(file_path, system, batch_size=1000, update_mode=True): # system is TenderAnalysisSystem
    """
    –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
    
    Args:
        file_path: —à–ª—è—Ö –¥–æ JSONL —Ñ–∞–π–ª—É
        system: –µ–∫–∑–µ–º–ø–ª—è—Ä TenderAnalysisSystem
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        update_mode: True - –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤, False - –ø–æ–≤–Ω–∞ –ø–µ—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    """
    logger.info(f"\nüìÅ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file_path}")
    file_size = Path(file_path).stat().st_size / (1024**3)
    logger.info(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size:.2f} –ì–ë")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤
    logger.info("üìù –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤...")
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    logger.info(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    errors = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                errors += 1
                if errors <= 10:  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 10 –ø–æ–º–∏–ª–æ–∫
                    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}")
    
    if errors > 0:
        logger.warning(f"‚ö†Ô∏è –í—Å—å–æ–≥–æ –ø–æ–º–∏–ª–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {errors}")
    
    # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–µ–∂–∏–º—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    logger.info(f"\nüîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ —Ä–µ–∂–∏–º—ñ: {'–æ–Ω–æ–≤–ª–µ–Ω–Ω—è' if update_mode else '–ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è'}")
    # Assuming system.vector_db is an instance of TenderVectorDB or similar, and has index_tenders
    stats = system.vector_db.index_tenders(
        data,
        update_mode=update_mode,  # –ü–µ—Ä–µ–¥–∞—î–º–æ —Ä–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
        batch_size=batch_size
    )
    
    return stats

def check_existing_collection(host="localhost", port=6333, collection_name="tender_vectors"):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
    
    Returns:
        tuple: (exists: bool, info: dict)
    """
    try:
        # This client is temporary for checking existence before full system init
        client = QdrantClient(host=host, port=port)
        collection_info_model = client.get_collection(collection_name)
        
        info = {
            'exists': True,
            'points_count': collection_info_model.points_count,
            'status': collection_info_model.status,
            'vectors_size': collection_info_model.config.params.vectors.size,
            # Segments count might not be directly available or named differently depending on client version
            'segments_count': getattr(collection_info_model, 'segments_count', 0)
        }
        client.close() # Close temporary client
        return True, info
    except Exception: # Broad exception for cases where collection doesn't exist or Qdrant is down
        return False, {'exists': False}


def process_file_fast(file_path, system, batch_size=5000, update_mode=True, fast_mode=True): # system is TenderAnalysisSystem
    """–®–í–ò–î–ö–ê –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É"""
    logger.info(f"‚ö° –®–í–ò–î–ö–ê –æ–±—Ä–æ–±–∫–∞: {file_path}")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    logger.info(f"üìä –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    errors = 0
    
    logger.info("üì• –®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—å...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è", unit="—Ä—è–¥–∫—ñ–≤"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                errors += 1
                if errors <= 5:
                    logger.warning(f"‚ö†Ô∏è JSON –ø–æ–º–∏–ª–∫–∞: {e}")
    
    logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data):,} –∑–∞–ø–∏—Å—ñ–≤ (–ø–æ–º–∏–ª–æ–∫: {errors})")
    
    # –®–í–ò–î–ö–ê —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    logger.info(f"‚ö° –®–í–ò–î–ö–ê —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑ –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...")
    # Assuming system.vector_db is an instance of TenderVectorDB or similar, and has index_tenders
    stats = system.vector_db.index_tenders(
        data,
        update_mode=update_mode,
        batch_size=batch_size
        # fast_mode might be a parameter to index_tenders or handled by how system.vector_db is configured
    )
    
    return stats


def create_optimized_vector_database(
    jsonl_files: list,
    categories_file: str = "categories.jsonl", # Default from TenderAnalysisSystem
    collection_name: str = "tender_vectors",
    batch_size: int = 5000,
    update_mode: bool | None = None, # If None, will ask user if collection exists
    force_recreate: bool = False,
    fast_mode: bool = True, # This might influence how index_tenders behaves or if indexing is deferred
    qdrant_host: str = "localhost", # Default from TenderAnalysisSystem
    qdrant_port: int = 6333         # Default from TenderAnalysisSystem
):
    """
    –°—Ç–≤–æ—Ä—é—î –∞–±–æ –æ–Ω–æ–≤–ª—é—î –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –¥–∞–Ω–∏—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –≥–ª–æ–±–∞–ª—å–Ω–∏–π TenderAnalysisSystem.
    """
    
    logger.info("="*60)
    if fast_mode:
        logger.info("‚ö° –®–í–ò–î–ö–ò–ô –†–ï–ñ–ò–ú –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø")
        logger.info("   ‚Ä¢ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –º–æ–∂–µ –±—É—Ç–∏ –í–ò–ú–ö–ù–ï–ù–ê –∞–±–æ –≤—ñ–¥–∫–ª–∞–¥–µ–Ω–∞ (–∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ TenderVectorDB.index_tenders)")
        logger.info("   ‚Ä¢ –ó–±—ñ–ª—å—à–µ–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É (—è–∫—â–æ –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ –≤ index_tenders)")
    else:
        logger.info("üêå –ó–í–ò–ß–ê–ô–ù–ò–ô –†–ï–ñ–ò–ú (–º–æ–∂–µ –≤–∫–ª—é—á–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é)")
    logger.info("="*60)
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó –î–û —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏ (—â–æ–± –Ω–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏, —è–∫—â–æ —Å–∫–∞—Å–æ–≤–∞–Ω–æ)
    # Qdrant client details should ideally match what TenderAnalysisSystem will use.
    exists, collection_info = check_existing_collection(host=qdrant_host, port=qdrant_port, collection_name=collection_name)
    
    if exists and not force_recreate:
        logger.info(f"\n‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î:")
        logger.info(f"   ‚Ä¢ –ó–∞–ø–∏—Å—ñ–≤: {collection_info.get('points_count', 'N/A'):,}")
        
        if update_mode is None: # Only ask if not specified
            choice = input("\n‚ùì –ö–æ–ª–µ–∫—Ü—ñ—è —ñ—Å–Ω—É—î. –û–Ω–æ–≤–∏—Ç–∏ (–¥–æ–¥–∞—Ç–∏ –Ω–æ–≤—ñ –¥–∞–Ω—ñ) —á–∏ –≤–∏–¥–∞–ª–∏—Ç–∏ —ñ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∑–∞–Ω–æ–≤–æ? (update/recreate/cancel): ").lower()
            if choice == 'recreate':
                update_mode = False # This means full recreate
            elif choice == 'update':
                update_mode = True # This means add new, skip existing
            else:
                logger.info("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º.")
                return False # Indicate cancellation
                
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ (force_recreate or user chose recreate)
        if not update_mode: # update_mode is False if we need to recreate
            try:
                # Use a temporary client for deletion before system init
                temp_client = QdrantClient(host=qdrant_host, port=qdrant_port)
                logger.info(f"üóëÔ∏è –°–ø—Ä–æ–±–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection_name}'...")
                temp_client.delete_collection(collection_name)
                logger.info(f"üóëÔ∏è –°—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}' –≤–∏–¥–∞–ª–µ–Ω–æ.")
                temp_client.close()
                exists = False # Collection no longer exists
            except Exception as e:
                # Log error but potentially continue if it's "not found"
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection_name}': {e}. –ú–æ–∂–ª–∏–≤–æ, —ó—ó –≤–∂–µ –Ω–µ –±—É–ª–æ.")
                # If deletion fails for other reasons, it might be an issue.
    elif force_recreate and exists:
        logger.info(f"üî• –ü—Ä–∏–º—É—Å–æ–≤–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection_name}'.")
        update_mode = False # force_recreate implies not updating but starting fresh
        try:
            temp_client = QdrantClient(host=qdrant_host, port=qdrant_port)
            logger.info(f"üóëÔ∏è –°–ø—Ä–æ–±–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection_name}' –¥–ª—è –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è...")
            temp_client.delete_collection(collection_name)
            logger.info(f"üóëÔ∏è –°—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}' –≤–∏–¥–∞–ª–µ–Ω–æ –¥–ª—è –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è.")
            temp_client.close()
            exists = False
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{collection_name}' –ø—ñ–¥ —á–∞—Å –ø—Ä–∏–º—É—Å–æ–≤–æ–≥–æ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: {e}.")
            # Depending on strictness, might want to return False here

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏ —á–µ—Ä–µ–∑ system_provider
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ categories_file, qdrant_host, qdrant_port –±—É–¥—É—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ system_provider.get_system()
    # —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –≤–∏–∫–ª–∏–∫—É. –Ø–∫—â–æ —Å–∏—Å—Ç–µ–º–∞ –≤–∂–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞, —Ü—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —ñ–≥–Ω–æ—Ä—É—é—Ç—å—Å—è.
    logger.info("\nüì¶ –°–ø—Ä–æ–±–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏/—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ TenderAnalysisSystem...")
    try:
        system = get_system(categories_file=categories_file, qdrant_host=qdrant_host, qdrant_port=qdrant_port)
    except Exception as e:
        logger.exception(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ç—Ä–∏–º–∞–Ω–Ω—è/—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏: {e}")
        return False

    if not is_system_initialized() or not system.is_initialized:
        logger.error("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó TenderAnalysisSystem —á–µ—Ä–µ–∑ system_provider!")
        return False
    
    logger.info("‚úÖ TenderAnalysisSystem –æ—Ç—Ä–∏–º–∞–Ω–æ/—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")

    # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ vector_db –≤ system –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø—Ä–∞–≤–∏–ª—å–Ω—É –Ω–∞–∑–≤—É –∫–æ–ª–µ–∫—Ü—ñ—ó.
    # TenderVectorDB.__init__ —Å—Ç–≤–æ—Ä—é—î –∫–æ–ª–µ–∫—Ü—ñ—é, —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î.
    # –Ø–∫—â–æ –º–∏ –≤–∏–¥–∞–ª–∏–ª–∏ —ó—ó –≤–∏—â–µ, –≤–æ–Ω–∞ –±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–∞ —Ç—É—Ç –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    # –Ø–∫—â–æ –≤–æ–Ω–∞ —ñ—Å–Ω—É—î, TenderVectorDB –ø—ñ–¥–∫–ª—é—á–∏—Ç—å—Å—è –¥–æ –Ω–µ—ó.
    system.vector_db.collection_name = collection_name # Ensure an explicit set, though constructor might handle it

    # –Ø–∫—â–æ –∫–æ–ª–µ–∫—Ü—ñ—è –Ω–µ —ñ—Å–Ω—É–≤–∞–ª–∞ (–∞–±–æ –±—É–ª–∞ –≤–∏–¥–∞–ª–µ–Ω–∞), TenderVectorDB —ó—ó —Å—Ç–≤–æ—Ä–∏—Ç—å –ø—ñ–¥ —á–∞—Å —Å–≤–æ—î—ó —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó (—è–∫–∞ —î —á–∞—Å—Ç–∏–Ω–æ—é TenderAnalysisSystem.initialize_system)
    # –ù–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏, —á–∏ –≤–æ–Ω–∞ —Å–ø—Ä–∞–≤–¥—ñ —Å—Ç–≤–æ—Ä–µ–Ω–∞, —è–∫—â–æ –º–∏ –æ—á—ñ–∫—É—î–º–æ, —â–æ –≤–æ–Ω–∞ –±—É–¥–µ.
    # –ê–±–æ, —è–∫—â–æ –º–∏ —Ö–æ—á–µ–º–æ –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç—É—Ç:
    if not exists: # If it didn't exist or was deleted
        logger.info(f"–°–ø—Ä–æ–±–∞ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}' —á–µ—Ä–µ–∑ system.vector_db (—è–∫—â–æ —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ)...")
        # The vector_db instance within the system should handle its own collection creation logic.
        # This might involve calling a specific method like system.vector_db.create_collection_if_not_exists()
        # For now, we assume TenderVectorDB's constructor or an internal call in initialize_system handles this.
        # Let's ensure it's actually there after initialization.
        try:
            system.vector_db.client.get_collection(collection_name=collection_name)
            logger.info(f"–ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' —ñ—Å–Ω—É—î –ø—ñ—Å–ª—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏.")
        except Exception as e:
            logger.error(f"–ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –Ω–∞–≤—ñ—Ç—å –ø—ñ—Å–ª—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å–∏—Å—Ç–µ–º–∏: {e}")
            logger.error("–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ª–æ–≥—ñ–∫—É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó –≤ TenderVectorDB –∞–±–æ TenderAnalysisSystem.initialize_system().")
            return False


    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    # update_mode being True means we are adding to an existing collection (or a newly created empty one).
    # update_mode being False means we started with a fresh, empty collection.
    initial_count = 0
    if update_mode: # Only relevant if we are in "add to existing" mode
        try:
            initial_count = system.vector_db.get_collection_size()
            logger.info(f"\nüìä –ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ (—Ä–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è): {initial_count:,}")
        except Exception as e:
            logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π —Ä–æ–∑–º—ñ—Ä –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")
            initial_count = 0 # Assume 0 if error
    else: # Recreate mode
        logger.info(f"\nüìä –ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤ (—Ä–µ–∂–∏–º –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è): 0")
    
    # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
    total_indexed_overall = 0
    total_skipped_overall = 0
    total_errors_overall = 0
    overall_start_time = datetime.now()

    for idx, jsonl_file_path_str in enumerate(jsonl_files, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"üìÅ –§–∞–π–ª {idx}/{len(jsonl_files)}: {Path(jsonl_file_path_str).name}")
        logger.info(f"{'='*60}")
        
        if not Path(jsonl_file_path_str).exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {jsonl_file_path_str}")
            continue

        # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É
        # update_mode –¥–ª—è process_file_fast –º–∞—î –±—É—Ç–∏ True, —è–∫—â–æ –º–∏ —Ö–æ—á–µ–º–æ –¥–æ–¥–∞–≤–∞—Ç–∏ –¥–∞–Ω—ñ
        # (—Ç–æ–±—Ç–æ, index_tenders –º–∞—î –æ–±—Ä–æ–±–ª—è—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∞–±–æ –¥–æ–¥–∞–≤–∞—Ç–∏ –Ω–æ–≤—ñ).
        # –Ø–∫—â–æ update_mode –¥–ª—è create_optimized_vector_database –±—É–ª–æ False (recreate),
        # —Ç–æ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ —Ñ–∞–π–ª—É —Ü–µ —Ñ–∞–∫—Ç–∏—á–Ω–æ –Ω–µ update, –∞ –ø–æ—á–∞—Ç–∫–æ–≤–µ –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è.
        # –î–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ —Ü–µ –∑–∞–≤–∂–¥–∏ update –¥–æ –ø–æ—Ç–æ—á–Ω–æ—ó –∫–æ–ª–µ–∫—Ü—ñ—ó.
        current_file_update_mode = True # By default, index_tenders should try to update/add

        stats = process_file_fast( # Renamed from process_file_with_stats; assuming process_file_fast is the one to use
            jsonl_file_path_str,
            system, 
            batch_size=batch_size,
            update_mode=current_file_update_mode, # index_tenders handles if it's new or adding
            fast_mode=fast_mode # This might be a hint for index_tenders
        )
        
        total_indexed_overall += stats.get('indexed_count', 0)
        total_skipped_overall += stats.get('skipped_count', 0)
        total_errors_overall += stats.get('error_count', 0)

        logger.info(f"\n‚úÖ –§–∞–π–ª {Path(jsonl_file_path_str).name} –æ–±—Ä–æ–±–ª–µ–Ω–æ:")
        logger.info(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ (–≤ —Ü—å–æ–º—É —Ñ–∞–π–ª—ñ): {stats.get('indexed_count', 0):,}")
        logger.info(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤ —Ü—å–æ–º—É —Ñ–∞–π–ª—ñ): {stats.get('skipped_count', 0):,}")
        logger.info(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫ (–≤ —Ü—å–æ–º—É —Ñ–∞–π–ª—ñ): {stats.get('error_count', 0):,}")

    # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    final_count = 0
    try:
        final_count = system.vector_db.get_collection_size()
    except Exception as e:
        logger.error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –∫–æ–ª–µ–∫—Ü—ñ—ó: {e}")

    processing_time_seconds = (datetime.now() - overall_start_time).total_seconds()

    logger.info("\n" + "="*60)
    logger.info("üìä –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø:")
    logger.info("="*60)

    logger.info(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ: {final_count:,}")
    # Records added depends on whether it was update or recreate, and initial_count
    if update_mode: # If we were in update mode for the whole process
        logger.info(f"üìà –î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ (–ø—Ä–∏–±–ª–∏–∑–Ω–æ): {final_count - initial_count:,}")
    else: # If we were in recreate mode
        logger.info(f"üìà –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ –ø—ñ—Å–ª—è –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: {final_count:,}")

    logger.info(f"üì¶ –í—Å—å–æ–≥–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ —Ç–∞ —Å–ø—Ä–æ–±—É–≤–∞–Ω–æ —ñ–Ω–¥–µ–∫—Å—É–≤–∞—Ç–∏: {total_indexed_overall:,}")
    logger.info(f"‚è≠Ô∏è –í—Å—å–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ (–º–æ–∂–ª–∏–≤–æ, –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤): {total_skipped_overall:,}")
    logger.info(f"‚ùå –í—Å—å–æ–≥–æ –ø–æ–º–∏–ª–æ–∫ –ø—ñ–¥ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—ñ–≤: {total_errors_overall:,}")
    logger.info(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å –æ–±—Ä–æ–±–∫–∏: {processing_time_seconds:.1f} —Å–µ–∫")
    
    if total_indexed_overall > 0 and processing_time_seconds > 0:
        logger.info(f"üöÄ –®–≤–∏–¥–∫—ñ—Å—Ç—å –æ–±—Ä–æ–±–∫–∏: {total_indexed_overall / processing_time_seconds:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
    
    if fast_mode: # Assuming fast_mode implies indexing might be deferred
        logger.warning(f"\n‚ö†Ô∏è  –£–í–ê–ì–ê (–®–í–ò–î–ö–ò–ô –†–ï–ñ–ò–ú):")
        logger.warning(f"   üî• –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –º–æ–≥–ª–∞ –±—É—Ç–∏ –í–Ü–î–ö–õ–ê–î–ï–ù–ê –∞–±–æ –í–ò–ú–ö–ù–ï–ù–ê –ø—ñ–¥ —á–∞—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è.")
        logger.warning(f"   üìã –î–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—à—É–∫—É –º–æ–∂–µ –∑–Ω–∞–¥–æ–±–∏—Ç–∏—Å—è —É–≤—ñ–º–∫–Ω—É—Ç–∏/–∑–∞–≤–µ—Ä—à–∏—Ç–∏ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é.")
        logger.warning(f"   üõ†Ô∏è –ó–∞–ø—É—Å—Ç—ñ—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π —Å–∫—Ä–∏–ø—Ç (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, enable_indexing.py), —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ.")
    
    logger.info("="*60)
    
    return True


# Main execution block
if __name__ == "__main__":
    # Default files and parameters for direct script execution
    DEFAULT_FILES = [
        "data/out_10_nodup_nonull.jsonl", # Assuming a data directory
        "data/out_12_nodup_nonull.jsonl"
    ]
    DEFAULT_CATEGORIES_FILE = "data/categories.jsonl" # Assuming a data directory
    DEFAULT_COLLECTION_NAME = "tender_vectors"
    DEFAULT_BATCH_SIZE = 1850
    DEFAULT_QDRANT_HOST = "localhost"
    DEFAULT_QDRANT_PORT = 6333

    # Script behavior parameters
    # UPDATE_MODE_CHOICE: None (ask), True (update), False (recreate)
    # For direct execution, let's default to asking the user if the collection exists.
    USER_CHOICE_UPDATE_MODE = None
    FORCE_RECREATE_COLLECTION = False # Set to True to always delete and recreate
    ENABLE_FAST_MODE = True           # Use fast processing logic

    # Setup basic logging for standalone script execution
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è/–æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö...")

    # Example: Check if default files exist, use placeholders if not
    actual_files_to_process = []
    for f_path_str in DEFAULT_FILES:
        f_path = Path(f_path_str)
        if f_path.exists():
            actual_files_to_process.append(f_path_str)
        else:
            logger.warning(f"‚ö†Ô∏è  –§–∞–π–ª –¥–∞–Ω–∏—Ö –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {f_path_str}. –ô–æ–≥–æ –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")

    if not Path(DEFAULT_CATEGORIES_FILE).exists():
        logger.warning(f"‚ö†Ô∏è  –§–∞–π–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {DEFAULT_CATEGORIES_FILE}. –°–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –∞–±–æ –Ω–µ –º–∞—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ–π.")

    if not actual_files_to_process:
        logger.error("‚ùå –ù–µ–º–∞—î —Ñ–∞–π–ª—ñ–≤ –¥–∞–Ω–∏—Ö –¥–ª—è –æ–±—Ä–æ–±–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏.")
    else:
        logger.info("="*50)
        if ENABLE_FAST_MODE:
            logger.info("‚ö° –†–µ–∂–∏–º –®–í–ò–î–ö–û–ì–û –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –£–í–Ü–ú–ö–ù–ï–ù–û.")
            logger.info("   –¶–µ –º–æ–∂–µ –æ–∑–Ω–∞—á–∞—Ç–∏ –≤—ñ–¥–∫–ª–∞–¥–µ–Ω—É —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ.")
        else:
            logger.info("üêå –†–µ–∂–∏–º –∑–≤–∏—á–∞–π–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è.")
        logger.info(f"üóÇÔ∏è  –§–∞–π–ª–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏: {actual_files_to_process}")
        logger.info(f"üè∑Ô∏è  –ö–æ–ª–µ–∫—Ü—ñ—è: {DEFAULT_COLLECTION_NAME}")
        logger.info(f"üì¶ –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É: {DEFAULT_BATCH_SIZE}")
        logger.info(f"üóÑÔ∏è  Qdrant: {DEFAULT_QDRANT_HOST}:{DEFAULT_QDRANT_PORT}")
        logger.info("="*50)

        # Confirmation from user
        response = input(f"\nüöÄ –ü–æ—á–∞—Ç–∏ –ø—Ä–æ—Ü–µ—Å –¥–ª—è –∫–æ–ª–µ–∫—Ü—ñ—ó '{DEFAULT_COLLECTION_NAME}'? (y/n): ")
        if response.lower() == 'y':
            success = create_optimized_vector_database(
                jsonl_files=actual_files_to_process,
                categories_file=DEFAULT_CATEGORIES_FILE,
                collection_name=DEFAULT_COLLECTION_NAME,
                batch_size=DEFAULT_BATCH_SIZE,
                update_mode=USER_CHOICE_UPDATE_MODE, # Will ask if None and collection exists
                force_recreate=FORCE_RECREATE_COLLECTION,
                fast_mode=ENABLE_FAST_MODE,
                qdrant_host=DEFAULT_QDRANT_HOST,
                qdrant_port=DEFAULT_QDRANT_PORT
                # monitor_interval and max_records are not used in the new signature, handled internally or by data size
            )

            if success:
                logger.info("\n" + "="*60)
                logger.info("üéâ –ü–†–û–¶–ï–° –°–¢–í–û–†–ï–ù–ù–Ø/–û–ù–û–í–õ–ï–ù–ù–Ø –í–ï–ö–¢–û–†–ù–û–á –ë–ê–ó–ò –ó–ê–í–ï–†–®–ï–ù–û!")
                logger.info("="*60)
                if ENABLE_FAST_MODE:
                    logger.warning("‚ö†Ô∏è  –£–í–ê–ì–ê: –Ø–∫—â–æ –±—É–ª–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —à–≤–∏–¥–∫–∏–π —Ä–µ–∂–∏–º, —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                    logger.warning("   –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Å—Ç–∞—Ç—É—Å —Ç–∞, –∑–∞ –ø–æ—Ç—Ä–µ–±–∏, –∑–∞–ø—É—Å—Ç—ñ—Ç—å enable_indexing.py.")
                logger.info("="*60)
            else:
                logger.error("\n‚ùå –ü—Ä–æ—Ü–µ—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è/–æ–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏ –∞–±–æ –±—É–≤ —Å–∫–∞—Å–æ–≤–∞–Ω–∏–π.")
        else:
            logger.info("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º.")
