
"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
"""

import logging
import json
from pathlib import Path
from tender_analysis_system import TenderAnalysisSystem
from qdrant_client import QdrantClient
from tqdm import tqdm
from datetime import datetime
from qdrant_client.http import models

from vector_database import TenderVectorDB

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def fast_upsert_batch(self, points, batch_num: int = 0) -> int:
    """–®–≤–∏–¥–∫–∞ –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è"""
    try:
        print(f"üì° –®–≤–∏–¥–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—É #{batch_num} –∑ {len(points)} —Ç–æ—á–æ–∫...")
        
        if not points:
            print(f"‚ùå –ü–æ—Ä–æ–∂–Ω—ñ–π –±–∞—Ç—á #{batch_num}")
            return 0
        
        # üî• –®–í–ò–î–ö–ê –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –ë–ï–ó –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
        self.client.upsert(
            collection_name=self.collection_name,
            points=points,
            wait=True,  # –ù–ï —á–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è!
            ordering=models.WriteOrdering.WEAK  # –°–ª–∞–±–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å
        )
        
        print(f"‚ö° –ë–∞—Ç—á #{batch_num} –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ")
        return len(points)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —à–≤–∏–¥–∫–æ—ó –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –±–∞—Ç—á—É #{batch_num}: {e}")
        return 0


def monitor_progress(system, start_count, interval=50000):
    """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—É —Ç–∞ —Ä–æ–∑–º—ñ—Ä—É"""
    current_count = system.vector_db.get_collection_size()
    if current_count - start_count >= interval:
        stats = system.vector_db.get_storage_info()
        logging.info(f"""
        üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
        - –ó–∞–ø–∏—Å—ñ–≤: {current_count:,}
        - –†–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä—ñ–≤: {stats['vectors_size_gb']} –ì–ë
        - –†–æ–∑–º—ñ—Ä –º–µ—Ç–∞–¥–∞–Ω–∏—Ö: {stats['payload_size_gb']} –ì–ë
        - –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {stats['estimated_total_gb']} –ì–ë
        """)
        return current_count
    return start_count

def process_file_with_stats(file_path, system, batch_size=1000, update_mode=True):
    """
    –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É –∑ –¥–µ—Ç–∞–ª—å–Ω–æ—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
    
    Args:
        file_path: —à–ª—è—Ö –¥–æ JSONL —Ñ–∞–π–ª—É
        system: –µ–∫–∑–µ–º–ø–ª—è—Ä TenderAnalysisSystem
        batch_size: —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        update_mode: True - –¥–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤, False - –ø–æ–≤–Ω–∞ –ø–µ—Ä–µ—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    """
    print(f"\nüìÅ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file_path}")
    file_size = Path(file_path).stat().st_size / (1024**3)
    print(f"üìä –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {file_size:.2f} –ì–ë")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤
    print("üìù –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤...")
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    errors = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                errors += 1
                if errors <= 10:  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 10 –ø–æ–º–∏–ª–æ–∫
                    print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}")
    
    if errors > 0:
        print(f"‚ö†Ô∏è –í—Å—å–æ–≥–æ –ø–æ–º–∏–ª–æ–∫ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {errors}")
    
    # –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–µ–∂–∏–º—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    print(f"\nüîÑ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –≤ —Ä–µ–∂–∏–º—ñ: {'–æ–Ω–æ–≤–ª–µ–Ω–Ω—è' if update_mode else '–ø–æ–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è'}")
    stats = system.vector_db.index_tenders(
        data,
        update_mode=update_mode,  # –ü–µ—Ä–µ–¥–∞—î–º–æ —Ä–µ–∂–∏–º –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
        batch_size=batch_size
    )
    
    return stats

def check_existing_collection(host="localhost", port=6333, collection_name="tender_vectors"):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó
    
    Returns:
        tuple: (exists: bool, info: dict)
    """
    try:
        client = QdrantClient(host=host, port=port)
        collection_info = client.get_collection(collection_name)
        
        info = {
            'exists': True,
            'points_count': collection_info.points_count,
            'status': collection_info.status,
            'vectors_size': collection_info.config.params.vectors.size,
            'segments_count': collection_info.segments_count if collection_info.segments_count else 0
        }
        
        return True, info
    except:
        return False, {'exists': False}


def process_file_fast(file_path, system, batch_size=5000, update_mode=True, fast_mode=True):
    """–®–í–ò–î–ö–ê –æ–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É"""
    print(f"‚ö° –®–í–ò–î–ö–ê –æ–±—Ä–æ–±–∫–∞: {file_path}")
    
    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–ø–∏—Å—ñ–≤
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for _ in f)
    print(f"üìä –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤: {total_lines:,}")
    
    # –®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ–±—Ä–æ–±–∫–∞
    data = []
    errors = 0
    
    print("üì• –®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ –ø–∞–º'—è—Ç—å...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è", unit="—Ä—è–¥–∫—ñ–≤"):
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                errors += 1
                if errors <= 5:
                    print(f"‚ö†Ô∏è JSON –ø–æ–º–∏–ª–∫–∞: {e}")
    
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data):,} –∑–∞–ø–∏—Å—ñ–≤ (–ø–æ–º–∏–ª–æ–∫: {errors})")
    
    # –®–í–ò–î–ö–ê —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è
    print(f"‚ö° –®–í–ò–î–ö–ê —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –∑ –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...")
    stats = system.vector_db.index_tenders(
        data,
        update_mode=update_mode,
        batch_size=batch_size
    )
    
    return stats



def create_optimized_vector_database(
    jsonl_files: list,
    categories_file: str = "categories.jsonl",
    collection_name: str = "tender_vectors",
    batch_size: int = 5000,
    update_mode: bool = None,
    force_recreate: bool = False,
    fast_mode: bool = True
):
    """
    –®–í–ò–î–ö–ï —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
    """
    
    print("="*60)
    if fast_mode:
        print("‚ö° –®–í–ò–î–ö–ò–ô –†–ï–ñ–ò–ú –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø")
        print("   ‚Ä¢ –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –í–ò–ú–ö–ù–ï–ù–ê")
        print("   ‚Ä¢ –ó–±—ñ–ª—å—à–µ–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É")
        print("   ‚Ä¢ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞")
    else:
        print("üêå –ó–í–ò–ß–ê–ô–ù–ò–ô –†–ï–ñ–ò–ú –ó –Ü–ù–î–ï–ö–°–ê–¶–Ü–Ñ–Æ")
    print("="*60)
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó
    exists, collection_info = check_existing_collection(collection_name=collection_name)
    
    if exists and not force_recreate:
        print(f"\n‚úÖ –ö–æ–ª–µ–∫—Ü—ñ—è '{collection_name}' –≤–∂–µ —ñ—Å–Ω—É—î:")
        print(f"   ‚Ä¢ –ó–∞–ø–∏—Å—ñ–≤: {collection_info['points_count']:,}")
        
        if update_mode is None:
            choice = input("\n‚ùì –í–∏–¥–∞–ª–∏—Ç–∏ —ñ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –∑–∞–Ω–æ–≤–æ? (y/n): ")
            if choice.lower() != 'y':
                print("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞")
                return False
            update_mode = False
                
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if not update_mode:
            try:
                client = QdrantClient(host="localhost", port=6333)
                client.delete_collection(collection_name)
                print(f"üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—É –∫–æ–ª–µ–∫—Ü—ñ—é '{collection_name}'")
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è: {e}")
                return False
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
    print("\nüì¶ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏...")
    
    # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º –¥–ª—è transformers
    import logging
    logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
    logging.getLogger("transformers").setLevel(logging.ERROR)
    
    system = TenderAnalysisSystem(
        categories_file=categories_file,
        qdrant_host="localhost",
        qdrant_port=6333
    )
    
    if not system.initialize_system():
        print("‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó!")
        return False
    
    # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É –Ω–∞–∑–≤—É –∫–æ–ª–µ–∫—Ü—ñ—ó
    system.vector_db.collection_name = collection_name
    
    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    initial_count = system.vector_db.get_collection_size() if update_mode else 0
    print(f"\nüìä –ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤: {initial_count:,}")
    
    # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—ñ–≤
    total_indexed = 0
    total_skipped = 0
    total_errors = 0
    start_time = datetime.now()

    for idx, jsonl_file in enumerate(jsonl_files, 1):
        print(f"\n{'='*60}")
        print(f"üìÅ –§–∞–π–ª {idx}/{len(jsonl_files)}: {Path(jsonl_file).name}")
        print(f"{'='*60}")
        
        if not Path(jsonl_file).exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {jsonl_file}")
            continue

        # –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É
        stats = process_file_fast(
            jsonl_file, 
            system, 
            batch_size=batch_size,
            update_mode=update_mode if update_mode is not None else True,
            fast_mode=fast_mode
        )
        
        total_indexed += stats.get('indexed_count', 0)
        total_skipped += stats.get('skipped_count', 0)
        total_errors += stats.get('error_count', 0)

        print(f"\n‚úÖ –§–∞–π–ª –æ–±—Ä–æ–±–ª–µ–Ω–æ:")
        print(f"   ‚Ä¢ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {stats.get('indexed_count', 0):,}")
        print(f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats.get('skipped_count', 0):,}")
        print(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {stats.get('error_count', 0):,}")

    # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    final_count = system.vector_db.get_collection_size()
    processing_time = (datetime.now() - start_time).total_seconds()
    
    print("\n" + "="*60)
    print("üìä –§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("="*60)
    
    print(f"‚úÖ –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É –±–∞–∑—ñ: {final_count:,}")
    print(f"üìà –î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {final_count - initial_count:,}")
    print(f"üì¶ –ü—Ä–æ—ñ–Ω–¥–µ–∫—Å–æ–≤–∞–Ω–æ: {total_indexed:,}")
    print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {total_skipped:,}")
    print(f"‚ùå –ü–æ–º–∏–ª–æ–∫: {total_errors:,}")
    print(f"‚è±Ô∏è –ß–∞—Å –æ–±—Ä–æ–±–∫–∏: {processing_time:.1f} —Å–µ–∫")
    
    if total_indexed > 0:
        print(f"üöÄ –®–≤–∏–¥–∫—ñ—Å—Ç—å: {total_indexed / processing_time:.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫")
    
    if fast_mode:
        print(f"\n‚ö†Ô∏è  –£–í–ê–ì–ê:")
        print(f"   üî• –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –í–ò–ú–ö–ù–ï–ù–ê!")
        print(f"   üìã –î–ª—è –ø–æ—à—É–∫—É –ø–æ—Ç—Ä—ñ–±–Ω–æ —É–≤—ñ–º–∫–Ω—É—Ç–∏ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é")
        print(f"   üõ†Ô∏è –ó–∞–ø—É—Å—Ç—ñ—Ç—å: python enable_indexing.py")
    
    print("="*60)
    
    return True



if __name__ == "__main__":
    # ===== –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –î–õ–Ø –®–í–ò–î–ö–û–ì–û –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø =====
    
    FILES = [
        "out_10_nodup_nonull.jsonl",
        "out_12_nodup_nonull.jsonl"
    ]
    
    # üî• –ó–ë–Ü–õ–¨–®–ï–ù–Ü –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    COLLECTION_NAME = "tender_vectors"
    BATCH_SIZE = 1850                   
    MONITOR_INTERVAL = 50000           
    
    # –†–µ–∂–∏–º —Ä–æ–±–æ—Ç–∏
    UPDATE_MODE = True                  # –ü–æ–≤–Ω–µ –ø–µ—Ä–µ—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è
    MAX_RECORDS = None                  # –í—Å—ñ –∑–∞–ø–∏—Å–∏
    
    print("üöÄ –®–í–ò–î–ö–ï –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø (–ë–ï–ó –Ü–ù–î–ï–ö–°–ê–¶–Ü–á)")
    print("="*50)
    print("‚ö° –ö–æ–ª–µ–∫—Ü—ñ—è –±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–∞ –ë–ï–ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó")
    print("‚ö° –ó–±—ñ–ª—å—à–µ–Ω–∏–π —Ä–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–æ 5000")
    print("‚ö° –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –±–∞—Ç—á—ñ–≤")
    print("‚ö° –ü—ñ—Å–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—É–¥–µ —É–≤—ñ–º–∫–Ω—É—Ç–∏ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é")
    print("="*50)
    
    response = input("\nüöÄ –ü–æ—á–∞—Ç–∏ –®–í–ò–î–ö–ï –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è? (y/n): ")
    if response.lower() == 'y':
        success = create_optimized_vector_database(
            jsonl_files=FILES,
            collection_name=COLLECTION_NAME,
            batch_size=BATCH_SIZE,
            max_records=MAX_RECORDS,
            monitor_interval=MONITOR_INTERVAL,
            update_mode=UPDATE_MODE,
            fast_mode=True  # üî• –ù–û–í–ò–ô –ø–∞—Ä–∞–º–µ—Ç—Ä
        )
        
        if success:
            print("\n" + "="*60)
            print("üéâ –®–í–ò–î–ö–ï –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û!")
            print("="*60)
            print("‚ö†Ô∏è  –£–í–ê–ì–ê: –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –í–ò–ú–ö–ù–ï–ù–ê!")
            print("üìã –î–ª—è —É–≤—ñ–º–∫–Ω–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –∑–∞–ø—É—Å—Ç—ñ—Ç—å:")
            print("   python enable_indexing.py")
            print("="*60)
        else:
            print("\n‚ùå –®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏")
    else:
        print("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—è —Å–∫–∞—Å–æ–≤–∞–Ω–∞")
